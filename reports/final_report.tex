% To compile with official ACL style:
% 1. Download acl.sty from https://github.com/acl-org/acl-style-files
% 2. Uncomment the line below and comment out the article line
% \documentclass[11pt,a4paper]{article}
%\usepackage[hyperref]{acl}
\documentclass[11pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{amsmath}

% Define \And command for multiple authors (ACL compatibility)
% Use \renewcommand since amsmath already defines \And
\renewcommand{\And}{\end{tabular}\hskip 1em \begin{tabular}[t]{c}}

\title{Polarization Detection and Characterization at POLAR @ SemEval-2026:\\
A MARBERTv2 Ensemble for Arabic (ARB) Subtasks 1--3\\
\large{\textit{Final Report}}}

\author{
  \begin{tabular}[t]{c}
    Guanghui Ma \\
    \texttt{guanghui.ma@sabanciuniv.edu}
  \end{tabular}
  \And
  \begin{tabular}[t]{c}
    Nuh Al Sharafi \\
    \texttt{nuh.sharafi@sabanciuniv.edu}
  \end{tabular}
  \And
  \begin{tabular}[t]{c}
    Ali Khaled A. Ishtay Altamimi \\
    \texttt{ali.altamimi@sabanciuniv.edu}
  \end{tabular}
  \And
  \begin{tabular}[t]{c}
    Hussein MH Nasser \\
    \texttt{h.nasser@sabanciuniv.edu}
  \end{tabular}
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
This report presents our systems for SemEval-2026 Task~9 (POLAR) on the Arabic (ARB) track.
We address Subtask~1 (binary polarization detection) and extend to Subtasks~2--3 (multi-label polarization type and manifestation).
Our best Subtask~1 system fine-tunes MARBERTv2 with a stratified K-fold ensemble (soft voting), achieving
\textbf{macro precision 0.835}, \textbf{macro recall 0.839}, and \textbf{macro F1 0.834} on internal cross-validation.
For Subtask~2, we first explored prompt-based classification with an Arabic GPT-style model, then obtained our best results by
reusing the MARBERTv2 encoder for multi-label learning with \textbf{Asymmetric Loss (ASL)} to suppress easy-negative noise,
\textbf{snapshot ensembling} (cosine restarts), a \textbf{logistic-regression meta-learner} over stacked probabilities, and
\textbf{per-label threshold tuning}. This meta-ensemble achieves \textbf{macro F1 0.6788} on an internal validation split.
We also describe an initial prompt-based baseline for Subtask~3 and discuss preprocessing trade-offs for Arabic social media text,
where excessive normalization may remove stylistic cues that correlate with polarized/toxic language.
\end{abstract}

\section{Introduction}

Polarization refers to the divergence of attitudes away from the center toward more extreme group-aligned positions.
In online settings, anonymity, rapid information diffusion, and algorithmic amplification can increase hostile inter-group discourse
and intensify the spread of harmful narratives. Detecting polarization in multilingual social media is therefore relevant for
content moderation, public-sphere monitoring, and downstream tasks such as hate-speech and toxic-language detection.

This project targets POLAR @ SemEval-2026 (Task~9): Detecting Multilingual, Multicultural, and Multievent Online Polarization \cite{polar-semeval-2026}.
POLAR defines three subtasks:
\textbf{Subtask~1} is binary classification to determine whether a text is \textit{polarized} or \textit{non-polarized}.
\textbf{Subtask~2} is multi-label classification over polarization \textit{types/targets}:
\{\textit{political}, \textit{racial/ethnic}, \textit{religious}, \textit{gender/sexual}, \textit{other}\}.
\textbf{Subtask~3} is multi-label classification over polarization \textit{manifestations}:
\{\textit{stereotype}, \textit{vilification}, \textit{dehumanization}, \textit{extreme\_language}, \textit{lack\_of\_empathy}, \textit{invalidation}\}.
The official shared-task metric for all subtasks is macro F1.

We focus exclusively on the Arabic (ARB) dataset and address all three subtasks.
Our methodology centers on fine-tuning MARBERTv2~\cite{abdulmageed2021arbert}, a transformer model pretrained on Arabic social media,
combined with ensemble strategies to improve robustness. For Subtask~1, we employ stratified K-fold ensembling with soft voting,
achieving macro F1 of 0.834. For Subtask~2, we combine Asymmetric Loss, snapshot ensembling, stacking, and per-label threshold tuning
to reach macro F1 of 0.6788. We also explored morphological feature-based hybrid ensembles and prompt-based classification as alternative approaches.
Beyond maximizing F1-score, we document our experimental journey, including preprocessing decisions, ablation studies, and methods that were ultimately not used.

\section{Related Work}

Polarization detection is related to but distinct from stance detection, toxicity modeling, and hate-speech research,
as it specifically targets language that intensifies inter-group division and hostility.
We review five key areas of related work that informed our approach.

\paragraph{Polarization and Hate Speech Detection.}
Vasist et al.~\cite{vasist2023polarizing} examined the polarizing impact of political disinformation and hate speech across countries,
finding that online polarization manifests through hostile inter-group discourse and divisive narratives.
Their cross-country analysis highlighted the need for culturally-aware detection systems, motivating our focus on Arabic-specific models.

\paragraph{Arabic NLP Challenges.}
Farghaly and Shaalan~\cite{farghaly2009arabic} provided a comprehensive overview of Arabic NLP challenges,
including dialectal variation, orthographic ambiguity, and morphological complexity.
They noted that Arabic social media exhibits significant code-switching and informal spelling,
which complicates traditional NLP approaches and motivates the use of models pretrained on user-generated content.

\paragraph{Arabic Transformer Models.}
Abdul-Mageed et al.~\cite{abdulmageed2021arbert} introduced ARBERT and MARBERT, bidirectional transformers pretrained on Arabic text.
MARBERT specifically targets dialectal Arabic from Twitter, making it well-suited for social media polarization detection.
Their experiments showed state-of-the-art performance on various Arabic NLP benchmarks.
AlShenaifi et al.~\cite{alshenafi2024rasid} demonstrated MARBERT's effectiveness for Arabic stance detection,
achieving strong results by fine-tuning on domain-specific data.

\paragraph{Prompt-Based Classification.}
Antoun et al.~\cite{antoun2021aragpt2} presented AraGPT2, an Arabic GPT-style language model for text generation.
Al Hariri and Abu Farha~\cite{alhariri2024smash} showed that prompt engineering with Arabic LLMs can be effective for stance detection,
though results are sensitive to prompt format and few-shot example selection.
We adapted this approach as a baseline for multi-label classification.

\paragraph{Multi-Label Learning and Loss Functions.}
Ridnik et al.~\cite{ridnik2021asl} proposed Asymmetric Loss (ASL) for multi-label classification,
which addresses the easy-negative dominance problem by down-weighting confident negative predictions.
Their method showed significant improvements on benchmarks with label imbalance, which is characteristic of our Subtask~2 and~3 data.

\paragraph{Arabic Morphological Analysis.}
Obeid et al.~\cite{obeid2020cameltools} developed CAMeL Tools, an open-source toolkit for Arabic NLP
including morphological analysis, disambiguation, and dialect identification.
We used CAMeL Tools for both preprocessing experiments and morphological feature extraction.

\section{Methodology}

\subsection{Dataset}

We use the official Arabic (ARB) data released by the POLAR @ SemEval-2026 organizers.
For Subtask~1, the labeled training set contains \textbf{3,380} instances with columns \texttt{id}, \texttt{text}, and \texttt{polarization}.
The class distribution is moderately imbalanced with \textbf{1,868} non-polarized (55.3\%) vs.\ \textbf{1,512} polarized (44.7\%) instances
(see Appendix Figure~\ref{fig:label_dist_bar}).

For Subtask~2, the training set has the same 3,380 instances with five binary label columns
(\texttt{political}, \texttt{racial/ethnic}, \texttt{religious}, \texttt{gender/sexual}, \texttt{other}).
Subtask~3 follows the same format with six binary label columns representing manifestations.
The provided development files are unlabeled and intended for submission (169 instances for Subtask~1).

We chose to focus exclusively on the Arabic track because:
(i) it ensures consistent linguistic phenomena (dialects, orthography) across our experiments;
(ii) Arabic social media presents unique challenges including dialectal variation, code-switching, and right-to-left script;
(iii) it reduces domain mismatch that would arise from multilingual training.

\paragraph{Exploratory Data Analysis.}
We performed exploratory analysis to understand the properties of Arabic social-media text and its interaction with labels.
Texts are typically short with a long-tail of longer entries (Appendix Figure~\ref{fig:text_length_dist}).
We observed frequent code-switching (Latin characters and mixed digits) and common social-media artifacts such as URLs, mentions, and hashtags
(Appendix Figures~\ref{fig:latin_presence} and~\ref{fig:mention_presence}).
These observations motivated (i) a robust normalization strategy for noisy text and (ii) model choices known to work well for dialectal Arabic.

\subsection{Preprocessing}

We implemented and tested two preprocessing pipelines:

\paragraph{Basic preprocessing (used in final models).}
Our basic pipeline performs light Arabic normalization while preserving stylistic cues that may correlate with polarization/toxicity.
It includes removing excessive whitespace and non-text artifacts, normalizing common Arabic character variants (e.g., alif variants),
and handling diacritics/tatweel to reduce sparsity~\cite{Shammari2007}.
We keep expressive signals (e.g., emojis, repeated characters) when possible because they can reflect intensity.

\paragraph{Advanced preprocessing (tested, then discarded).}
We implemented an advanced pipeline using CAMeL Tools~\cite{obeid2020cameltools} with morphological analysis and clitic segmentation.
Our \texttt{ArabicAdvancedPreprocessor} can split enclitics (pronominal clitics), optionally split proclitics, and optionally apply lemmatization.
While linguistically appealing, we found that \textbf{over-processing removes or distorts surface cues}
(e.g., dialect spelling, emphatic elongation) that are predictive for polarized content.
We therefore proceeded with basic preprocessing (see Appendix Figure~\ref{fig:preprocessing_comparison} for comparison).

\subsection{Subtask 1: Binary Polarization Detection}

Our best Subtask~1 approach fine-tunes MARBERTv2 (\texttt{UBC-NLP/MARBERTv2}) for binary sequence classification.
To reduce variance and improve robustness under moderate class imbalance, we train a stratified K-fold ensemble and average predicted probabilities
(\textit{soft voting}). We used K=8 in our final setup.

\paragraph{Training configuration.}
We tokenize with maximum length 128, train for 4 epochs with learning rate $2\times10^{-5}$, batch size 16,
warmup steps 500, and weight decay 0.01. We use mixed precision when available.

\paragraph{Morphological feature exploration.}
We also explored a hybrid approach combining transformer-based models with traditional machine learning classifiers using morphological features.
Using CAMeL Tools~\cite{obeid2020cameltools}, we extracted 20 morphological features from each text, including:
\begin{itemize}
\item Part-of-speech (POS) ratios: noun, verb, adjective, adverb, particle, and pronoun frequencies.
\item Clitic patterns: proclitic (e.g., definite article \textit{al-}, conjunctions, prepositions) and enclitic (e.g., possessive/pronominal suffixes) ratios.
\item Morphological complexity: average morphemes per word, unique lemma ratio, and clitic-to-word ratio.
\item Structural indicators: sentence length, word count, and punctuation density.
\end{itemize}

We trained gradient boosting models (XGBoost, LightGBM) on these morphological features and experimented with four hybrid ensemble strategies:
(i)~\textbf{weighted averaging} of transformer and ML probabilities with various weight combinations;
(ii)~\textbf{stacking with a meta-learner} (logistic regression) over both models' predictions and confidence scores;
(iii)~\textbf{confidence-based selection}, choosing the prediction from the model with higher confidence; and
(iv)~\textbf{adaptive weighted ensembling}, dynamically weighting models proportionally to their per-sample confidence.

Our analysis revealed that certain morphological features correlate with polarized content---in particular, higher verb ratios
and certain clitic patterns showed statistically significant differences between polarized and non-polarized texts
(see Figure~\ref{fig:morph_feature_importance} for feature importances and Figure~\ref{fig:morph_roc_comparison} for ROC comparison).
However, in our experiments, the hybrid ensemble did not consistently outperform the pure MARBERTv2 K-fold ensemble
(Figure~\ref{fig:morph_ensemble_cm} shows confusion matrices for all ensemble strategies).
We attribute this to MARBERTv2's strong performance on dialectal Arabic already capturing much of the relevant linguistic signal,
and the relatively small gains from morphological features not justifying the added complexity.
Nonetheless, the morphological analysis provided valuable interpretability insights and identified challenging cases where both models fail
(Figure~\ref{fig:morph_prediction_analysis}), which could inform future data collection and model improvement efforts.

\subsection{Subtask 2: Multi-label Polarization Type}

\paragraph{Prompt-based baseline.}
We first attempted a prompt-based multi-label classifier using AraGPT2-medium \cite{antoun2021aragpt2}.
AraGPT2 was chosen for its generative capabilities and prior success in Arabic NLP tasks, as well as its manageable size for experimentation in comparison to larger arabic models such as acegpt and aragpt3.
We posed a yes/no decision per label using Arabic instructions and few-shot examples, the model would generate answers for each label, and we would parse the outputs to obtain binary predictions of presence/absence of the label.

The prompt format (shown here in English translation) was as follows:
\begin{quote}
Text: ``The government is corrupt and the people are oppressed''\\
Question: Does it contain political polarization?\\
Answer: Yes, it contains political polarization

Text: ``The weather is nice today''\\
Question: Does it contain political polarization?\\
Answer: No, it does not contain political polarization

Note: It should relate to politics, government, or political parties.

Text: ``The head of state is an infidel and the people are silent''\\
Question: Does it contain political polarization?\\
Answer: [model generates]
\end{quote}

As AraGPT2 is not fine-tuned for classification, this approach served as a baseline, by giving the model two examples per label (one positive, one negative), pulled directly from the dataset, and a hint about the label's scope in the form of a 'note' which differs for each label and helps the model understand the context of the label.

This approach yielded modest results, with macro F1 scores hovering around .4 to .43 on the test set, these are the results achieved after experimentation with parsing strategies, prompt variations and temperature settings.


\paragraph{Best system: MARBERTv2 + ASL + snapshot ensemble + stacking + thresholds.}
Our strongest Subtask~2 system adapts MARBERTv2 to multi-label classification and combines four components:

\begin{itemize}
\item \textbf{Loss: Asymmetric Loss (ASL).} We replace standard BCE with ASL \cite{ridnik2021asl} to reduce easy-negative dominance.
In our implementation, $\gamma_{\text{neg}}=4.0$, $\gamma_{\text{pos}}=1.0$, with clipping $c=0.05$:
\begin{align}
p &= \sigma(z), \\
\mathcal{L}_{\text{ASL}} &= -\Big[(1-p)^{\gamma_{\text{pos}}}y\log(p) + p^{\gamma_{\text{neg}}}(1-y)\log(1-p)\Big],
\end{align}
with a probability clipping step on $(1-p)$ for stability (see \texttt{Final\_subtask2.ipynb}).

\item \textbf{Snapshot ensembles.} During training, we save $S=4$ snapshots using a cosine schedule with restarts
(\texttt{lr\_scheduler\_type = cosine\_with\_restarts}). This yields multiple diverse checkpoints without retraining from scratch.

\item \textbf{Meta-learner (stacking).} We stack snapshot probabilities on the validation set and train a One-vs-Rest logistic regression
(\texttt{solver=liblinear}, \texttt{max\_iter=2000}) to combine snapshots into better-calibrated per-label probabilities.

\item \textbf{Per-label threshold tuning.} Instead of a global 0.5 threshold, we grid-search thresholds per label on the validation set
to maximize per-label F1, then apply these thresholds for final predictions.
\end{itemize}

\paragraph{Training configuration.}
We use max length 256, epochs 4, learning rate $2\times10^{-5}$, batch size 16, warmup steps 500, and weight decay 0.01.

\subsection{Subtask 3: Multi-label Manifestations}

For Subtask~3, we implemented an initial prompt-based baseline using the same AraGPT2-medium framework as Subtask~2, but with manifestation labels
and label-specific hints.
Due to time constraints, Subtask~3 was not optimized to the same extent as Subtasks~1--2.


We provide an initial AraGPT2 prompt-based baseline for Subtask~3. The general format mirrors Subtask~2 but with manifestation labels instead of polarization types.

Below is an example prompt for the \textit{stereotype} label (shown in English translation):
\begin{quote}
Text: ``All refugees are lazy and do not work''\\
Question: Does it contain stereotypes?\\
Answer: Yes, it contains stereotypes

Text: ``The weather is nice today and people are happy''\\
Question: Does it contain stereotypes?\\
Answer: No, it does not contain stereotypes

Note: It should contain a clear generalization about a group of people.

Text: ``These people always cause trouble''\\
Question: Does it contain stereotypes?\\
Answer: [model generates]
\end{quote}

As with subtask 2, this prompt is repeated for each of the six manifestation labels with appropriate label-specific hints.

Results from this initial prompt-based Subtask~3 system are modest, with macro F1 scores around 0.54 on the train set validation split.

\subsection{Training Setup}

All transformer models are trained using Hugging Face \texttt{transformers}.
Subtask~1 uses stratified K-fold cross-validation (K=8); Subtask~2 uses an 85/15 train/validation split
because the official dev set is unlabeled and intended for submission.
All training runs were performed on Google Colab using an NVIDIA L4 GPU (24\,GB VRAM).
Mixed-precision training (FP16) was enabled to reduce memory usage and accelerate training.
We used random seed 42 for reproducibility across all experiments.

\section{Results}

We report F1-score, macro precision, and macro recall for all subtasks, along with confusion matrices and ROC curves.
We also document alternative methods tried during our experimental journey.

\subsection{Subtask 1: Binary Polarization Detection}

Table~\ref{tab:subtask1_results} reports mean macro precision/recall/F1 over our internal K-fold validation.
Soft-voting ensembles are consistently more stable than single fine-tuned checkpoints.

\begin{table}[htbp]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{System} & \textbf{Macro P} & \textbf{Macro R} & \textbf{Macro F1} \\
\midrule
MARBERTv2 (single model) & 0.820 & 0.825 & 0.819 \\
MARBERTv2 + K-fold Ensemble (K=8) & \textbf{0.835} & \textbf{0.839} & \textbf{0.834} \\
XGBoost (morphological features only) & 0.672 & 0.672 & 0.672 \\
Hybrid Weighted Ensemble (0.4/0.6) & 0.831 & 0.831 & 0.821 \\
\bottomrule
\end{tabular}
\caption{Subtask~1 performance comparison. The MARBERTv2 K-fold ensemble achieves the best results.}
\label{tab:subtask1_results}
\end{table}

\paragraph{Morphological Hybrid Ensemble Results.}
We experimented with combining MARBERTv2 predictions with XGBoost trained on morphological features.
Figure~\ref{fig:morph_roc_comparison} shows ROC curves for all models: MARBERTv2 achieves AUC 0.8886,
XGBoost achieves AUC 0.7273, and hybrid ensembles range from 0.8705 to 0.8888.
The confidence-based selection strategy achieved the highest AUC (0.8888), marginally above MARBERTv2 alone.

Table~\ref{tab:morph_ensemble} summarizes the hybrid ensemble strategies.
While the weighted ensemble achieved the highest F1 (0.8213), it did not surpass our pure MARBERTv2 K-fold ensemble (0.834),
which benefits from training on multiple data folds rather than a single 90/10 split.

\begin{table}[htbp]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Ensemble Strategy} & \textbf{F1} & \textbf{Accuracy} & \textbf{AUC} \\
\midrule
Weighted (0.4 BERT / 0.6 ML) & \textbf{0.8213} & 0.831 & 0.8708 \\
Stacking Meta-Learner & 0.8150 & 0.825 & 0.8705 \\
Confidence-Based Selection & 0.8125 & 0.823 & \textbf{0.8888} \\
Adaptive Weighted & 0.8125 & 0.823 & 0.8709 \\
\bottomrule
\end{tabular}
\caption{Hybrid ensemble strategies combining MARBERTv2 and XGBoost (morphological features).}
\label{tab:morph_ensemble}
\end{table}

Figure~\ref{fig:morph_prediction_analysis} shows that 57.4\% of samples are correctly classified by both models,
24.6\% are correct only for MARBERTv2, and 9.8\% only for XGBoost.
This complementarity motivated the ensemble exploration, though the overall gains were limited.

\subsection{Subtask 2: Multi-label Polarization Type}

Table~\ref{tab:subtask2_perlabel} reports per-label precision/recall/F1 with tuned thresholds, and Table~\ref{tab:subtask2_overall}
reports overall validation scores. Our meta-ensemble improves substantially over a simple snapshot average.

\begin{table}[htbp]
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Label} & \textbf{Threshold} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
\midrule
political       & 0.33 & 0.747 & 0.881 & 0.808 \\
racial/ethnic   & 0.39 & 0.677 & 0.663 & 0.670 \\
religious       & 0.27 & 0.611 & 0.579 & 0.595 \\
gender/sexual   & 0.27 & 0.603 & 0.679 & 0.639 \\
other           & 0.44 & 0.792 & 0.600 & 0.683 \\
\bottomrule
\end{tabular}
\caption{Subtask~2 per-label validation metrics with tuned thresholds.}
\label{tab:subtask2_perlabel}
\end{table}

\begin{table}[htbp]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{System} & \textbf{Macro F1} & \textbf{Micro F1} & \textbf{Weighted F1} \\
\midrule
AraGPT2 Prompt Baseline & 0.40--0.43 & --- & --- \\
MARBERTv2 + BCE (no ensemble) & 0.35 & --- & --- \\
Snapshot Avg. (no meta) & 0.3818 & --- & --- \\
Meta-ensemble (ASL + snapshots + LR + thr.) & \textbf{0.6788} & \textbf{0.7092} & \textbf{0.7059} \\
\bottomrule
\end{tabular}
\caption{Subtask~2 overall validation metrics showing progression from baselines to final system.}
\label{tab:subtask2_overall}
\end{table}

\subsection{Subtask 3: Multi-label Manifestations}

For Subtask~3, we implemented the AraGPT2 prompt-based baseline, achieving macro F1 around 0.54 on internal validation.
Due to time constraints, we did not apply the full ASL+snapshot+stacking pipeline to this subtask.

\subsection{Alternative Methods Tried}

Throughout our experiments, we explored several approaches that were ultimately not used in our final systems:

\begin{itemize}
\item \textbf{Advanced morphological preprocessing:} Clitic segmentation and lemmatization reduced performance by removing dialectal cues.
\item \textbf{Lexicon-based toxicity features:} A curated toxic word lexicon provided weak signal compared to transformer representations.
\item \textbf{Feature engineering baselines:} Dialect-normalized text variants and TF-IDF features underperformed fine-tuned transformers.
\item \textbf{Morphological hybrid ensembles:} While interpretable, these did not consistently improve over pure MARBERTv2 ensembles.
\item \textbf{Different transformer backbones:} We briefly tested AraBERT but found MARBERTv2 superior for social media text.
\end{itemize}

\section{Discussion}

\subsection{Dataset Selection and Impact}

Restricting to the Arabic (ARB) track ensures consistent linguistic phenomena (dialects, orthography) and reduces domain mismatch
that would arise from multilingual training. However, this choice limits generalizability to other languages.
The moderate class imbalance in Subtask~1 (55\% vs.\ 45\%) was manageable with stratified sampling,
but label sparsity and multi-label imbalance in Subtasks~2--3 made macro-F1 optimization more challenging.
The relatively small dataset size (3,380 instances) motivated our use of ensembling and cross-validation to maximize data utilization.

\subsection{Approach Advantages and Disadvantages}

\paragraph{Advantages of our approach:}
\begin{itemize}
\item MARBERTv2 is pretrained on Arabic Twitter, making it well-suited for social media polarization detection.
\item K-fold ensembling reduces variance and improves robustness without requiring additional training data.
\item Asymmetric Loss effectively addresses the easy-negative problem in multi-label classification.
\item Per-label threshold tuning adapts to varying label prevalence.
\item Light preprocessing preserves stylistic cues that correlate with polarization.
\end{itemize}

\paragraph{Disadvantages and trade-offs:}
\begin{itemize}
\item K-fold ensembling increases inference cost by a factor of K.
\item The prompt-based approach for Subtasks~2--3 is sensitive to prompt format and example selection.
\item Our morphological hybrid approach added complexity without consistent gains.
\item Limited exploration of data augmentation techniques.
\end{itemize}

\subsection{Comparison to Related Work}

While direct comparison to prior POLAR shared task results is not possible (as this is the first iteration),
we contextualize our results against related Arabic NLP benchmarks:
\begin{itemize}
\item AlShenaifi et al.~\cite{alshenafi2024rasid} achieved F1 scores of 0.70--0.75 on Arabic stance detection using fine-tuned MARBERT,
comparable to our Subtask~1 results.
\item Our Subtask~2 macro F1 of 0.6788 is competitive with multi-label hate speech detection benchmarks,
though direct comparison is difficult due to different label schemas.
\item The morphological feature analysis revealed that verb count, conjunctions, and imperative forms are most predictive
(Figure~\ref{fig:morph_feature_importance}), aligning with prior observations that polarized language often uses action-oriented framing.
\end{itemize}

\subsection{Limitations}

\begin{itemize}
\item \textbf{Subtask~3 remains a baseline:} Our prompt-based approach likely underperforms fine-tuned discriminative models.
\item \textbf{Difficult cases persist:} Sarcasm, implicit polarization, and context-dependent references remain hard to classify.
\item \textbf{Internal validation bias:} Our reported metrics are on internal splits; leaderboard performance may differ due to distribution shift.
\item \textbf{Morphological hybrid limitations:} The hybrid ensemble did not provide consistent improvements, suggesting MARBERTv2 already captures relevant linguistic patterns.
\item \textbf{No external data:} We did not use additional Arabic polarization/toxicity datasets for pretraining or augmentation.
\end{itemize}

\subsection{Potential Improvements}

With more time and resources, we would pursue:
\begin{itemize}
\item \textbf{Subtask~3 optimization:} Apply the ASL+snapshot+stacking recipe that worked well for Subtask~2.
\item \textbf{Multi-task learning:} Joint training across subtasks to leverage shared representations.
\item \textbf{Data augmentation:} Back-translation, synonym replacement, or targeted augmentation for misclassified examples.
\item \textbf{Calibration methods:} Temperature scaling for better-calibrated probabilities.
\item \textbf{Dialect-aware modeling:} Auxiliary heads for dialect identification to improve predictions on dialectal content.
\item \textbf{Error-driven iteration:} Use the morphological error analysis to collect more training examples for difficult cases.
\end{itemize}

We presented ARB-focused systems for POLAR @ SemEval-2026 Task~9.
Our best Subtask~1 model is a MARBERTv2 stratified K-fold ensemble achieving macro F1 0.834 on internal validation.
For Subtask~2, we showed that combining ASL, snapshot ensembling, stacking with logistic regression, and per-label thresholding yields strong multi-label performance
(macro F1 0.6788). Future work will prioritize an optimized Subtask~3 discriminative model and multi-task training across labels.

\section{Individual Contributions}

\begin{itemize}
\item \textbf{Guanghui Ma:} preprocessing pipelines; feature engineering; Subtask~1 training and ensembling; report drafting.
\item \textbf{Nuh Al Sharafi:} morphological feature analysis; hybrid ensemble exploration; model experiments and tuning; Subtask~2 development and stacking; evaluation and literature review.
\item \textbf{Ali Khaled A. Ishtay Altamimi:} Exploratory data analysis for subtask 1 and not subtask related patterns, visualization, error analysis, and report figures.
\item \textbf{Hussein MH Nasser:} Exploratory data analysis for subtask 1 and not subtask related patterns, visualization, error analysis, and report figures.
\end{itemize}


\clearpage

\bibliographystyle{acl_natbib}
\begin{thebibliography}{10}

\bibitem[Abdul-Mageed et~al.(2021)]{abdulmageed2021arbert}
Muhammad Abdul-Mageed, AbdelRahim Elmadany, and ElMoemen B.~Nagoudi.
\newblock 2021.
\newblock ARBERT \& MARBERT: Deep Bidirectional Transformers for Arabic.
\newblock In \emph{Proceedings of ACL 2021}.

\bibitem[AlShenaifi et~al.(2024)]{alshenafi2024rasid}
N.~AlShenaifi, N.~Alangari, and H.~Al-Negheimish.
\newblock 2024.
\newblock Rasid at StanceEval: Fine-tuning MARBERT for Arabic Stance Detection.
\newblock In \emph{Proceedings of the Second Arabic Natural Language Processing Conference}, pages 828--831.

\bibitem[Al~Hariri and Abu~Farha(2024)]{alhariri2024smash}
Y.~Al~Hariri and I.~Abu~Farha.
\newblock 2024.
\newblock SMASH at StanceEval 2024: Prompt Engineering LLMs for Arabic Stance Detection.
\newblock In \emph{Proceedings of the Second Arabic Natural Language Processing Conference}, pages 800--806.

\bibitem[Antoun et~al.(2021)]{antoun2021aragpt2}
Wissam Antoun, Fady Baly, and Hazem Hajj.
\newblock 2021.
\newblock AraGPT2: Pre-trained Transformer for Arabic Language Generation.
\newblock In \emph{Proceedings of WANLP 2021}.

\bibitem[Farghaly and Shaalan(2009)]{farghaly2009arabic}
Ali Farghaly and Khaled Shaalan.
\newblock 2009.
\newblock Arabic Natural Language Processing: Challenges and Solutions.
\newblock In \emph{Proceedings of the 2009 Workshop on Arabic Natural Language Processing}.

\bibitem[Obeid et~al.(2020)]{obeid2020cameltools}
O.~Obeid, K.~Eryani, N.~Zalmout, S.~Khalifa, D.~Taji, B.~Alhafni, B.~AlKhamissi, and N.~Habash.
\newblock 2020.
\newblock CAMeL Tools: An Open Source Python Toolkit for Arabic Natural Language Processing.
\newblock In \emph{Proceedings of LREC 2020}.

\bibitem[POLAR @ SemEval(2025)]{polar-semeval-2026}
POLAR @ SemEval.
\newblock 2025.
\newblock Task Description: Detecting Multilingual, Multicultural, and Multievent Online Polarization.

\bibitem[Ridnik et~al.(2021)]{ridnik2021asl}
Tal Ridnik, Emanuel Ben-Baruch, Nadav Zamir, Asaf Noy, Itamar Friedman, Matan Protter, and Lior Wolf.
\newblock 2021.
\newblock Asymmetric Loss for Multi-Label Classification.
\newblock In \emph{Proceedings of ICCV 2021}.

\bibitem[Vasist et~al.(2023)]{vasist2023polarizing}
P.~N.~Vasist, D.~Chatterjee, and S.~Krishnan.
\newblock 2023.
\newblock The Polarizing Impact of Political Disinformation and Hate Speech: A Cross-country Configural Narrative.
\newblock \emph{Information Systems Frontiers}, pages 1--26.

\bibitem[Al-Shammari(2007)]{Shammari2007}
E.~Al-Shammari.
\newblock 2007.
\newblock Arabic Text Preprocessing for Natural Language Processing Applications.
\newblock Princess Sumaya University for Technology.

\end{thebibliography}

\clearpage

\section*{Appendix: Figures and Outputs}

% Reuse the same figure placeholders and filenames as in your milestone report.
% Ensure the ./pics directory exists (or change paths accordingly).

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{./pics/polarization.png}
\caption{Label distribution for Subtask~1 (ARB training).}
\label{fig:label_dist_bar}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{./pics/text_length.png}
\caption{Text length and word count distributions; label-wise comparisons.}
\label{fig:text_length_dist}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{./pics/multilingual.png}
\caption{Code-switching indicators: Latin characters and digit variants.}
\label{fig:latin_presence}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{./pics/social_media.png}
\caption{URLs, mentions, and hashtags by label.}
\label{fig:mention_presence}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{./pics/emojis.png}
\caption{Emoji presence and counts by label.}
\label{fig:emoji_presence}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{./pics/og vs basic vs advanced preprocess.png}
\caption{Original vs.\ basic vs.\ advanced preprocessing outputs (advanced includes clitic segmentation).}
\label{fig:preprocessing_comparison}
\end{figure}

% Morphological Feature Analysis Figures

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.85\textwidth]{./pics/XGBoost_top_features.jpg}
\caption{Top 15 morphological feature importances from XGBoost classifier. Verb count (\texttt{num\_verbs}) is the most predictive feature, followed by conjunctions and imperative forms.}
\label{fig:morph_feature_importance}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{./pics/model_ROC_comparison.jpg}
\caption{ROC curves comparing MARBERTv2, XGBoost (morphological features), and hybrid ensemble strategies. MARBERTv2 achieves AUC~0.8886, while the confidence-based hybrid slightly improves to AUC~0.8888.}
\label{fig:morph_roc_comparison}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{./pics/Confusion_Matrices_Ensemble_Models.jpg}
\caption{Confusion matrices for four hybrid ensemble strategies: weighted averaging (0.4/0.6), stacking meta-learner, confidence-based selection, and adaptive weighted. The weighted approach achieves the highest F1 (0.8213).}
\label{fig:morph_ensemble_cm}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{./pics/model_prediction_comparison.jpg}
\caption{Left: Prediction analysis showing model agreement---57.4\% of samples are correctly classified by both MARBERTv2 and XGBoost. Right: Accuracy comparison showing the best ensemble (0.8314) slightly outperforms individual models.}
\label{fig:morph_prediction_analysis}
\end{figure}

% CS445-required outputs: confusion matrix + PR curves.
% Export these figures from your notebooks and update paths below.

% TODO: Export subtask1_confusion_matrix.png from Training_8Fold_Ensemble.ipynb
% \begin{figure}[!htbp]
% \centering
% \includegraphics[width=0.75\textwidth]{./pics/subtask1_confusion_matrix.png}
% \caption{Subtask~1 confusion matrix (exported from \texttt{Training\_8Fold\_Ensemble.ipynb}).}
% \label{fig:subtask1_cm}
% \end{figure}

% TODO: Export subtask1_pr_curve.png from Training_8Fold_Ensemble.ipynb
% \begin{figure}[!htbp]
% \centering
% \includegraphics[width=0.85\textwidth]{./pics/subtask1_pr_curve.png}
% \caption{Subtask~1 precision--recall curve (exported from notebook).}
% \label{fig:subtask1_pr}
% \end{figure}

% TODO: Export subtask2_pr_curves.png from Final_subtask2.ipynb
% \begin{figure}[!htbp]
% \centering
% \includegraphics[width=0.95\textwidth]{./pics/subtask2_pr_curves.png}
% \caption{Subtask~2 per-label precision--recall curves (exported from notebook).}
% \label{fig:subtask2_pr}
% \end{figure}

\section*{Appendix: Reproducibility (Run Order)}

\begin{itemize}
\item \textbf{Basic preprocessing:} \texttt{Preprocessing\_basic.ipynb} \ $\rightarrow$ produces cleaned CSVs.
\item \textbf{Advanced preprocessing (optional/ablations):} \texttt{ArbPreAdv.py}.
\item \textbf{Feature engineering (optional/analysis):} \texttt{feature\_engineering.py}.
\item \textbf{Subtask~1 training/ensemble:} \texttt{Training\_8Fold\_Ensemble.ipynb}.
\item \textbf{Subtask~2 best system:} \texttt{Final\_subtask2.ipynb}.
\item \textbf{Prompt baselines (Subtasks~2--3):} \texttt{subtask2\_acegpt\_single\_cell.py}, \texttt{subtask3\_acegpt\_single\_cell.py}.
\end{itemize}

\end{document}
