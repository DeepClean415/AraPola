{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45285acc",
   "metadata": {},
   "source": [
    "# Polarization Manifestation Classifier (Subtask 3)\n",
    "\n",
    "**Multi-Label Classification for Arabic Text**\n",
    "\n",
    "This notebook classifies Arabic text snippets across 6 manifestation types:\n",
    "- **Stereotype**: Generalizations about groups\n",
    "- **Vilification**: Abusive or defamatory language\n",
    "- **Dehumanization**: Denying human qualities\n",
    "- **Extreme Language**: Inflammatory or aggressive language\n",
    "- **Lack of Empathy**: Dismissive of others' experiences\n",
    "- **Invalidation**: Denying legitimacy of perspectives\n",
    "\n",
    "**Approach**: ACE-GPT (AraGPT2) with few-shot learning and culturally-aware prompting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeab6be6",
   "metadata": {},
   "source": [
    "## Part 1: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a164bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.metrics import f1_score, hamming_loss, precision_recall_fscore_support\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"✓ PyTorch version: {torch.__version__}\")\n",
    "print(f\"✓ CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc60afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    model_name = \"aubmindlab/aragpt2-medium\"\n",
    "    max_length = 1024\n",
    "    temperature = 0.3  # Lower for consistency\n",
    "    top_p = 0.9\n",
    "    max_new_tokens = 100\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seed = 42\n",
    "    eval_samples = 30\n",
    "    data_path = \"../arb.csv\"\n",
    "\n",
    "config = Config()\n",
    "np.random.seed(config.seed)\n",
    "torch.manual_seed(config.seed)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Device: {config.device}\")\n",
    "print(f\"Temperature: {config.temperature}\")\n",
    "print(f\"Max Length: {config.max_length}\")\n",
    "print(f\"Eval Samples: {config.eval_samples}\")\n",
    "print(f\"Data Path: {config.data_path}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6fe113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(config.data_path)\n",
    "\n",
    "# Define manifestation types\n",
    "MANIFESTATION_TYPES = [\n",
    "    'stereotype', \n",
    "    'vilification', \n",
    "    'dehumanization', \n",
    "    'extreme_language', \n",
    "    'lack_of_empathy', \n",
    "    'invalidation'\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total samples: {len(df):,}\")\n",
    "print(f\"\\nManifestations: {MANIFESTATION_TYPES}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "for label in MANIFESTATION_TYPES:\n",
    "    positive_count = df[label].sum()\n",
    "    print(f\"  {label:20s}: {positive_count:5,} ({positive_count/len(df)*100:.1f}%)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample data:\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6465e2",
   "metadata": {},
   "source": [
    "## Part 2: Manifestation Context Mapper\n",
    "\n",
    "Maps each manifestation type to Arabic names and contextual descriptions for prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManifestationContextMapper:\n",
    "    \"\"\"Maps manifestation types to Arabic names and contextual descriptions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.manifestation_contexts = {\n",
    "            'stereotype': {\n",
    "                'ar_name': 'القوالب النمطية',\n",
    "                'context': 'القوالب النمطية هي تعميمات مبسطة عن مجموعة من الناس',\n",
    "                'examples': ['كل X كسالى', 'دائماً Y يفعلون Z', 'المعروف عن هؤلاء']\n",
    "            },\n",
    "            'vilification': {\n",
    "                'ar_name': 'التشويه والإهانة',\n",
    "                'context': 'التشويه يشمل لغة مسيئة أو تشهيرية تهاجم شخص أو مجموعة',\n",
    "                'examples': ['شتائم', 'إهانات', 'تحقير']\n",
    "            },\n",
    "            'dehumanization': {\n",
    "                'ar_name': 'التجريد من الإنسانية',\n",
    "                'context': 'التجريد من الإنسانية ينكر الصفات الإنسانية ويشبه الناس بالحيوانات',\n",
    "                'examples': ['كلاب', 'حيوانات', 'وحوش', 'قطيع']\n",
    "            },\n",
    "            'extreme_language': {\n",
    "                'ar_name': 'اللغة المتطرفة',\n",
    "                'context': 'اللغة المتطرفة تشمل كلمات عدوانية أو تحريضية قوية',\n",
    "                'examples': ['يجب القضاء على', 'لا يستحقون الحياة', 'يستحقون الموت']\n",
    "            },\n",
    "            'lack_of_empathy': {\n",
    "                'ar_name': 'عدم التعاطف',\n",
    "                'context': 'عدم التعاطف يتجاهل أو يستهزئ بمعاناة الآخرين',\n",
    "                'examples': ['يستحقون ما حدث', 'لا أهتم', 'مشكلتهم']\n",
    "            },\n",
    "            'invalidation': {\n",
    "                'ar_name': 'الإلغاء والإنكار',\n",
    "                'context': 'الإلغاء ينكر شرعية وجهات نظر أو تجارب الآخرين',\n",
    "                'examples': ['هذا غير صحيح', 'لا يحق لهم', 'رأيهم باطل']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_ar_name(self, manifestation: str) -> str:\n",
    "        \"\"\"Get Arabic name for manifestation.\"\"\"\n",
    "        return self.manifestation_contexts.get(manifestation, {}).get('ar_name', manifestation)\n",
    "    \n",
    "    def get_context(self, manifestation: str) -> str:\n",
    "        \"\"\"Get context description for manifestation.\"\"\"\n",
    "        return self.manifestation_contexts.get(manifestation, {}).get('context', '')\n",
    "    \n",
    "    def get_examples(self, manifestation: str) -> List[str]:\n",
    "        \"\"\"Get example indicators for manifestation.\"\"\"\n",
    "        return self.manifestation_contexts.get(manifestation, {}).get('examples', [])\n",
    "\n",
    "# Initialize mapper\n",
    "manifestation_mapper = ManifestationContextMapper()\n",
    "\n",
    "print(\"✓ Manifestation Context Mapper initialized\")\n",
    "print(\"\\nManifestation Arabic Names:\")\n",
    "for manif in MANIFESTATION_TYPES:\n",
    "    print(f\"  {manif:20s} → {manifestation_mapper.get_ar_name(manif)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9e3bf9",
   "metadata": {},
   "source": [
    "## Part 3: Few-Shot Example Bank\n",
    "\n",
    "Builds a bank of positive and negative examples for each manifestation type from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1520c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotExampleBank:\n",
    "    \"\"\"Builds and manages few-shot examples for each manifestation type.\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, labels: List[str]):\n",
    "        self.df = df\n",
    "        self.labels = labels\n",
    "        self.example_bank = self._build_example_bank()\n",
    "    \n",
    "    def _build_example_bank(self) -> Dict:\n",
    "        \"\"\"Build example bank with positive and negative samples.\"\"\"\n",
    "        bank = {label: {'positive': [], 'negative': []} for label in self.labels}\n",
    "        \n",
    "        for _, row in self.df.iterrows():\n",
    "            text = row['text']\n",
    "            if pd.isna(text) or len(text) > 200:  # Skip too long texts\n",
    "                continue\n",
    "            \n",
    "            for label in self.labels:\n",
    "                label_val = row[label]\n",
    "                if pd.isna(label_val):\n",
    "                    continue\n",
    "                \n",
    "                # Collect positive examples\n",
    "                if label_val == 1 and len(bank[label]['positive']) < 100:\n",
    "                    bank[label]['positive'].append({'text': text, 'label': 1})\n",
    "                \n",
    "                # Collect negative examples (where this label is 0 and no other label is 1)\n",
    "                elif label_val == 0 and len(bank[label]['negative']) < 100:\n",
    "                    other_labels_sum = sum([row[l] for l in self.labels if pd.notna(row[l]) and l != label])\n",
    "                    if other_labels_sum == 0:\n",
    "                        bank[label]['negative'].append({'text': text, 'label': 0})\n",
    "        \n",
    "        return bank\n",
    "    \n",
    "    def get_few_shot_examples(self, manifestation: str, n: int = 2) -> List[Dict]:\n",
    "        \"\"\"Get balanced few-shot examples for a manifestation.\"\"\"\n",
    "        positive_examples = self.example_bank[manifestation]['positive']\n",
    "        negative_examples = self.example_bank[manifestation]['negative']\n",
    "        \n",
    "        # Sample positive examples\n",
    "        pos_sample = np.random.choice(\n",
    "            len(positive_examples), \n",
    "            size=min(n, len(positive_examples)), \n",
    "            replace=False\n",
    "        ) if len(positive_examples) > 0 else []\n",
    "        \n",
    "        # Sample negative examples\n",
    "        neg_sample = np.random.choice(\n",
    "            len(negative_examples), \n",
    "            size=min(n, len(negative_examples)), \n",
    "            replace=False\n",
    "        ) if len(negative_examples) > 0 else []\n",
    "        \n",
    "        # Combine examples\n",
    "        examples = []\n",
    "        for idx in pos_sample:\n",
    "            examples.append(positive_examples[idx])\n",
    "        for idx in neg_sample:\n",
    "            examples.append(negative_examples[idx])\n",
    "        \n",
    "        np.random.shuffle(examples)\n",
    "        return examples\n",
    "\n",
    "# Build example bank\n",
    "few_shot_bank = FewShotExampleBank(df, MANIFESTATION_TYPES)\n",
    "\n",
    "print(\"✓ Few-Shot Example Bank built\")\n",
    "print(\"\\nExample bank statistics:\")\n",
    "for manif in MANIFESTATION_TYPES:\n",
    "    pos_count = len(few_shot_bank.example_bank[manif]['positive'])\n",
    "    neg_count = len(few_shot_bank.example_bank[manif]['negative'])\n",
    "    print(f\"  {manif:20s}: {pos_count} positive, {neg_count} negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4199b0",
   "metadata": {},
   "source": [
    "## Part 4: Improved Prompter\n",
    "\n",
    "Creates culturally-aware prompts for each manifestation type with few-shot examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5456028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedPrompter:\n",
    "    \"\"\"Creates culturally-aware prompts for manifestation detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, manifestation_mapper: ManifestationContextMapper):\n",
    "        self.manifestation_mapper = manifestation_mapper\n",
    "    \n",
    "    def get_manifestation_hints(self, manifestation: str) -> str:\n",
    "        \"\"\"Get specific guidance for each manifestation to reduce false positives.\"\"\"\n",
    "        hints = {\n",
    "            'stereotype': 'ملاحظة: يجب أن يحتوي على تعميم واضح عن مجموعة من الناس.',\n",
    "            'vilification': 'ملاحظة: يجب أن يحتوي على شتائم أو إهانات مباشرة.',\n",
    "            'dehumanization': 'ملاحظة: يجب أن يشبه الناس بالحيوانات أو ينكر إنسانيتهم.',\n",
    "            'extreme_language': 'ملاحظة: يجب أن يحتوي على لغة عدوانية أو تحريضية قوية.',\n",
    "            'lack_of_empathy': 'ملاحظة: يجب أن يتجاهل أو يستهزئ بمعاناة الآخرين.',\n",
    "            'invalidation': 'ملاحظة: يجب أن ينكر شرعية وجهات نظر أو تجارب الآخرين.'\n",
    "        }\n",
    "        return hints.get(manifestation, '')\n",
    "    \n",
    "    def create_completion_prompt(self, text: str, manifestation: str, few_shot_examples: str = \"\") -> str:\n",
    "        \"\"\"Create completion-style prompt for the model.\"\"\"\n",
    "        ar_name = self.manifestation_mapper.get_ar_name(manifestation)\n",
    "        hint = self.get_manifestation_hints(manifestation)\n",
    "        \n",
    "        prompt = f\"\"\"{few_shot_examples}{hint}\n",
    "\n",
    "النص: \"{text}\"\n",
    "السؤال: هل يحتوي على {ar_name}؟\n",
    "الإجابة:\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def format_few_shot_with_reasoning(self, examples: List[Dict], manifestation: str) -> str:\n",
    "        \"\"\"Format few-shot examples with reasoning to teach the model.\"\"\"\n",
    "        if not examples:\n",
    "            return \"\"\n",
    "        \n",
    "        ar_name = self.manifestation_mapper.get_ar_name(manifestation)\n",
    "        prompt = \"\"\n",
    "        \n",
    "        for example in examples:\n",
    "            text = example['text'][:80] + \"...\" if len(example['text']) > 80 else example['text']\n",
    "            label = example['label']\n",
    "            \n",
    "            if label == 1:\n",
    "                answer = f\"نعم، يحتوي على {ar_name}\"\n",
    "            else:\n",
    "                answer = f\"لا، لا يحتوي على {ar_name}\"\n",
    "            \n",
    "            prompt += f\"\"\"النص: \"{text}\"\n",
    "السؤال: هل يحتوي على {ar_name}؟\n",
    "الإجابة: {answer}\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def parse_response_flexible(self, response: str) -> Tuple[int, str]:\n",
    "        \"\"\"Parse model response with balanced precision/recall.\"\"\"\n",
    "        response_lower = response.lower().strip()\n",
    "        reasoning = response\n",
    "        \n",
    "        # Check first 100 characters (most important)\n",
    "        first_part = response_lower[:100]\n",
    "        \n",
    "        # Explicit patterns (highest confidence)\n",
    "        explicit_yes = ['نعم، يحتوي', 'نعم يحتوي', 'يحتوي بالفعل']\n",
    "        explicit_no = ['لا، لا يحتوي', 'لا يحتوي', 'لا يوجد']\n",
    "        \n",
    "        # Check explicit patterns first\n",
    "        for yes_phrase in explicit_yes:\n",
    "            if yes_phrase in first_part[:50]:\n",
    "                return 1, reasoning\n",
    "        \n",
    "        for no_phrase in explicit_no:\n",
    "            if no_phrase in first_part[:50]:\n",
    "                return 0, reasoning\n",
    "        \n",
    "        # Indicators with different weights\n",
    "        strong_yes = ['نعم', 'يحتوي', 'يوجد']\n",
    "        strong_no = ['لا،', 'لا.', 'لا يحتوي', 'لا يوجد']\n",
    "        weak_yes = ['موجود', 'واضح']\n",
    "        weak_no = ['غير', 'ليس']\n",
    "        \n",
    "        # Score calculation (focus on first 50 chars)\n",
    "        first_50 = first_part[:50]\n",
    "        yes_score = 0\n",
    "        no_score = 0\n",
    "        \n",
    "        # Strong indicators in first 50 chars (high weight)\n",
    "        yes_score += sum(4 for word in strong_yes if word in first_50)\n",
    "        no_score += sum(4 for word in strong_no if word in first_50)\n",
    "        \n",
    "        # Weak indicators in first 50 chars (medium weight)\n",
    "        yes_score += sum(2 for word in weak_yes if word in first_50)\n",
    "        no_score += sum(2 for word in weak_no if word in first_50)\n",
    "        \n",
    "        # Strong indicators in rest of first 100 (lower weight)\n",
    "        rest_50 = first_part[50:]\n",
    "        yes_score += sum(1 for word in strong_yes if word in rest_50)\n",
    "        no_score += sum(1 for word in strong_no if word in rest_50)\n",
    "        \n",
    "        # Decision with threshold\n",
    "        if yes_score > no_score * 1.2:\n",
    "            return 1, reasoning\n",
    "        elif no_score > yes_score:\n",
    "            return 0, reasoning\n",
    "        else:\n",
    "            return 0, reasoning  # Default to negative in ties\n",
    "\n",
    "# Initialize prompter\n",
    "improved_prompter = ImprovedPrompter(manifestation_mapper)\n",
    "\n",
    "print(\"✓ Improved Prompter initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9a7cbd",
   "metadata": {},
   "source": [
    "## Part 5: Load Model\n",
    "\n",
    "Loading AraGPT2-Medium model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5984cf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading AraGPT2 model and tokenizer...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16 if config.device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\" if config.device == \"cuda\" else None,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "if config.device == \"cpu\":\n",
    "    model = model.to(config.device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"✓ Model and tokenizer loaded successfully\")\n",
    "print(f\"✓ Model device: {next(model.parameters()).device}\")\n",
    "print(f\"✓ Model dtype: {next(model.parameters()).dtype}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abcb1a3",
   "metadata": {},
   "source": [
    "## Part 6: Manifestation Classifier\n",
    "\n",
    "Main classifier that predicts all 6 manifestations for a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678d6186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManifestationClassifier:\n",
    "    \"\"\"Multi-label classifier for polarization manifestations.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, manifestation_mapper, few_shot_bank, improved_prompter, labels):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.manifestation_mapper = manifestation_mapper\n",
    "        self.few_shot_bank = few_shot_bank\n",
    "        self.improved_prompter = improved_prompter\n",
    "        self.labels = labels\n",
    "    \n",
    "    def classify_single_manifestation(self, text: str, manifestation: str, num_few_shot: int = 2) -> Dict:\n",
    "        \"\"\"Classify a single manifestation type for the given text.\"\"\"\n",
    "        try:\n",
    "            # Get few-shot examples\n",
    "            examples = self.few_shot_bank.get_few_shot_examples(manifestation, n=num_few_shot)\n",
    "            few_shot_text = self.improved_prompter.format_few_shot_with_reasoning(examples, manifestation)\n",
    "            \n",
    "            # Create completion prompt\n",
    "            prompt = self.improved_prompter.create_completion_prompt(text, manifestation, few_shot_text)\n",
    "            \n",
    "            # Generate\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", \n",
    "                                   max_length=config.max_length, truncation=True).to(config.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs, \n",
    "                    max_new_tokens=config.max_new_tokens,\n",
    "                    temperature=config.temperature,\n",
    "                    do_sample=True,\n",
    "                    top_p=config.top_p,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    repetition_penalty=1.2\n",
    "                )\n",
    "            \n",
    "            response = self.tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], \n",
    "                                            skip_special_tokens=True)\n",
    "            \n",
    "            # Parse response\n",
    "            prediction, reasoning = self.improved_prompter.parse_response_flexible(response)\n",
    "            \n",
    "            return {\n",
    "                'manifestation': manifestation,\n",
    "                'prediction': prediction,\n",
    "                'reasoning': response\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'manifestation': manifestation, 'prediction': 0, 'reasoning': f\"Error: {str(e)}\"}\n",
    "    \n",
    "    def classify_text(self, text: str, num_few_shot: int = 2) -> Dict:\n",
    "        \"\"\"Classify all manifestation types for the given text.\"\"\"\n",
    "        results = {'text': text, 'predictions': {}, 'manifestation_details': {}}\n",
    "        \n",
    "        for manifestation in self.labels:\n",
    "            manifestation_result = self.classify_single_manifestation(text, manifestation, num_few_shot)\n",
    "            results['predictions'][manifestation] = manifestation_result['prediction']\n",
    "            results['manifestation_details'][manifestation] = manifestation_result\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def batch_classify(self, texts: List[str], num_few_shot: int = 2, show_progress: bool = True) -> List[Dict]:\n",
    "        \"\"\"Classify multiple texts.\"\"\"\n",
    "        results = []\n",
    "        iterator = tqdm(texts, desc=\"Classifying\") if show_progress else texts\n",
    "        \n",
    "        for text in iterator:\n",
    "            try:\n",
    "                result = self.classify_text(text, num_few_shot)\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing text: {e}\")\n",
    "                results.append({\n",
    "                    'text': text, \n",
    "                    'predictions': {label: 0 for label in self.labels}, \n",
    "                    'manifestation_details': {}\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize classifier\n",
    "classifier = ManifestationClassifier(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    manifestation_mapper=manifestation_mapper,\n",
    "    few_shot_bank=few_shot_bank,\n",
    "    improved_prompter=improved_prompter,\n",
    "    labels=MANIFESTATION_TYPES\n",
    ")\n",
    "\n",
    "print(\"✓ Manifestation Classifier initialized and ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe463a0",
   "metadata": {},
   "source": [
    "## Part 7: Test on Sample Texts\n",
    "\n",
    "Let's test the classifier on a few example texts to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134af47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a few sample texts\n",
    "test_texts = [\n",
    "    \"احلام انتي ونعالي ومنو انتي حتى تقيمين الفنانين الملكه احلام هههههههه البقره احلام بابا عوفي الفن لااهل الفن\",\n",
    "    \"الله يخزي احلام هي والبرنامج الخايس الي كله مصخره\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING ON SAMPLE TEXTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    print(f\"\\n{'─' * 70}\")\n",
    "    print(f\"Test {i}:\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"{'─' * 70}\")\n",
    "    \n",
    "    result = classifier.classify_text(text, num_few_shot=3)\n",
    "    \n",
    "    print(\"\\nPredictions:\")\n",
    "    for manif, pred in result['predictions'].items():\n",
    "        print(f\"  {manif:20s}: {pred}\")\n",
    "    \n",
    "    print(\"\\nDetailed reasoning:\")\n",
    "    for manif, details in result['manifestation_details'].items():\n",
    "        if details['prediction'] == 1:\n",
    "            print(f\"\\n  ✓ {manif}:\")\n",
    "            print(f\"    Response: {details['reasoning'][:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c8566c",
   "metadata": {},
   "source": [
    "## Part 8: Prepare Evaluation Dataset\n",
    "\n",
    "Select a balanced evaluation set from labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9081e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare evaluation dataset\n",
    "print(\"=\" * 70)\n",
    "print(\"PREPARING EVALUATION DATASET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get samples with at least one positive label\n",
    "df_labeled = df[(df[MANIFESTATION_TYPES].notna().all(axis=1)) & \n",
    "                (df[MANIFESTATION_TYPES].sum(axis=1) > 0)].copy()\n",
    "\n",
    "eval_size = min(config.eval_samples, len(df_labeled))\n",
    "eval_df = df_labeled.sample(n=eval_size, random_state=config.seed)\n",
    "\n",
    "print(f\"\\nEvaluation set size: {len(eval_df)} samples\")\n",
    "print(\"\\nLabel distribution in evaluation set:\")\n",
    "for label in MANIFESTATION_TYPES:\n",
    "    count = eval_df[label].sum()\n",
    "    print(f\"  {label:20s}: {count} ({count/len(eval_df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nMulti-label statistics:\")\n",
    "eval_df['label_count'] = eval_df[MANIFESTATION_TYPES].sum(axis=1)\n",
    "print(f\"  Average labels per sample: {eval_df['label_count'].mean():.2f}\")\n",
    "print(f\"  Label count distribution:\")\n",
    "print(eval_df['label_count'].value_counts().sort_index().to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aab608",
   "metadata": {},
   "source": [
    "## Part 9: Run Evaluation\n",
    "\n",
    "Classify the evaluation set and calculate performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e38ec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"RUNNING EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nProcessing {len(eval_df)} samples × {len(MANIFESTATION_TYPES)} manifestations\")\n",
    "print(f\"Total classifications: {len(eval_df) * len(MANIFESTATION_TYPES)}\")\n",
    "print(\"\\nThis may take several minutes...\\n\")\n",
    "\n",
    "# Run batch classification\n",
    "eval_results = classifier.batch_classify(\n",
    "    texts=eval_df['text'].tolist(),\n",
    "    num_few_shot=3,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Evaluation complete\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b581b6c",
   "metadata": {},
   "source": [
    "## Part 10: Calculate Metrics\n",
    "\n",
    "Calculate comprehensive performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd45636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predictions and ground truth\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for idx, row in eval_df.iterrows():\n",
    "    true_labels = [int(row[label]) for label in MANIFESTATION_TYPES]\n",
    "    y_true.append(true_labels)\n",
    "\n",
    "for result in eval_results:\n",
    "    pred_labels = [result['predictions'][label] for label in MANIFESTATION_TYPES]\n",
    "    y_pred.append(pred_labels)\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Calculate overall metrics\n",
    "hamming = hamming_loss(y_true, y_pred)\n",
    "f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "f1_samples = f1_score(y_true, y_pred, average='samples', zero_division=0)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nOverall Metrics:\")\n",
    "print(f\"  Hamming Loss:     {hamming:.4f}\")\n",
    "print(f\"  F1 Micro:         {f1_micro:.4f}\")\n",
    "print(f\"  F1 Macro:         {f1_macro:.4f}\")\n",
    "print(f\"  F1 Samples:       {f1_samples:.4f}\")\n",
    "\n",
    "# Per-manifestation metrics\n",
    "print(\"\\n\" + \"─\" * 70)\n",
    "print(\"Per-Manifestation Metrics:\")\n",
    "print(\"─\" * 70)\n",
    "print(f\"{'Manifestation':<22} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support'}\")\n",
    "print(\"─\" * 70)\n",
    "\n",
    "per_class_metrics = {}\n",
    "for i, label in enumerate(MANIFESTATION_TYPES):\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true[:, i], y_pred[:, i], average='binary', zero_division=0\n",
    "    )\n",
    "    pos_support = int(y_true[:, i].sum())\n",
    "    \n",
    "    # Format label name\n",
    "    label_name = label.replace('_', ' ').title()\n",
    "    \n",
    "    print(f\"{label_name:<22} {precision:<12.4f} {recall:<12.4f} {f1:<12.4f} {pos_support}\")\n",
    "    \n",
    "    per_class_metrics[label] = {\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1': float(f1),\n",
    "        'support': pos_support\n",
    "    }\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Store metrics\n",
    "metrics_summary = {\n",
    "    'hamming_loss': float(hamming),\n",
    "    'f1_micro': float(f1_micro),\n",
    "    'f1_macro': float(f1_macro),\n",
    "    'f1_samples': float(f1_samples),\n",
    "    'per_class': per_class_metrics\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199a6ba6",
   "metadata": {},
   "source": [
    "## Part 11: Visualize Results\n",
    "\n",
    "Create visualizations of the performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0018bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "fig.suptitle('Manifestation Classifier Performance Metrics', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Color palette\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71', '#9b59b6', '#f39c12', '#e67e22']\n",
    "\n",
    "# 1. F1 Scores by Manifestation\n",
    "ax1 = axes[0, 0]\n",
    "f1_scores = [per_class_metrics[m]['f1'] for m in MANIFESTATION_TYPES]\n",
    "labels_formatted = [m.replace('_', '\\n').title() for m in MANIFESTATION_TYPES]\n",
    "bars = ax1.bar(labels_formatted, f1_scores, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax1.set_title('F1-Score by Manifestation Type', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylabel('F1-Score', fontsize=11)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.axhline(y=f1_macro, color='red', linestyle='--', linewidth=2, label=f'F1 Macro: {f1_macro:.3f}')\n",
    "for bar, score in zip(bars, f1_scores):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "            f'{score:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Precision vs Recall\n",
    "ax2 = axes[0, 1]\n",
    "precisions = [per_class_metrics[m]['precision'] for m in MANIFESTATION_TYPES]\n",
    "recalls = [per_class_metrics[m]['recall'] for m in MANIFESTATION_TYPES]\n",
    "x = np.arange(len(MANIFESTATION_TYPES))\n",
    "width = 0.35\n",
    "ax2.bar(x - width/2, precisions, width, label='Precision', color='#3498db', alpha=0.8)\n",
    "ax2.bar(x + width/2, recalls, width, label='Recall', color='#2ecc71', alpha=0.8)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(labels_formatted, fontsize=9)\n",
    "ax2.set_title('Precision vs Recall', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylabel('Score', fontsize=11)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Support (sample count) by manifestation\n",
    "ax3 = axes[1, 0]\n",
    "supports = [per_class_metrics[m]['support'] for m in MANIFESTATION_TYPES]\n",
    "bars = ax3.bar(labels_formatted, supports, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax3.set_title('Sample Count by Manifestation (Evaluation Set)', fontsize=13, fontweight='bold')\n",
    "ax3.set_ylabel('Count', fontsize=11)\n",
    "for bar, count in zip(bars, supports):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.3,\n",
    "            str(count), ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Overall metrics comparison\n",
    "ax4 = axes[1, 1]\n",
    "overall_metrics = {\n",
    "    'F1 Micro': f1_micro,\n",
    "    'F1 Macro': f1_macro,\n",
    "    'F1 Samples': f1_samples,\n",
    "    'Hamming\\nLoss': 1 - hamming  # Invert for consistency\n",
    "}\n",
    "metric_names = list(overall_metrics.keys())\n",
    "metric_values = list(overall_metrics.values())\n",
    "bars = ax4.bar(metric_names, metric_values, color=['#9b59b6', '#e74c3c', '#f39c12', '#2ecc71'], \n",
    "               alpha=0.8, edgecolor='black')\n",
    "ax4.set_title('Overall Performance Metrics', fontsize=13, fontweight='bold')\n",
    "ax4.set_ylabel('Score', fontsize=11)\n",
    "ax4.set_ylim(0, 1)\n",
    "for bar, value in zip(bars, metric_values):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "            f'{value:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Visualizations created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048bc1d0",
   "metadata": {},
   "source": [
    "## Part 12: Save Results\n",
    "\n",
    "Save predictions and metrics to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a8a972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "results_df = eval_df.copy()\n",
    "\n",
    "# Add predicted labels\n",
    "for i, label in enumerate(MANIFESTATION_TYPES):\n",
    "    results_df[f'{label}_pred'] = y_pred[:, i]\n",
    "\n",
    "# Add prediction details\n",
    "for idx, (df_idx, result) in enumerate(zip(eval_df.index, eval_results)):\n",
    "    for manif in MANIFESTATION_TYPES:\n",
    "        results_df.loc[df_idx, f'{manif}_reasoning'] = result['manifestation_details'][manif]['reasoning'][:200]\n",
    "\n",
    "# Save to CSV\n",
    "predictions_path = 'manifestation_predictions.csv'\n",
    "results_df.to_csv(predictions_path, index=False, encoding='utf-8')\n",
    "print(f\"✓ Predictions saved to: {predictions_path}\")\n",
    "\n",
    "# Save metrics to JSON\n",
    "metrics_path = 'manifestation_metrics.json'\n",
    "with open(metrics_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics_summary, f, indent=2, ensure_ascii=False)\n",
    "print(f\"✓ Metrics saved to: {metrics_path}\")\n",
    "\n",
    "# Create summary report\n",
    "summary_report = f\"\"\"\n",
    "{'=' * 70}\n",
    "POLARIZATION MANIFESTATION CLASSIFIER - EVALUATION REPORT\n",
    "{'=' * 70}\n",
    "\n",
    "Model: {config.model_name}\n",
    "Evaluation Samples: {len(eval_df)}\n",
    "Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "{'─' * 70}\n",
    "OVERALL METRICS\n",
    "{'─' * 70}\n",
    "Hamming Loss:     {hamming:.4f}\n",
    "F1 Micro:         {f1_micro:.4f}\n",
    "F1 Macro:         {f1_macro:.4f}\n",
    "F1 Samples:       {f1_samples:.4f}\n",
    "\n",
    "{'─' * 70}\n",
    "PER-MANIFESTATION METRICS\n",
    "{'─' * 70}\n",
    "{'Manifestation':<22} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support'}\n",
    "{'─' * 70}\n",
    "\"\"\"\n",
    "\n",
    "for label in MANIFESTATION_TYPES:\n",
    "    metrics = per_class_metrics[label]\n",
    "    label_name = label.replace('_', ' ').title()\n",
    "    summary_report += f\"{label_name:<22} {metrics['precision']:<12.4f} {metrics['recall']:<12.4f} {metrics['f1']:<12.4f} {metrics['support']}\\n\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "{'=' * 70}\n",
    "\n",
    "KEY FINDINGS:\n",
    "\"\"\"\n",
    "\n",
    "# Add key findings\n",
    "best_f1 = max(per_class_metrics.items(), key=lambda x: x[1]['f1'])\n",
    "worst_f1 = min(per_class_metrics.items(), key=lambda x: x[1]['f1'])\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "- Best performing manifestation: {best_f1[0].replace('_', ' ').title()} (F1: {best_f1[1]['f1']:.4f})\n",
    "- Most challenging manifestation: {worst_f1[0].replace('_', ' ').title()} (F1: {worst_f1[1]['f1']:.4f})\n",
    "- Average F1-Score: {f1_macro:.4f}\n",
    "\n",
    "{'=' * 70}\n",
    "\"\"\"\n",
    "\n",
    "# Save report\n",
    "report_path = 'manifestation_evaluation_report.txt'\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(summary_report)\n",
    "print(f\"✓ Report saved to: {report_path}\")\n",
    "\n",
    "print(\"\\n\" + summary_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cedc55",
   "metadata": {},
   "source": [
    "## Part 13: Interactive Prediction Function\n",
    "\n",
    "Create a convenient function to classify new texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0d1cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_manifestations(text: str, show_details: bool = False) -> Dict:\n",
    "    \"\"\"\n",
    "    Predict polarization manifestations for a given Arabic text.\n",
    "    \n",
    "    Args:\n",
    "        text: Arabic text to classify\n",
    "        show_details: If True, show detailed reasoning for each manifestation\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with predictions and details\n",
    "    \"\"\"\n",
    "    result = classifier.classify_text(text, num_few_shot=3)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"MANIFESTATION PREDICTION\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nText: {text}\\n\")\n",
    "    print(\"─\" * 70)\n",
    "    print(\"Predictions:\")\n",
    "    print(\"─\" * 70)\n",
    "    \n",
    "    # Count positive predictions\n",
    "    positive_count = sum(result['predictions'].values())\n",
    "    \n",
    "    for manif, pred in result['predictions'].items():\n",
    "        ar_name = manifestation_mapper.get_ar_name(manif)\n",
    "        status = \"✓\" if pred == 1 else \"✗\"\n",
    "        label = \"YES\" if pred == 1 else \"NO\"\n",
    "        print(f\"  {status} {manif.replace('_', ' ').title():22s} ({ar_name:25s}): {label}\")\n",
    "    \n",
    "    print(\"\\n\" + \"─\" * 70)\n",
    "    print(f\"Total manifestations detected: {positive_count}/{len(MANIFESTATION_TYPES)}\")\n",
    "    \n",
    "    if show_details:\n",
    "        print(\"\\n\" + \"─\" * 70)\n",
    "        print(\"Detailed Reasoning:\")\n",
    "        print(\"─\" * 70)\n",
    "        for manif, details in result['manifestation_details'].items():\n",
    "            if details['prediction'] == 1:\n",
    "                ar_name = manifestation_mapper.get_ar_name(manif)\n",
    "                print(f\"\\n✓ {manif.replace('_', ' ').upper()} ({ar_name}):\")\n",
    "                print(f\"  {details['reasoning'][:150]}...\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✓ Interactive prediction function created\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  result = predict_manifestations('your arabic text here')\")\n",
    "print(\"  result = predict_manifestations('your arabic text here', show_details=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8406de",
   "metadata": {},
   "source": [
    "## Part 14: Example Usage\n",
    "\n",
    "Test the interactive prediction function on custom texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f198f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Test on a custom text\n",
    "custom_text = \"هؤلاء الناس كلهم كسالى ولا يستحقون أي احترام\"\n",
    "\n",
    "result = predict_manifestations(custom_text, show_details=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9176c1ac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook implements a multi-label classifier for 6 polarization manifestations in Arabic text:\n",
    "\n",
    "**Manifestation Types:**\n",
    "1. **Stereotype (القوالب النمطية)**: Generalizations about groups\n",
    "2. **Vilification (التشويه والإهانة)**: Abusive or defamatory language\n",
    "3. **Dehumanization (التجريد من الإنسانية)**: Denying human qualities\n",
    "4. **Extreme Language (اللغة المتطرفة)**: Inflammatory language\n",
    "5. **Lack of Empathy (عدم التعاطف)**: Dismissive of others' experiences\n",
    "6. **Invalidation (الإلغاء والإنكار)**: Denying legitimacy of perspectives\n",
    "\n",
    "**Key Components:**\n",
    "- **Model**: AraGPT2-Medium (aubmindlab/aragpt2-medium)\n",
    "- **Approach**: Few-shot learning with culturally-aware prompting\n",
    "- **Features**: Balanced response parsing, manifestation-specific hints\n",
    "- **Evaluation**: Multi-label metrics (Hamming Loss, F1 Micro/Macro/Samples)\n",
    "\n",
    "**Usage:**\n",
    "```python\n",
    "# Classify a single text\n",
    "result = predict_manifestations(\"your arabic text here\")\n",
    "\n",
    "# Classify with detailed reasoning\n",
    "result = predict_manifestations(\"your arabic text here\", show_details=True)\n",
    "\n",
    "# Batch classify\n",
    "results = classifier.batch_classify(texts_list, num_few_shot=3)\n",
    "```\n",
    "\n",
    "**Output Files:**\n",
    "- `manifestation_predictions.csv`: Predictions with ground truth\n",
    "- `manifestation_metrics.json`: Performance metrics in JSON format\n",
    "- `manifestation_evaluation_report.txt`: Human-readable report"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
