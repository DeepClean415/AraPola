{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d3734b",
   "metadata": {},
   "source": [
    "# English to Arabic Dialect Translation\n",
    "\n",
    "This notebook translates English polarization comments to various Arabic dialects using weighted random selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e50825",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1194c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad14166",
   "metadata": {},
   "source": [
    "## 2. Define Arabic Dialects with Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53070edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic Dialect Distribution:\n",
      "==================================================\n",
      "Standard Arabic                (ar):  30.0%\n",
      "Egyptian Arabic                (arz):  22.5%\n",
      "North Levantine Arabic         (apc):  22.5%\n",
      "Gulf Arabic                    (afb):  15.0%\n",
      "Hejazi Arabic                  (acw):  10.0%\n",
      "==================================================\n",
      "Total: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Define Arabic dialects with their codes, names, and percentage weights\n",
    "dialects = {\n",
    "    'ar': {'name': 'Standard Arabic', 'weight': 0.3},\n",
    "    'arz': {'name': 'Egyptian Arabic', 'weight': 0.225},\n",
    "    'apc': {'name': 'North Levantine Arabic', 'weight': 0.225},\n",
    "    'afb': {'name': 'Gulf Arabic', 'weight': 0.15},\n",
    "    'acw': {'name': 'Hejazi Arabic', 'weight': 0.1}\n",
    "}\n",
    "\n",
    "# Extract codes and weights for random selection\n",
    "dialect_codes = list(dialects.keys())\n",
    "dialect_weights = [dialects[code]['weight'] for code in dialect_codes]\n",
    "\n",
    "# Display dialect distribution\n",
    "print(\"Arabic Dialect Distribution:\")\n",
    "print(\"=\"*50)\n",
    "for code, info in dialects.items():\n",
    "    print(f\"{info['name']:30} ({code}): {info['weight']*100:5.1f}%\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total: {sum(dialect_weights)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bd7715",
   "metadata": {},
   "source": [
    "## 3. Load English Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fb6aa21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 2676 rows\n",
      "\n",
      "Columns: ['id', 'text', 'polarization']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>polarization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng_973938b90b0ff5d87d35a582f83f5c89</td>\n",
       "      <td>is defending imperialism in the dnd chat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng_07dfd4600426caca6e2c5883fcbea9ea</td>\n",
       "      <td>Still playing with this. I am now following Ra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng_f14519ff2302b6cd47712073f13bc461</td>\n",
       "      <td>.senate.gov Theres 3 groups out there Republic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eng_e48b7e7542faafa544ac57b64bc80daf</td>\n",
       "      <td>\"ABC MD, David Anderson, said the additional f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng_7c581fb77bce8033aeba3d6dbd6273eb</td>\n",
       "      <td>\"bad people\" I have some conservative values s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  eng_973938b90b0ff5d87d35a582f83f5c89   \n",
       "1  eng_07dfd4600426caca6e2c5883fcbea9ea   \n",
       "2  eng_f14519ff2302b6cd47712073f13bc461   \n",
       "3  eng_e48b7e7542faafa544ac57b64bc80daf   \n",
       "4  eng_7c581fb77bce8033aeba3d6dbd6273eb   \n",
       "\n",
       "                                                text  polarization  \n",
       "0           is defending imperialism in the dnd chat             0  \n",
       "1  Still playing with this. I am now following Ra...             0  \n",
       "2  .senate.gov Theres 3 groups out there Republic...             0  \n",
       "3  \"ABC MD, David Anderson, said the additional f...             0  \n",
       "4  \"bad people\" I have some conservative values s...             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the English dataset\n",
    "df = pd.read_csv('eng.csv')\n",
    "\n",
    "print(f\"Dataset loaded: {len(df)} rows\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd7da0e",
   "metadata": {},
   "source": [
    "## 4. Assign Random Dialects to Each Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1a4f0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned Dialect Distribution:\n",
      "==================================================\n",
      "Standard Arabic               :   815 ( 30.5%)\n",
      "North Levantine Arabic        :   608 ( 22.7%)\n",
      "Egyptian Arabic               :   584 ( 21.8%)\n",
      "Gulf Arabic                   :   410 ( 15.3%)\n",
      "Hejazi Arabic                 :   259 (  9.7%)\n",
      "==================================================\n",
      "Total: 2676 rows\n"
     ]
    }
   ],
   "source": [
    "# Assign a random dialect to each row based on the weighted distribution\n",
    "np.random.seed(42)  # For reproducibility\n",
    "df['dialect_code'] = np.random.choice(dialect_codes, size=len(df), p=dialect_weights)\n",
    "df['dialect_name'] = df['dialect_code'].map(lambda code: dialects[code]['name'])\n",
    "\n",
    "# Show distribution of assigned dialects\n",
    "print(\"Assigned Dialect Distribution:\")\n",
    "print(\"=\"*50)\n",
    "dialect_counts = df['dialect_name'].value_counts()\n",
    "for dialect_name, count in dialect_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"{dialect_name:30}: {count:5} ({percentage:5.1f}%)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff9f2cc",
   "metadata": {},
   "source": [
    "## 5. Define Translation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00b370e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test translation:\n",
      "['ŸÖÿ±ÿ≠ÿ®Ÿãÿßÿå ŸÉŸäŸÅ ÿ≠ÿßŸÑŸÉÿü', 'ÿßŸÑÿ∑ŸÇÿ≥ ŸÑÿ∑ŸäŸÅ ÿßŸÑŸäŸàŸÖ.']\n"
     ]
    }
   ],
   "source": [
    "# API configuration\n",
    "load_dotenv()  # loads .env into system environment\n",
    "\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "API_URL = \"https://openl-translate.p.rapidapi.com/translate/bulk\"\n",
    "\n",
    "def translate_batch(texts, target_lang, max_retries=3, retry_delay=2):\n",
    "    \"\"\"\n",
    "    Translate a batch of texts to the specified target language with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings to translate\n",
    "        target_lang: Target language code\n",
    "        max_retries: Maximum number of retry attempts\n",
    "        retry_delay: Delay between retries in seconds\n",
    "        \n",
    "    Returns:\n",
    "        List of translated texts or None if error\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"target_lang\": target_lang,\n",
    "        \"text\": texts\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"x-rapidapi-key\": API_KEY,\n",
    "        \"x-rapidapi-host\": \"openl-translate.p.rapidapi.com\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post(API_URL, json=payload, headers=headers, timeout=30)\n",
    "            \n",
    "            # Check if response is OK\n",
    "            if response.status_code == 200:\n",
    "                try:\n",
    "                    result = response.json()\n",
    "                    translated = result.get('translatedTexts', None)\n",
    "                    if translated and len(translated) == len(texts):\n",
    "                        return translated\n",
    "                    else:\n",
    "                        print(f\"Warning: Response missing translations. Got {len(translated) if translated else 0}, expected {len(texts)}\")\n",
    "                except ValueError as json_err:\n",
    "                    print(f\"JSON decode error: {json_err}\")\n",
    "                    print(f\"Response text: {response.text[:200]}\")\n",
    "            else:\n",
    "                print(f\"HTTP {response.status_code}: {response.text[:200]}\")\n",
    "            \n",
    "            # If we got here, something went wrong - retry if attempts remain\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {retry_delay} seconds... (Attempt {attempt + 2}/{max_retries})\")\n",
    "                time.sleep(retry_delay)\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Request timeout (attempt {attempt + 1}/{max_retries})\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_delay)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {type(e).__name__}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_delay)\n",
    "    \n",
    "    print(f\"‚ùå Failed after {max_retries} attempts\")\n",
    "    return None\n",
    "\n",
    "# Test with a sample\n",
    "test_texts = [\"Hello, how are you?\", \"The weather is nice today.\"]\n",
    "test_result = translate_batch(test_texts, \"ar\")\n",
    "print(\"Test translation:\")\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9623f852",
   "metadata": {},
   "source": [
    "## 6. Translate Texts in Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e54b4e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_dataframe(df, batch_size=50, delay=2, checkpoint_file='translation_checkpoint.csv'):\n",
    "    \"\"\"\n",
    "    Translate all texts in the dataframe to their assigned dialects with checkpoint saving.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'text' and 'dialect_code' columns\n",
    "        batch_size: Number of texts to translate in one API call\n",
    "        delay: Delay in seconds between API calls to avoid rate limiting\n",
    "        checkpoint_file: File to save progress periodically\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added 'translated_text' column\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Initialize translated_text column if it doesn't exist\n",
    "    if 'translated_text' not in df.columns:\n",
    "        df['translated_text'] = None\n",
    "    \n",
    "    total_translated = 0\n",
    "    total_failed = 0\n",
    "    \n",
    "    # Group by dialect to minimize API calls\n",
    "    for dialect_idx, dialect_code in enumerate(dialect_codes):\n",
    "        dialect_df = df[df['dialect_code'] == dialect_code]\n",
    "        \n",
    "        if len(dialect_df) == 0:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n[{dialect_idx+1}/{len(dialect_codes)}] Translating {len(dialect_df)} texts to {dialects[dialect_code]['name']} ({dialect_code})...\")\n",
    "\n",
    "        #dialect_df = dialect_df.reset_index(drop=True)\n",
    "        \n",
    "        indices = dialect_df.index.tolist()\n",
    "        texts = dialect_df['text'].tolist()\n",
    "        \n",
    "        dialect_translated = 0\n",
    "        dialect_failed = 0\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=f\"{dialects[dialect_code]['name'][:20]}\"):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            translated = translate_batch(batch_texts, dialect_code, max_retries=3, retry_delay=2)\n",
    "            \n",
    "            if translated and len(translated) == len(batch_texts):\n",
    "                for idx, trans_text in zip(batch_indices, translated):\n",
    "                    df.at[idx, 'translated_text'] = trans_text\n",
    "                dialect_translated += len(translated)\n",
    "                total_translated += len(translated)\n",
    "            else:\n",
    "                print(f\"\\n‚ö†Ô∏è  Failed to translate batch {i//batch_size + 1} (indices {i} to {i+len(batch_texts)-1})\")\n",
    "                dialect_failed += len(batch_texts)\n",
    "                total_failed += len(batch_texts)\n",
    "                \n",
    "                # Save checkpoint on failure\n",
    "                df.to_csv(checkpoint_file, index=False, encoding='utf-8-sig')\n",
    "                print(f\"üíæ Checkpoint saved to {checkpoint_file}\")\n",
    "            \n",
    "            # Delay to avoid rate limiting\n",
    "            if i + batch_size < len(texts):\n",
    "                time.sleep(delay)\n",
    "        \n",
    "        # Save checkpoint after completing each dialect\n",
    "        df.to_csv(checkpoint_file + f\"_{dialect_code}.csv\", index=False, encoding='utf-8-sig')\n",
    "        print(f\"‚úÖ {dialects[dialect_code]['name']}: {dialect_translated} succeeded, {dialect_failed} failed\")\n",
    "        print(f\"üíæ Checkpoint saved ({total_translated} total translated so far)\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Overall: {total_translated} succeeded, {total_failed} failed\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Save final checkpoint\n",
    "    df.to_csv(\"final_translated.csv\", index=False, encoding='utf-8-sig')\n",
    "    print(f\"üíæ Final checkpoint saved to final_translated.csv\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Note: Uncomment the line below to start translation\n",
    "# This may take a while depending on the dataset size and API limits\n",
    "# df_translated = translate_dataframe(df, batch_size=50, delay=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156d153e",
   "metadata": {},
   "source": [
    "## 7. Run Translation (Execute this cell to start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1b4dba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/5] Translating 815 texts to Standard Arabic (ar)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard Arabic: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 272/272 [37:48<00:00,  8.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Standard Arabic: 815 succeeded, 0 failed\n",
      "üíæ Checkpoint saved (815 total translated so far)\n",
      "\n",
      "[2/5] Translating 584 texts to Egyptian Arabic (arz)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Egyptian Arabic:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 86/195 [11:52<14:40,  8.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP 502: {\"messages\":\"The API is unreachable, please contact the API provider\", \"info\": \"Your Client (working) ---> Gateway (working) ---> API (not working)\"}\n",
      "Retrying in 2 seconds... (Attempt 2/3)\n",
      "HTTP 502: {\"messages\":\"The API is unreachable, please contact the API provider\", \"info\": \"Your Client (working) ---> Gateway (working) ---> API (not working)\"}\n",
      "Retrying in 2 seconds... (Attempt 3/3)\n",
      "Request timeout (attempt 3/3)\n",
      "‚ùå Failed after 3 attempts\n",
      "\n",
      "‚ö†Ô∏è  Failed to translate batch 87 (indices 258 to 260)\n",
      "üíæ Checkpoint saved to translation_checkpoint.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Egyptian Arabic: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 195/195 [29:27<00:00,  9.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Egyptian Arabic: 581 succeeded, 3 failed\n",
      "üíæ Checkpoint saved (1396 total translated so far)\n",
      "\n",
      "[3/5] Translating 608 texts to North Levantine Arabic (apc)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "North Levantine Arab: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 203/203 [28:07<00:00,  8.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ North Levantine Arabic: 608 succeeded, 0 failed\n",
      "üíæ Checkpoint saved (2004 total translated so far)\n",
      "\n",
      "[4/5] Translating 410 texts to Gulf Arabic (afb)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gulf Arabic: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 137/137 [20:30<00:00,  8.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gulf Arabic: 410 succeeded, 0 failed\n",
      "üíæ Checkpoint saved (2414 total translated so far)\n",
      "\n",
      "[5/5] Translating 259 texts to Hejazi Arabic (acw)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hejazi Arabic: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 87/87 [12:38<00:00,  8.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hejazi Arabic: 259 succeeded, 0 failed\n",
      "üíæ Checkpoint saved (2673 total translated so far)\n",
      "\n",
      "============================================================\n",
      "Overall: 2673 succeeded, 3 failed\n",
      "============================================================\n",
      "üíæ Final checkpoint saved to final_translated.csv\n",
      "\n",
      "==================================================\n",
      "Translation Complete!\n",
      "Total rows: 2676\n",
      "Successfully translated: 2673\n",
      "Failed: 3\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Start the translation process\n",
    "# Adjust batch_size and delay based on API rate limits\n",
    "df_translated = translate_dataframe(df, batch_size=3, delay=0.5)\n",
    "\n",
    "# Check for any failed translations\n",
    "failed_count = df_translated['translated_text'].isna().sum()\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Translation Complete!\")\n",
    "print(f\"Total rows: {len(df_translated)}\")\n",
    "print(f\"Successfully translated: {len(df_translated) - failed_count}\")\n",
    "print(f\"Failed: {failed_count}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c0cd66",
   "metadata": {},
   "source": [
    "## 8. Preview Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633a07e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample translations\n",
    "print(\"Sample Translations:\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Show examples from different dialects\n",
    "for dialect_code in dialect_codes[:3]:  # Show first 3 dialects as examples\n",
    "    sample = df_translated[df_translated['dialect_code'] == dialect_code].head(2)\n",
    "    \n",
    "    if len(sample) > 0:\n",
    "        print(f\"\\n{dialects[dialect_code]['name']} ({dialect_code}):\")\n",
    "        print(\"-\"*100)\n",
    "        \n",
    "        for idx, row in sample.iterrows():\n",
    "            print(f\"Original:  {row['text'][:80]}...\")\n",
    "            print(f\"Translated: {row['translated_text'][:80] if pd.notna(row['translated_text']) else 'N/A'}...\")\n",
    "            print()\n",
    "\n",
    "# Show full dataframe structure\n",
    "print(\"\\nDataFrame columns:\")\n",
    "print(df_translated.columns.tolist())\n",
    "print(f\"\\nShape: {df_translated.shape}\")\n",
    "df_translated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ec7b3",
   "metadata": {},
   "source": [
    "## 9. Save Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cb7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final dataframe with desired columns\n",
    "output_df = df_translated[['id', 'translated_text', 'polarization', 'dialect_code', 'dialect_name']].copy()\n",
    "\n",
    "# Rename 'translated_text' to 'text' for consistency\n",
    "output_df = output_df.rename(columns={'translated_text': 'text'})\n",
    "\n",
    "# Reorder columns: id, text, polarization, dialect_code, dialect_name\n",
    "output_df = output_df[['id', 'text', 'polarization', 'dialect_code', 'dialect_name']]\n",
    "\n",
    "# Save to CSV\n",
    "output_filename = 'arb_translated.csv'\n",
    "output_df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Results saved to: {output_filename}\")\n",
    "print(f\"Total rows saved: {len(output_df)}\")\n",
    "print(f\"\\nFinal DataFrame structure:\")\n",
    "output_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983101a3",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be3d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final statistics\n",
    "print(\"=\"*70)\n",
    "print(\"TRANSLATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. Dialect Distribution in Output:\")\n",
    "print(\"-\"*70)\n",
    "dialect_dist = output_df['dialect_name'].value_counts().sort_index()\n",
    "for dialect_name, count in dialect_dist.items():\n",
    "    percentage = (count / len(output_df)) * 100\n",
    "    print(f\"{dialect_name:30}: {count:5} rows ({percentage:5.1f}%)\")\n",
    "\n",
    "print(\"\\n2. Polarization Distribution:\")\n",
    "print(\"-\"*70)\n",
    "pol_dist = output_df['polarization'].value_counts().sort_index()\n",
    "for pol, count in pol_dist.items():\n",
    "    percentage = (count / len(output_df)) * 100\n",
    "    print(f\"Polarization {pol:3}: {count:5} rows ({percentage:5.1f}%)\")\n",
    "\n",
    "print(\"\\n3. Missing Translations:\")\n",
    "print(\"-\"*70)\n",
    "missing = output_df['text'].isna().sum()\n",
    "print(f\"Missing translations: {missing} ({(missing/len(output_df)*100):.2f}%)\")\n",
    "\n",
    "print(\"\\n4. Dialect Code Distribution:\")\n",
    "print(\"-\"*70)\n",
    "code_dist = output_df['dialect_code'].value_counts().sort_index()\n",
    "for code, count in code_dist.items():\n",
    "    print(f\"{code}: {count} rows\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Total Rows: {len(output_df)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc5572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# API configuration\n",
    "API_KEY = \"39d5a0f514msh69f245174f96dcep1c9f91jsnb0e702698bf4\"\n",
    "API_URL = \"https://openl-translate.p.rapidapi.com/translate/bulk\"\n",
    "\n",
    "# Test texts - one for each dialect\n",
    "test_data = [\n",
    "    {\"text\": \"Republicans want to defund public schools AND make public schools polling places? And invite bomb threats on schools? WTF. azmirror.combriefstena...\", \"dialect\": \"arz\", \"dialect_name\": \"Egyptian Arabic\"},\n",
    "    {\"text\": \"Thats what I dont understand. When did we become the snowflake pussies that the Republicans have been calling us? Where is the spine in our elected members of Congress and the military???\", \"dialect\": \"apc\", \"dialect_name\": \"North Levantine Arabic\"},\n",
    "    {\"text\": \"The Felon Trump and his chief minion Musk should be dragged out of the white house in handcuffs. A stolen election which Trump admitted to was not enough? Hes now using Putins playbook in making us less safe and less financially stable! trumpmusk treason\", \"dialect\": \"afb\", \"dialect_name\": \"Gulf Arabic\"},\n",
    "    {\"text\": \"The only thing we get from them is oil and election interference. They have the GDP of Iowa. No loss to us. Buh bye.\", \"dialect\": \"ayn\", \"dialect_name\": \"Yemeni Arabic\"}\n",
    "]\n",
    "\n",
    "print(\"Testing translation API with 4 different dialects...\")\n",
    "\n",
    "for item in test_data:\n",
    "    print(f\"\\nTranslating to {item['dialect_name']} ({item['dialect']}):\")\n",
    "    print(f\"Original: {item['text']}\")\n",
    "    \n",
    "    # Prepare API request\n",
    "    payload = {\n",
    "        \"target_lang\": item['dialect'],\n",
    "        \"text\": [item['text']]\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"x-rapidapi-key\": API_KEY,\n",
    "        \"x-rapidapi-host\": \"openl-translate.p.rapidapi.com\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(API_URL, json=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        \n",
    "        if 'translatedTexts' in result and len(result['translatedTexts']) > 0:\n",
    "            translated = result['translatedTexts'][0]\n",
    "            print(f\"Translated: {translated}\")\n",
    "            print(\"‚úì Success\")\n",
    "        else:\n",
    "            print(f\"‚úó No translation returned\")\n",
    "            print(f\"Response: {result}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error: {e}\")\n",
    "        if hasattr(e, 'response'):\n",
    "            print(f\"Response: {e.response.text}\")\n",
    "    \n",
    "    print(\"-\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
