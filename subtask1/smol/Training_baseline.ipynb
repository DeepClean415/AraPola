{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae8a0076",
   "metadata": {},
   "source": [
    "# MARBERT Baseline Training for Arabic Polarization Detection\n",
    "\n",
    "This notebook trains and evaluates MARBERT and MARBERT v2 models on the preprocessed Arabic dataset.\n",
    "\n",
    "**Dataset**: Cleaned Arabic text with polarization labels  \n",
    "**Models**: \n",
    "- MARBERT (UBC-NLP/MARBERT)\n",
    "- MARBERT v2 (UBC-NLP/MARBERTv2)\n",
    "\n",
    "**Task**: Binary classification (polarization detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7d68a5",
   "metadata": {},
   "source": [
    "## Setup: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e39405",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch scikit-learn pandas numpy tqdm accelerate -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44812d16",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673513a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce4b876",
   "metadata": {},
   "source": [
    "## Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47afe02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "# Note: Update the path if running on Colab and data is uploaded\n",
    "data_path = 'arb_clean_basic.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['polarization'].value_counts())\n",
    "print(f\"\\nClass balance:\")\n",
    "print(df['polarization'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be40ce01",
   "metadata": {},
   "source": [
    "## Split Data (90/10 Train/Test with Stratification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2959c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split with stratification to maintain class balance\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.1, \n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df['polarization']\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test set size: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTrain set class distribution:\")\n",
    "print(train_df['polarization'].value_counts())\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(test_df['polarization'].value_counts())\n",
    "\n",
    "print(f\"\\nClass balance verification:\")\n",
    "print(f\"Train: {train_df['polarization'].value_counts(normalize=True)}\")\n",
    "print(f\"Test:  {test_df['polarization'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb7228a",
   "metadata": {},
   "source": [
    "## Prepare Datasets for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df[['text', 'polarization']].rename(columns={'polarization': 'label'}))\n",
    "test_dataset = Dataset.from_pandas(test_df[['text', 'polarization']].rename(columns={'polarization': 'label'}))\n",
    "\n",
    "print(f\"‚úì Datasets prepared\")\n",
    "print(f\"Train dataset: {train_dataset}\")\n",
    "print(f\"Test dataset: {test_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb11a69f",
   "metadata": {},
   "source": [
    "## Helper Functions for Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816c0ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples, tokenizer):\n",
    "    \"\"\"Tokenize the texts\"\"\"\n",
    "    return tokenizer(examples['text'], truncation=True, padding=False, max_length=512)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute metrics for evaluation\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(trainer, test_dataset, model_name):\n",
    "    \"\"\"Evaluate model and print detailed metrics\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Evaluating {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    preds = np.argmax(predictions.predictions, axis=1)\n",
    "    labels = predictions.label_ids\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(labels, preds, target_names=['Class 0', 'Class 1']))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(labels, preds, title=f\"{model_name} - Confusion Matrix\")\n",
    "    \n",
    "    # Return metrics\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'f1': f1_score(labels, preds, average='weighted'),\n",
    "        'predictions': predictions\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b8441d",
   "metadata": {},
   "source": [
    "## Train MARBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b51acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MARBERT tokenizer and model\n",
    "marbert_model_name = \"UBC-NLP/MARBERT\"\n",
    "print(f\"Loading {marbert_model_name}...\")\n",
    "\n",
    "marbert_tokenizer = AutoTokenizer.from_pretrained(marbert_model_name)\n",
    "marbert_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    marbert_model_name,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "marbert_train_dataset = train_dataset.map(\n",
    "    lambda x: tokenize_function(x, marbert_tokenizer),\n",
    "    batched=True\n",
    ")\n",
    "marbert_test_dataset = test_dataset.map(\n",
    "    lambda x: tokenize_function(x, marbert_tokenizer),\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(marbert_train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(marbert_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dc9e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_marbert',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_marbert',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    seed=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Create Trainer\n",
    "marbert_trainer = Trainer(\n",
    "    model=marbert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=marbert_train_dataset,\n",
    "    eval_dataset=marbert_test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training MARBERT...\")\n",
    "marbert_trainer.train()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cc2f4d",
   "metadata": {},
   "source": [
    "## Evaluate MARBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68d3607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MARBERT\n",
    "marbert_results = evaluate_model(marbert_trainer, marbert_test_dataset, \"MARBERT\")\n",
    "print(f\"\\nMARBERT Accuracy: {marbert_results['accuracy']:.4f}\")\n",
    "print(f\"MARBERT F1 Score: {marbert_results['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a388ef1e",
   "metadata": {},
   "source": [
    "## Train MARBERT v2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29db3e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MARBERT v2 tokenizer and model\n",
    "marbertv2_model_name = \"UBC-NLP/MARBERTv2\"\n",
    "print(f\"Loading {marbertv2_model_name}...\")\n",
    "\n",
    "marbertv2_tokenizer = AutoTokenizer.from_pretrained(marbertv2_model_name)\n",
    "marbertv2_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    marbertv2_model_name,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "marbertv2_train_dataset = train_dataset.map(\n",
    "    lambda x: tokenize_function(x, marbertv2_tokenizer),\n",
    "    batched=True\n",
    ")\n",
    "marbertv2_test_dataset = test_dataset.map(\n",
    "    lambda x: tokenize_function(x, marbertv2_tokenizer),\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(marbertv2_train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(marbertv2_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7364754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments for MARBERT v2\n",
    "training_args_v2 = TrainingArguments(\n",
    "    output_dir='./results_marbertv2',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_marbertv2',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    seed=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Create Trainer\n",
    "marbertv2_trainer = Trainer(\n",
    "    model=marbertv2_model,\n",
    "    args=training_args_v2,\n",
    "    train_dataset=marbertv2_train_dataset,\n",
    "    eval_dataset=marbertv2_test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training MARBERT v2...\")\n",
    "marbertv2_trainer.train()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b292893c",
   "metadata": {},
   "source": [
    "## Evaluate MARBERT v2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe57d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MARBERT v2\n",
    "marbertv2_results = evaluate_model(marbertv2_trainer, marbertv2_test_dataset, \"MARBERT v2\")\n",
    "print(f\"\\nMARBERT v2 Accuracy: {marbertv2_results['accuracy']:.4f}\")\n",
    "print(f\"MARBERT v2 F1 Score: {marbertv2_results['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94079e3",
   "metadata": {},
   "source": [
    "## Compare Results: MARBERT vs MARBERT v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35539428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['MARBERT', 'MARBERT v2'],\n",
    "    'Accuracy': [marbert_results['accuracy'], marbertv2_results['accuracy']],\n",
    "    'F1 Score': [marbert_results['f1'], marbertv2_results['f1']]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "models = ['MARBERT', 'MARBERT v2']\n",
    "accuracy_scores = [marbert_results['accuracy'], marbertv2_results['accuracy']]\n",
    "ax1.bar(models, accuracy_scores, color=['#3498db', '#e74c3c'])\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Model Accuracy Comparison')\n",
    "ax1.set_ylim([0, 1])\n",
    "for i, v in enumerate(accuracy_scores):\n",
    "    ax1.text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# F1 Score comparison\n",
    "f1_scores = [marbert_results['f1'], marbertv2_results['f1']]\n",
    "ax2.bar(models, f1_scores, color=['#3498db', '#e74c3c'])\n",
    "ax2.set_ylabel('F1 Score')\n",
    "ax2.set_title('Model F1 Score Comparison')\n",
    "ax2.set_ylim([0, 1])\n",
    "for i, v in enumerate(f1_scores):\n",
    "    ax2.text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Determine winner\n",
    "if marbert_results['f1'] > marbertv2_results['f1']:\n",
    "    winner = \"MARBERT\"\n",
    "    improvement = marbert_results['f1'] - marbertv2_results['f1']\n",
    "elif marbertv2_results['f1'] > marbert_results['f1']:\n",
    "    winner = \"MARBERT v2\"\n",
    "    improvement = marbertv2_results['f1'] - marbert_results['f1']\n",
    "else:\n",
    "    winner = \"TIE\"\n",
    "    improvement = 0\n",
    "\n",
    "print(f\"\\nüèÜ Winner: {winner}\")\n",
    "if winner != \"TIE\":\n",
    "    print(f\"   Improvement: +{improvement:.4f} F1 score\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
