{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bc3a456",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b758d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets torch scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac57a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directories to path for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent))\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f369c314",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set your data paths and training parameters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e2984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DATA CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "# Path to preprocessed training data (output from preprocessing pipeline)\n",
    "TRAIN_DATA_PATH = '../processed/train_arb_bas.csv'\n",
    "\n",
    "# Path to preprocessed dev data\n",
    "DEV_DATA_PATH = '../processed/dev_arb_bas.csv'\n",
    "\n",
    "# Column names from preprocessing pipeline\n",
    "TEXT_COL = 'text_normalized'  # Use dialect-normalized text for training\n",
    "LABEL_COL = 'polarization'\n",
    "TOXIC_RATIO_COL = 'toxic_ratio'\n",
    "\n",
    "# ============================================\n",
    "# CASCADING CLASSIFIER CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "# Enable cascading classifier for pre-filtering\n",
    "USE_CASCADING = True\n",
    "\n",
    "# Thresholds for rule-based classification\n",
    "TOXIC_THRESHOLD = 0.15  # Ratio above this -> TOXIC (confidence 0.9)\n",
    "CLEAN_THRESHOLD = 0.02  # Ratio below this -> CLEAN (confidence 0.85)\n",
    "\n",
    "# ============================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "MODEL_NAME = \"UBC-NLP/MARBERTv2\"\n",
    "RANDOM_SEED = 42\n",
    "N_FOLDS = 8\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "training_config = {\n",
    "    'num_train_epochs': 4,\n",
    "    'per_device_train_batch_size': 16,\n",
    "    'per_device_eval_batch_size': 32,\n",
    "    'learning_rate': 2e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_steps': 500,\n",
    "    'logging_steps': 50,\n",
    "    'save_strategy': 'epoch',\n",
    "    'fp16': torch.cuda.is_available(),\n",
    "    'seed': RANDOM_SEED\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Train data: {TRAIN_DATA_PATH}\")\n",
    "print(f\"  Dev data: {DEV_DATA_PATH}\")\n",
    "print(f\"  Text column: {TEXT_COL}\")\n",
    "print(f\"  Use cascading: {USE_CASCADING}\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Folds: {N_FOLDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bc8a20",
   "metadata": {},
   "source": [
    "## 3. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c3c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING DATA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Size: {len(train_df)}\")\n",
    "print(f\"Columns: {train_df.columns.tolist()}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(train_df[LABEL_COL].value_counts())\n",
    "print(f\"\\nClass balance:\")\n",
    "print(train_df[LABEL_COL].value_counts(normalize=True))\n",
    "\n",
    "# Check for required columns\n",
    "required_cols = ['id', TEXT_COL, LABEL_COL, TOXIC_RATIO_COL]\n",
    "missing_cols = [col for col in required_cols if col not in train_df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"\\nâš ï¸ WARNING: Missing columns: {missing_cols}\")\n",
    "    print(\"Please run the preprocessing pipeline first!\")\n",
    "else:\n",
    "    print(f\"\\nâœ“ All required columns present\")\n",
    "\n",
    "print(f\"\\nSample data:\")\n",
    "print(train_df[[TEXT_COL, LABEL_COL, TOXIC_RATIO_COL]].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a37f471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dev data\n",
    "dev_df = pd.read_csv(DEV_DATA_PATH)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DEV DATA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Size: {len(dev_df)}\")\n",
    "print(f\"Columns: {dev_df.columns.tolist()}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(dev_df[[TEXT_COL, TOXIC_RATIO_COL]].head(3) if TOXIC_RATIO_COL in dev_df.columns else dev_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fdeb36",
   "metadata": {},
   "source": [
    "## 4. Toxic Ratio Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7619bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TOXIC_RATIO_COL in train_df.columns:\n",
    "    print(\"=\"*80)\n",
    "    print(\"TOXIC RATIO ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"\\nOverall Statistics:\")\n",
    "    print(train_df[TOXIC_RATIO_COL].describe())\n",
    "    \n",
    "    # Statistics by class\n",
    "    print(f\"\\nStatistics by Class:\")\n",
    "    for label in sorted(train_df[LABEL_COL].unique()):\n",
    "        subset = train_df[train_df[LABEL_COL] == label]\n",
    "        print(f\"\\nClass {label}:\")\n",
    "        print(f\"  Mean toxic ratio: {subset[TOXIC_RATIO_COL].mean():.4f}\")\n",
    "        print(f\"  Std toxic ratio: {subset[TOXIC_RATIO_COL].std():.4f}\")\n",
    "        print(f\"  Non-zero ratio samples: {(subset[TOXIC_RATIO_COL] > 0).sum()} ({100*(subset[TOXIC_RATIO_COL] > 0).mean():.1f}%)\")\n",
    "    \n",
    "    # Cascading classifier potential\n",
    "    if USE_CASCADING:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"CASCADING CLASSIFIER POTENTIAL\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        high_toxic = (train_df[TOXIC_RATIO_COL] > TOXIC_THRESHOLD).sum()\n",
    "        very_clean = (train_df[TOXIC_RATIO_COL] < CLEAN_THRESHOLD).sum()\n",
    "        uncertain = len(train_df) - high_toxic - very_clean\n",
    "        \n",
    "        print(f\"\\nWith thresholds: TOXIC > {TOXIC_THRESHOLD}, CLEAN < {CLEAN_THRESHOLD}\")\n",
    "        print(f\"  High toxic (rule-based TOXIC): {high_toxic} ({100*high_toxic/len(train_df):.1f}%)\")\n",
    "        print(f\"  Very clean (rule-based CLEAN): {very_clean} ({100*very_clean/len(train_df):.1f}%)\")\n",
    "        print(f\"  Uncertain (needs model): {uncertain} ({100*uncertain/len(train_df):.1f}%)\")\n",
    "        print(f\"\\nðŸ’¡ Model only needs to process {100*uncertain/len(train_df):.1f}% of samples!\")\n",
    "        \n",
    "        # Accuracy of rule-based decisions on training data\n",
    "        high_toxic_correct = ((train_df[TOXIC_RATIO_COL] > TOXIC_THRESHOLD) & (train_df[LABEL_COL] == 1)).sum()\n",
    "        very_clean_correct = ((train_df[TOXIC_RATIO_COL] < CLEAN_THRESHOLD) & (train_df[LABEL_COL] == 0)).sum()\n",
    "        \n",
    "        print(f\"\\nRule-based accuracy (on training data):\")\n",
    "        if high_toxic > 0:\n",
    "            print(f\"  High toxic -> TOXIC: {100*high_toxic_correct/high_toxic:.1f}% accurate\")\n",
    "        if very_clean > 0:\n",
    "            print(f\"  Very clean -> CLEAN: {100*very_clean_correct/very_clean:.1f}% accurate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f645ab4",
   "metadata": {},
   "source": [
    "## 5. Setup Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7e1d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"âœ“ Tokenizer loaded: {MODEL_NAME}\")\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff121b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function using normalized text\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[TEXT_COL],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "print(f\"âœ“ Tokenization function defined (using column: '{TEXT_COL}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11576e17",
   "metadata": {},
   "source": [
    "## 6. Prepare Dev Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de952e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dev dataset\n",
    "dev_dataset = Dataset.from_pandas(dev_df[[TEXT_COL]])\n",
    "\n",
    "print(\"Tokenizing dev data...\")\n",
    "dev_dataset_tokenized = dev_dataset.map(tokenize_function, batched=True)\n",
    "dev_dataset_tokenized.set_format('torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "print(f\"âœ“ Dev dataset tokenized: {len(dev_dataset_tokenized)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36ebcd1",
   "metadata": {},
   "source": [
    "## 7. Setup Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50142f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup stratified k-fold\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "print(f\"âœ“ {N_FOLDS}-Fold Cross-Validation configured\")\n",
    "print(f\"  Random Seed: {RANDOM_SEED}\")\n",
    "print(f\"  Stratification: Enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf8277",
   "metadata": {},
   "source": [
    "## 8. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d65376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute metrics for evaluation\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "print(\"âœ“ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963f98f4",
   "metadata": {},
   "source": [
    "## 9. Train 8-Fold Models\n",
    "\n",
    "Train 8 models with memory-efficient model saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f27e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for model paths and results\n",
    "model_paths = []\n",
    "fold_results = []\n",
    "\n",
    "# Create directory for saved models\n",
    "os.makedirs('./saved_models_pipeline', exist_ok=True)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Get indices for stratified k-fold\n",
    "X = train_df[TEXT_COL].values\n",
    "y = train_df[LABEL_COL].values\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"STARTING {N_FOLDS}-FOLD CROSS-VALIDATION TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FOLD {fold}/{N_FOLDS}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Split data\n",
    "    fold_train_df = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "    fold_val_df = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Training samples: {len(fold_train_df)}\")\n",
    "    print(f\"Validation samples: {len(fold_val_df)}\")\n",
    "    print(f\"Train class dist: {fold_train_df[LABEL_COL].value_counts().to_dict()}\")\n",
    "    print(f\"Val class dist: {fold_val_df[LABEL_COL].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Create datasets using normalized text\n",
    "    fold_train_dataset = Dataset.from_pandas(fold_train_df[[TEXT_COL, LABEL_COL]])\n",
    "    fold_val_dataset = Dataset.from_pandas(fold_val_df[[TEXT_COL, LABEL_COL]])\n",
    "    \n",
    "    # Tokenize\n",
    "    fold_train_dataset = fold_train_dataset.map(tokenize_function, batched=True)\n",
    "    fold_train_dataset = fold_train_dataset.rename_column(LABEL_COL, 'labels')\n",
    "    fold_train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    \n",
    "    fold_val_dataset = fold_val_dataset.map(tokenize_function, batched=True)\n",
    "    fold_val_dataset = fold_val_dataset.rename_column(LABEL_COL, 'labels')\n",
    "    fold_val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    \n",
    "    # Load fresh model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=2\n",
    "    )\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results_pipeline_fold_{fold}',\n",
    "        num_train_epochs=training_config['num_train_epochs'],\n",
    "        per_device_train_batch_size=training_config['per_device_train_batch_size'],\n",
    "        per_device_eval_batch_size=training_config['per_device_eval_batch_size'],\n",
    "        learning_rate=training_config['learning_rate'],\n",
    "        weight_decay=training_config['weight_decay'],\n",
    "        warmup_steps=training_config['warmup_steps'],\n",
    "        logging_dir=f'./logs_pipeline_fold_{fold}',\n",
    "        logging_steps=training_config['logging_steps'],\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=training_config['save_strategy'],\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        seed=training_config['seed'],\n",
    "        fp16=training_config['fp16'],\n",
    "        report_to='none'\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=fold_train_dataset,\n",
    "        eval_dataset=fold_val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\nTraining Fold {fold}...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate\n",
    "    print(f\"\\nEvaluating Fold {fold}...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    print(f\"\\nFold {fold} Results:\")\n",
    "    print(f\"  Validation Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "    print(f\"  Validation F1 Score: {eval_results['eval_f1']:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    model_save_path = f'./saved_models_pipeline/fold_{fold}_model'\n",
    "    trainer.save_model(model_save_path)\n",
    "    print(f\"  Model saved to: {model_save_path}\")\n",
    "    \n",
    "    # Store results\n",
    "    model_paths.append(model_save_path)\n",
    "    fold_results.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': eval_results['eval_accuracy'],\n",
    "        'f1': eval_results['eval_f1']\n",
    "    })\n",
    "    \n",
    "    # Detailed report\n",
    "    predictions = trainer.predict(fold_val_dataset)\n",
    "    preds = np.argmax(predictions.predictions, axis=1)\n",
    "    labels = predictions.label_ids\n",
    "    \n",
    "    print(f\"\\nClassification Report (Fold {fold}):\")\n",
    "    print(classification_report(labels, preds, target_names=['Class 0', 'Class 1']))\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    del model, trainer, fold_train_dataset, fold_val_dataset, predictions\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(f\"  GPU memory cleared\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"{N_FOLDS}-FOLD CROSS-VALIDATION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d8476",
   "metadata": {},
   "source": [
    "## 10. Cross-Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef772e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results summary\n",
    "results_df = pd.DataFrame(fold_results)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"{N_FOLDS}-FOLD CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nPer-Fold Results:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Mean Accuracy: {results_df['accuracy'].mean():.4f} Â± {results_df['accuracy'].std():.4f}\")\n",
    "print(f\"Mean F1 Score: {results_df['f1'].mean():.4f} Â± {results_df['f1'].std():.4f}\")\n",
    "\n",
    "# 95% confidence interval\n",
    "f1_mean = results_df['f1'].mean()\n",
    "f1_std = results_df['f1'].std()\n",
    "f1_ci_lower = f1_mean - 1.96 * f1_std / np.sqrt(N_FOLDS)\n",
    "f1_ci_upper = f1_mean + 1.96 * f1_std / np.sqrt(N_FOLDS)\n",
    "print(f\"\\n95% Confidence Interval: [{f1_ci_lower:.4f}, {f1_ci_upper:.4f}]\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(results_df['fold'], results_df['accuracy'])\n",
    "plt.axhline(y=results_df['accuracy'].mean(), color='r', linestyle='--', label='Mean')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy per Fold')\n",
    "plt.legend()\n",
    "plt.ylim([0.7, 1.0])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(results_df['fold'], results_df['f1'])\n",
    "plt.axhline(y=results_df['f1'].mean(), color='r', linestyle='--', label='Mean')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score per Fold')\n",
    "plt.legend()\n",
    "plt.ylim([0.7, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa65eb08",
   "metadata": {},
   "source": [
    "## 11. Ensemble Predictions with Cascading Classifier\n",
    "\n",
    "Apply cascading classifier first, then use ensemble for uncertain cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fc906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GENERATING PREDICTIONS ON DEV SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize predictions array\n",
    "n_dev = len(dev_df)\n",
    "final_predictions = np.full(n_dev, -1)  # -1 means not yet predicted\n",
    "final_confidences = np.zeros(n_dev)\n",
    "prediction_source = [''] * n_dev  # Track prediction source\n",
    "\n",
    "# Step 1: Apply cascading classifier (if enabled and toxic_ratio available)\n",
    "if USE_CASCADING and TOXIC_RATIO_COL in dev_df.columns:\n",
    "    print(\"\\nðŸ“Š Step 1: Applying Cascading Classifier...\")\n",
    "    \n",
    "    toxic_ratios = dev_df[TOXIC_RATIO_COL].values\n",
    "    \n",
    "    # High toxic -> TOXIC (class 1)\n",
    "    high_toxic_mask = toxic_ratios > TOXIC_THRESHOLD\n",
    "    final_predictions[high_toxic_mask] = 1\n",
    "    final_confidences[high_toxic_mask] = 0.9\n",
    "    for i in np.where(high_toxic_mask)[0]:\n",
    "        prediction_source[i] = 'rule_toxic'\n",
    "    \n",
    "    # Very clean -> CLEAN (class 0)\n",
    "    very_clean_mask = toxic_ratios < CLEAN_THRESHOLD\n",
    "    final_predictions[very_clean_mask] = 0\n",
    "    final_confidences[very_clean_mask] = 0.85\n",
    "    for i in np.where(very_clean_mask)[0]:\n",
    "        prediction_source[i] = 'rule_clean'\n",
    "    \n",
    "    n_rule_based = high_toxic_mask.sum() + very_clean_mask.sum()\n",
    "    print(f\"  Rule-based predictions: {n_rule_based} ({100*n_rule_based/n_dev:.1f}%)\")\n",
    "    print(f\"    - High toxic -> TOXIC: {high_toxic_mask.sum()}\")\n",
    "    print(f\"    - Very clean -> CLEAN: {very_clean_mask.sum()}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Cascading classifier disabled or toxic_ratio not available\")\n",
    "    n_rule_based = 0\n",
    "\n",
    "# Step 2: Get indices that need model prediction\n",
    "uncertain_mask = final_predictions == -1\n",
    "uncertain_indices = np.where(uncertain_mask)[0]\n",
    "n_uncertain = len(uncertain_indices)\n",
    "\n",
    "print(f\"\\nðŸ¤– Step 2: Model prediction needed for {n_uncertain} samples ({100*n_uncertain/n_dev:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a91f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Get ensemble predictions for uncertain samples\n",
    "if n_uncertain > 0:\n",
    "    print(\"\\nðŸ”„ Getting ensemble predictions for uncertain samples...\")\n",
    "    \n",
    "    # Create dataset for uncertain samples only\n",
    "    uncertain_df = dev_df.iloc[uncertain_indices].reset_index(drop=True)\n",
    "    uncertain_dataset = Dataset.from_pandas(uncertain_df[[TEXT_COL]])\n",
    "    uncertain_dataset = uncertain_dataset.map(tokenize_function, batched=True)\n",
    "    uncertain_dataset.set_format('torch', columns=['input_ids', 'attention_mask'])\n",
    "    \n",
    "    # Collect predictions from all models\n",
    "    all_model_probs = []\n",
    "    \n",
    "    for fold, model_path in enumerate(model_paths, 1):\n",
    "        print(f\"  Loading Fold {fold} model...\")\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        model.eval()\n",
    "        model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        temp_trainer = Trainer(\n",
    "            model=model,\n",
    "            args=TrainingArguments(\n",
    "                output_dir='./temp',\n",
    "                per_device_eval_batch_size=32,\n",
    "                fp16=torch.cuda.is_available(),\n",
    "                report_to='none'\n",
    "            ),\n",
    "            data_collator=data_collator\n",
    "        )\n",
    "        \n",
    "        predictions = temp_trainer.predict(uncertain_dataset)\n",
    "        probs = torch.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
    "        all_model_probs.append(probs)\n",
    "        \n",
    "        del model, temp_trainer, predictions\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Ensemble: average probabilities\n",
    "    all_model_probs = np.array(all_model_probs)\n",
    "    ensemble_probs = all_model_probs.mean(axis=0)\n",
    "    ensemble_preds = np.argmax(ensemble_probs, axis=1)\n",
    "    ensemble_conf = ensemble_probs.max(axis=1)\n",
    "    \n",
    "    # Fill in uncertain predictions\n",
    "    for i, (orig_idx, pred, conf) in enumerate(zip(uncertain_indices, ensemble_preds, ensemble_conf)):\n",
    "        final_predictions[orig_idx] = pred\n",
    "        final_confidences[orig_idx] = conf\n",
    "        prediction_source[orig_idx] = 'ensemble'\n",
    "    \n",
    "    print(f\"\\nâœ“ Ensemble predictions complete for {n_uncertain} samples\")\n",
    "\n",
    "print(f\"\\nâœ“ All predictions complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7cba1c",
   "metadata": {},
   "source": [
    "## 12. Prediction Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d494fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PREDICTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prediction distribution\n",
    "print(f\"\\nPrediction Distribution:\")\n",
    "unique, counts = np.unique(final_predictions, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  Class {label}: {count} ({100*count/len(final_predictions):.1f}%)\")\n",
    "\n",
    "# Source distribution\n",
    "print(f\"\\nPrediction Source:\")\n",
    "source_counts = pd.Series(prediction_source).value_counts()\n",
    "for source, count in source_counts.items():\n",
    "    print(f\"  {source}: {count} ({100*count/len(prediction_source):.1f}%)\")\n",
    "\n",
    "# Confidence statistics\n",
    "print(f\"\\nConfidence Statistics:\")\n",
    "print(f\"  Mean: {final_confidences.mean():.3f}\")\n",
    "print(f\"  Min: {final_confidences.min():.3f}\")\n",
    "print(f\"  Max: {final_confidences.max():.3f}\")\n",
    "\n",
    "# High confidence count\n",
    "high_conf = (final_confidences > 0.9).sum()\n",
    "print(f\"  High confidence (>0.9): {high_conf} ({100*high_conf/len(final_confidences):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dd0752",
   "metadata": {},
   "source": [
    "## 13. Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': dev_df['id'],\n",
    "    'polarization': final_predictions.astype(int)\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "output_file = 'pred_arb_pipeline_ensemble.csv'\n",
    "submission_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"âœ“ Submission file created: {output_file}\")\n",
    "print(f\"\\nFile preview:\")\n",
    "print(submission_df.head(10))\n",
    "print(f\"\\nTotal predictions: {len(submission_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888c82a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show class balance\n",
    "print(\"Predicted class distribution:\")\n",
    "print(submission_df['polarization'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c97f9c",
   "metadata": {},
   "source": [
    "## 14. Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1965b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"VERIFYING SUBMISSION FILE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "verify_df = pd.read_csv(output_file)\n",
    "\n",
    "# Check columns\n",
    "expected_columns = ['id', 'polarization']\n",
    "if list(verify_df.columns) == expected_columns:\n",
    "    print(\"âœ“ Columns are correct\")\n",
    "else:\n",
    "    print(f\"âœ— Column mismatch!\")\n",
    "\n",
    "# Check for missing values\n",
    "if verify_df.isnull().sum().sum() == 0:\n",
    "    print(\"âœ“ No missing values\")\n",
    "else:\n",
    "    print(f\"âœ— Missing values found\")\n",
    "\n",
    "# Check values\n",
    "if set(verify_df['polarization'].unique()).issubset({0, 1}):\n",
    "    print(f\"âœ“ Polarization values are valid: {sorted(verify_df['polarization'].unique())}\")\n",
    "else:\n",
    "    print(f\"âœ— Invalid values\")\n",
    "\n",
    "# Check count\n",
    "if len(verify_df) == len(dev_df):\n",
    "    print(f\"âœ“ Prediction count matches: {len(verify_df)}\")\n",
    "else:\n",
    "    print(f\"âœ— Count mismatch\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nðŸ“„ File: {output_file}\")\n",
    "print(f\"ðŸ“Š Total predictions: {len(verify_df)}\")\n",
    "print(f\"\\nðŸŽ¯ Training Performance ({N_FOLDS}-Fold CV):\")\n",
    "print(f\"   Mean F1 Score: {results_df['f1'].mean():.4f} Â± {results_df['f1'].std():.4f}\")\n",
    "if USE_CASCADING and TOXIC_RATIO_COL in dev_df.columns:\n",
    "    print(f\"\\nâš¡ Cascading Classifier:\")\n",
    "    print(f\"   Rule-based predictions: {n_rule_based} ({100*n_rule_based/n_dev:.1f}%)\")\n",
    "    print(f\"   Model predictions: {n_uncertain} ({100*n_uncertain/n_dev:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97019917",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Pipeline Integration\n",
    "- **Input:** Preprocessed data from preprocessing pipeline\n",
    "- **Text Column:** `text_normalized` (dialect-normalized text)\n",
    "- **Features Used:** `toxic_ratio` for cascading classifier\n",
    "\n",
    "### Cascading Classifier\n",
    "- **Toxic threshold:** > 0.15 â†’ Predict TOXIC (confidence 0.9)\n",
    "- **Clean threshold:** < 0.02 â†’ Predict CLEAN (confidence 0.85)\n",
    "- **Uncertain:** Falls back to ensemble model prediction\n",
    "\n",
    "### Ensemble\n",
    "- **Models:** 8 MARBERT v2 models (one per fold)\n",
    "- **Method:** Soft voting (averaged probabilities)\n",
    "- **Memory efficient:** One model loaded at a time\n",
    "\n",
    "### Output\n",
    "- **File:** `pred_arb_pipeline_ensemble.csv`\n",
    "- **Format:** CSV with `id` and `polarization` columns"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
