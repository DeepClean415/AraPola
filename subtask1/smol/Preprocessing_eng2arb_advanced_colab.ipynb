{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18512f44",
   "metadata": {},
   "source": [
    "# Advanced Preprocessing for eng2arb Dataset - Google Colab\n",
    "\n",
    "This notebook applies advanced preprocessing to the Arabic eng2arb dataset using CAMeL Tools with morphological segmentation.\n",
    "\n",
    "**Input**: `arb_eng2arb_clean_basic.csv` (basic preprocessed)\n",
    "**Output**: `arb_eng2arb_clean_advanced.csv` (advanced morphological preprocessing)\n",
    "\n",
    "**Processing includes:**\n",
    "- Morphological segmentation\n",
    "- Pronominal enclitic splitting\n",
    "- Definite article preservation\n",
    "- Particle preservation\n",
    "\n",
    "**Steps:**\n",
    "1. Install CAMeL Tools\n",
    "2. Upload preprocessing modules and dataset\n",
    "3. Download CAMeL Tools database\n",
    "4. Process the dataset\n",
    "5. Download results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3084125a",
   "metadata": {},
   "source": [
    "## ⚠️ Important Note About Dependency Warnings\n",
    "\n",
    "When installing CAMeL Tools, you may see **numpy dependency conflict warnings**. These are **safe to ignore** because:\n",
    "\n",
    "1. Google Colab manages multiple package versions simultaneously\n",
    "2. CAMeL Tools requires `numpy<2.0`, but some Colab packages need `numpy>=2.0`\n",
    "3. Both versions coexist in Colab without issues\n",
    "4. The warnings don't affect functionality\n",
    "\n",
    "**TL;DR:** The error messages about numpy are expected and won't break anything. Just proceed with the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc87aca",
   "metadata": {},
   "source": [
    "## 1. Setup - Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fc8e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install CAMeL Tools with dependency fix\n",
    "print(\"Installing CAMeL Tools...\")\n",
    "print(\"Note: Fixing numpy dependency conflicts...\\n\")\n",
    "\n",
    "# Install camel-tools which requires numpy<2.0\n",
    "!pip install -q camel-tools\n",
    "\n",
    "# Restart warning for numpy conflicts (these are usually safe to ignore in Colab)\n",
    "print(\"✓ CAMeL Tools installed successfully\")\n",
    "print(\"  (Numpy version conflicts are expected and won't affect functionality)\\n\")\n",
    "\n",
    "# Import basic libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d35fd5d",
   "metadata": {},
   "source": [
    "## 2. Upload Preprocessing Modules\n",
    "\n",
    "Upload your `ArbPreBasic.py` and `ArbPreAdv.py` files when prompted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d3ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Please upload ArbPreBasic.py and ArbPreAdv.py:\")\n",
    "print(\"You should see a 'Choose Files' button below.\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Verify files\n",
    "if 'ArbPreBasic.py' in uploaded and 'ArbPreAdv.py' in uploaded:\n",
    "    print(\"\\n✓ Both preprocessing modules uploaded successfully\")\n",
    "    print(f\"  - ArbPreBasic.py ({len(uploaded['ArbPreBasic.py'])} bytes)\")\n",
    "    print(f\"  - ArbPreAdv.py ({len(uploaded['ArbPreAdv.py'])} bytes)\")\n",
    "else:\n",
    "    print(\"\\n⚠ Warning: Make sure both files are uploaded\")\n",
    "    print(f\"Files received: {list(uploaded.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefeff7f",
   "metadata": {},
   "source": [
    "## 3. Upload Dataset\n",
    "\n",
    "Upload your `arb_eng2arb_clean_basic.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f3d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Please upload your arb_eng2arb_clean_basic.csv file:\")\n",
    "print(\"You should see a 'Choose Files' button below.\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "if 'arb_eng2arb_clean_basic.csv' in uploaded:\n",
    "    print(\"\\n✓ Dataset uploaded successfully\")\n",
    "    \n",
    "    # Show dataset info\n",
    "    df_preview = pd.read_csv('arb_eng2arb_clean_basic.csv', nrows=5)\n",
    "    print(f\"\\nDataset preview:\")\n",
    "    print(f\"  Columns: {list(df_preview.columns)}\")\n",
    "    print(f\"  First few rows:\")\n",
    "    print(df_preview)\n",
    "else:\n",
    "    print(\"\\n⚠ Error: arb_eng2arb_clean_basic.csv not found in uploaded files\")\n",
    "    print(f\"Files received: {list(uploaded.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce01a33",
   "metadata": {},
   "source": [
    "## 4. Download CAMeL Tools Database\n",
    "\n",
    "CAMeL Tools needs to download morphological database (first time only, ~50MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505a152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CAMeL Tools morphological database\n",
    "print(\"Downloading CAMeL Tools database...\")\n",
    "print(\"This is a one-time download (~50MB) and may take 1-2 minutes.\\n\")\n",
    "\n",
    "!camel_data -i morphology-db-msa-r13\n",
    "\n",
    "print(\"\\n✓ Database downloaded successfully!\")\n",
    "print(\"  Location: ~/.camel_tools/data/\")\n",
    "print(\"\\nNow ready to process the dataset...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1764a8c",
   "metadata": {},
   "source": [
    "## 5. Import Preprocessing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a656482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ArbPreAdv import ArabicAdvancedPreprocessor\n",
    "\n",
    "print(\"✓ Preprocessing modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18482114",
   "metadata": {},
   "source": [
    "## 6. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c893dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the eng2arb dataset\n",
    "data_path = 'arb_eng2arb_clean_basic.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"✓ Dataset loaded from: {data_path}\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Columns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Dataset info\n",
    "print(f\"\\nDataset Information:\")\n",
    "print(f\"  Total records: {len(df)}\")\n",
    "print(f\"  Null values:\\n{df.isnull().sum()}\")\n",
    "print(f\"\\nPolarization distribution:\")\n",
    "print(df['polarization'].value_counts())\n",
    "print(f\"\\nClass balance:\")\n",
    "print(df['polarization'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04c507a",
   "metadata": {},
   "source": [
    "## 7. Initialize Advanced Preprocessor\n",
    "\n",
    "This will load CAMeL Tools models (may take 1-2 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a783d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the advanced preprocessor\n",
    "print(\"Initializing advanced preprocessor...\")\n",
    "print(\"This may take 1-2 minutes to load CAMeL Tools models...\\n\")\n",
    "\n",
    "preprocessor = ArabicAdvancedPreprocessor(\n",
    "    split_proclitics=None,              # Don't split proclitics by default\n",
    "    split_enclitics={'PRON'},           # Split pronominal enclitics\n",
    "    keep_definite_article=True,         # Keep 'Al' attached\n",
    "    keep_particles=True,                # Keep particles attached\n",
    "    use_light_stemming=False,           # Don't apply stemming\n",
    "    use_lemmatization=False,            # Don't apply lemmatization\n",
    "    use_basic_preprocessing=False       # Data is already basically preprocessed\n",
    ")\n",
    "\n",
    "print(\"✓ Advanced preprocessor initialized\")\n",
    "print(\"\\nPreprocessor features:\")\n",
    "print(\"  • Morphological segmentation\")\n",
    "print(\"  • Pronominal enclitic splitting (e.g., كتابهم → كتاب + هم)\")\n",
    "print(\"  • Definite article preserved\")\n",
    "print(\"  • Particles preserved\")\n",
    "print(\"  • Basic preprocessing: SKIPPED (already done)\")\n",
    "\n",
    "# Test on a sample\n",
    "sample = df['text'].iloc[0]\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Sample preprocessing:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Original:\\n{sample[:100]}...\")\n",
    "print(f\"\\nPreprocessed:\\n{preprocessor.preprocess(sample)[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a98500",
   "metadata": {},
   "source": [
    "## 8. Process All Texts\n",
    "\n",
    "⚠️ **Note:** This may take several minutes depending on dataset size.\n",
    "- Dataset size: ~7000 texts\n",
    "- Estimated time: 20-30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e60c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to all texts\n",
    "print(f\"Preprocessing {len(df)} texts...\")\n",
    "print(\"This will take approximately 20-30 minutes...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Handle null values\n",
    "df['text_advanced'] = df['text'].apply(\n",
    "    lambda x: preprocessor.preprocess(x) if pd.notna(x) else x\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"✓ Preprocessing complete!\")\n",
    "print(f\"  Time taken: {elapsed_time/60:.2f} minutes\")\n",
    "print(f\"  Average: {elapsed_time/len(df):.3f} seconds per text\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d6bc0e",
   "metadata": {},
   "source": [
    "## 9. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1b2543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs preprocessed\n",
    "print(\"Comparison: Original vs Advanced Preprocessing\\n\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Original:  {df['text'].iloc[i][:80]}...\")\n",
    "    print(f\"  Advanced:  {df['text_advanced'].iloc[i][:80]}...\")\n",
    "    print(f\"  Label:     {df['polarization'].iloc[i]}\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "# Statistics\n",
    "orig_len = df['text'].str.len().mean()\n",
    "adv_len = df['text_advanced'].str.len().mean()\n",
    "orig_words = df['text'].str.split().str.len().mean()\n",
    "adv_words = df['text_advanced'].str.split().str.len().mean()\n",
    "\n",
    "print(f\"\\nText Statistics:\")\n",
    "print(f\"  Original average length:     {orig_len:.2f} chars\")\n",
    "print(f\"  Preprocessed average length: {adv_len:.2f} chars\")\n",
    "print(f\"  Character change:            {(adv_len - orig_len) / orig_len * 100:+.2f}%\")\n",
    "print(f\"\\n  Original average words:      {orig_words:.2f}\")\n",
    "print(f\"  Preprocessed average tokens: {adv_words:.2f}\")\n",
    "print(f\"  Token change:                {(adv_words - orig_words) / orig_words * 100:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e201236f",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227db55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed dataset\n",
    "output_path_full = 'arb_eng2arb_with_advanced.csv'\n",
    "df.to_csv(output_path_full, index=False)\n",
    "\n",
    "print(f\"✓ Full dataset saved!\")\n",
    "print(f\"  Location: {output_path_full}\")\n",
    "print(f\"  Columns: {list(df.columns)}\")\n",
    "\n",
    "# Also create a clean version with only the preprocessed text\n",
    "df_clean = df[['id', 'text_advanced', 'polarization']].copy()\n",
    "df_clean.rename(columns={'text_advanced': 'text'}, inplace=True)\n",
    "\n",
    "clean_output_path = 'arb_eng2arb_clean_advanced.csv'\n",
    "df_clean.to_csv(clean_output_path, index=False)\n",
    "\n",
    "print(f\"\\n✓ Clean version saved!\")\n",
    "print(f\"  Location: {clean_output_path}\")\n",
    "print(f\"  Shape: {df_clean.shape}\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Processed {len(df)} records with advanced morphological preprocessing\")\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  1. {output_path_full}\")\n",
    "print(f\"     (Contains: id, text, polarization, text_advanced)\")\n",
    "print(f\"  2. {clean_output_path}\")\n",
    "print(f\"     (Contains: id, text, polarization)\")\n",
    "print(f\"\\n⭐ Use {clean_output_path} for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9a9d89",
   "metadata": {},
   "source": [
    "## 11. Download Results\n",
    "\n",
    "Download the processed files to your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c1b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"Downloading files...\\n\")\n",
    "\n",
    "# Download full dataset with both original and processed text\n",
    "print(\"Downloading arb_eng2arb_with_advanced.csv...\")\n",
    "files.download('arb_eng2arb_with_advanced.csv')\n",
    "\n",
    "# Download clean version with only processed text\n",
    "print(\"Downloading arb_eng2arb_clean_advanced.csv...\")\n",
    "files.download('arb_eng2arb_clean_advanced.csv')\n",
    "\n",
    "print(\"\\n✓ Downloads complete!\")\n",
    "print(\"Check your browser's download folder.\")\n",
    "print(\"\\n⭐ Use arb_eng2arb_clean_advanced.csv for training!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
