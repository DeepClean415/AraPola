{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f598f6c",
   "metadata": {},
   "source": [
    "# MARBERT v2 10-Fold Cross-Validation Ensemble Training\n",
    "\n",
    "## Overview\n",
    "This notebook trains 10 MARBERT v2 models using 10-fold cross-validation and combines them into an ensemble classifier for final predictions on the dev set.\n",
    "\n",
    "## Ensemble Strategy\n",
    "- **Models:** 10 MARBERT v2 models (one per fold)\n",
    "- **Ensemble Method:** Soft voting (averaging predicted probabilities)\n",
    "- **Training:** Each model trained on 9/10 of the data\n",
    "- **Validation:** Each model evaluated on held-out 1/10 fold\n",
    "\n",
    "## Configuration\n",
    "- **Model:** MARBERT v2 (UBC-NLP/MARBERTv2)\n",
    "- **Preprocessing:** Basic (character normalization)\n",
    "- **Folds:** 10\n",
    "- **Epochs per model:** 4\n",
    "- **Learning Rate:** 2e-5\n",
    "- **Batch Size:** 16\n",
    "- **Warmup Steps:** 500\n",
    "- **Weight Decay:** 0.01\n",
    "\n",
    "## Memory Management (16GB VRAM Safe!)\n",
    "- **Strategy:** Train one model at a time, save to disk, clear GPU memory\n",
    "- **Peak VRAM:** ~4-5GB per model during training (well within 16GB limit)\n",
    "- **Inference:** Load one model at a time for predictions (~2-3GB)\n",
    "- **Disk Space:** ~2GB per model Ã— 10 = ~20GB total disk space needed\n",
    "\n",
    "## Expected Benefits\n",
    "- Reduced variance through model averaging\n",
    "- More robust predictions\n",
    "- Better generalization to unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8340c0",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a968d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed42bfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1fd183",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data\n",
    "\n",
    "**Training data:** `../train/arb_clean_basic.csv` with columns: `id`, `text`, `polarization`  \n",
    "**Dev data:** `../dev/arb_clean.csv` with columns: `id`, `text_clean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be70c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full training data\n",
    "train_df = pd.read_csv('../train/arb_clean_basic.csv')\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Columns: {train_df.columns.tolist()}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(train_df['polarization'].value_counts())\n",
    "print(f\"\\nClass balance:\")\n",
    "print(train_df['polarization'].value_counts(normalize=True))\n",
    "print(f\"\\nSample training data:\")\n",
    "print(train_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad01f73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed dev data\n",
    "dev_df = pd.read_csv('../dev/arb_clean.csv')\n",
    "\n",
    "print(f\"Dev set size: {len(dev_df)}\")\n",
    "print(f\"Columns: {dev_df.columns.tolist()}\")\n",
    "print(f\"\\nSample dev data:\")\n",
    "print(dev_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093992f7",
   "metadata": {},
   "source": [
    "## 3. Setup Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "model_name = \"UBC-NLP/MARBERTv2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Tokenizer loaded: {model_name}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccf59e0",
   "metadata": {},
   "source": [
    "## 4. Tokenization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b889219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function for training data\n",
    "def tokenize_function_train(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Tokenization function for dev data\n",
    "def tokenize_function_dev(examples):\n",
    "    return tokenizer(\n",
    "        examples['text_clean'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "print(\"âœ“ Tokenization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7cdd96",
   "metadata": {},
   "source": [
    "## 5. Prepare Dev Dataset (tokenize once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d4e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dev dataset\n",
    "dev_dataset = Dataset.from_pandas(dev_df[['text_clean']])\n",
    "\n",
    "print(\"Tokenizing dev data...\")\n",
    "dev_dataset_tokenized = dev_dataset.map(tokenize_function_dev, batched=True)\n",
    "dev_dataset_tokenized.set_format('torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "print(f\"âœ“ Dev dataset tokenized: {len(dev_dataset_tokenized)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c529d0",
   "metadata": {},
   "source": [
    "## 6. Setup 10-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628a1237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup stratified k-fold\n",
    "RANDOM_SEED = 42\n",
    "N_FOLDS = 10\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "print(f\"âœ“ 10-Fold Cross-Validation configured\")\n",
    "print(f\"  Random Seed: {RANDOM_SEED}\")\n",
    "print(f\"  Number of Folds: {N_FOLDS}\")\n",
    "print(f\"  Stratification: Enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b7556e",
   "metadata": {},
   "source": [
    "## 7. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75c4874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters (best configuration from finetuning)\n",
    "training_config = {\n",
    "    'num_train_epochs': 4,\n",
    "    'per_device_train_batch_size': 16,\n",
    "    'per_device_eval_batch_size': 32,\n",
    "    'learning_rate': 2e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_steps': 500,\n",
    "    'logging_steps': 50,\n",
    "    'save_strategy': 'epoch',\n",
    "    'fp16': torch.cuda.is_available(),\n",
    "    'seed': RANDOM_SEED\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in training_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840cd4a5",
   "metadata": {},
   "source": [
    "## 8. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20449074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute metrics for evaluation\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "print(\"âœ“ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaf6ad2",
   "metadata": {},
   "source": [
    "## 9. Train 10-Fold Models\n",
    "\n",
    "This will train 10 models, one for each fold. Each model is trained on 90% of the data and validated on the remaining 10%.\n",
    "\n",
    "**Memory Management:** Models are saved to disk and cleared from GPU memory after training to avoid OOM errors with 16GB VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40731b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "# Storage for model paths and results (NOT storing models in memory)\n",
    "model_paths = []\n",
    "fold_results = []\n",
    "\n",
    "# Create directory for saved models\n",
    "os.makedirs('./saved_models', exist_ok=True)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Get indices for stratified k-fold\n",
    "X = train_df['text'].values\n",
    "y = train_df['polarization'].values\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING 10-FOLD CROSS-VALIDATION TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FOLD {fold}/{N_FOLDS}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Split data\n",
    "    fold_train_df = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "    fold_val_df = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Training samples: {len(fold_train_df)}\")\n",
    "    print(f\"Validation samples: {len(fold_val_df)}\")\n",
    "    print(f\"Train class distribution: {fold_train_df['polarization'].value_counts().to_dict()}\")\n",
    "    print(f\"Val class distribution: {fold_val_df['polarization'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    fold_train_dataset = Dataset.from_pandas(fold_train_df[['text', 'polarization']])\n",
    "    fold_val_dataset = Dataset.from_pandas(fold_val_df[['text', 'polarization']])\n",
    "    \n",
    "    # Tokenize\n",
    "    fold_train_dataset = fold_train_dataset.map(tokenize_function_train, batched=True)\n",
    "    fold_train_dataset = fold_train_dataset.rename_column('polarization', 'labels')\n",
    "    fold_train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    \n",
    "    fold_val_dataset = fold_val_dataset.map(tokenize_function_train, batched=True)\n",
    "    fold_val_dataset = fold_val_dataset.rename_column('polarization', 'labels')\n",
    "    fold_val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    \n",
    "    # Load fresh model for this fold\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2\n",
    "    )\n",
    "    \n",
    "    # Training arguments for this fold\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results_fold_{fold}',\n",
    "        num_train_epochs=training_config['num_train_epochs'],\n",
    "        per_device_train_batch_size=training_config['per_device_train_batch_size'],\n",
    "        per_device_eval_batch_size=training_config['per_device_eval_batch_size'],\n",
    "        learning_rate=training_config['learning_rate'],\n",
    "        weight_decay=training_config['weight_decay'],\n",
    "        warmup_steps=training_config['warmup_steps'],\n",
    "        logging_dir=f'./logs_fold_{fold}',\n",
    "        logging_steps=training_config['logging_steps'],\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=training_config['save_strategy'],\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        seed=training_config['seed'],\n",
    "        fp16=training_config['fp16'],\n",
    "        report_to='none'\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=fold_train_dataset,\n",
    "        eval_dataset=fold_val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\nTraining Fold {fold}...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate on validation fold\n",
    "    print(f\"\\nEvaluating Fold {fold}...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    print(f\"\\nFold {fold} Results:\")\n",
    "    print(f\"  Validation Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "    print(f\"  Validation F1 Score: {eval_results['eval_f1']:.4f}\")\n",
    "    \n",
    "    # Save model to disk (important for memory management!)\n",
    "    model_save_path = f'./saved_models/fold_{fold}_model'\n",
    "    trainer.save_model(model_save_path)\n",
    "    print(f\"  Model saved to: {model_save_path}\")\n",
    "    \n",
    "    # Store path and results\n",
    "    model_paths.append(model_save_path)\n",
    "    fold_results.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': eval_results['eval_accuracy'],\n",
    "        'f1': eval_results['eval_f1']\n",
    "    })\n",
    "    \n",
    "    # Get detailed predictions for this fold\n",
    "    predictions = trainer.predict(fold_val_dataset)\n",
    "    preds = np.argmax(predictions.predictions, axis=1)\n",
    "    labels = predictions.label_ids\n",
    "    \n",
    "    print(f\"\\nDetailed Classification Report (Fold {fold}):\")\n",
    "    print(classification_report(labels, preds, target_names=['Class 0', 'Class 1']))\n",
    "    \n",
    "    # CRITICAL: Clear GPU memory after each fold\n",
    "    del model, trainer, fold_train_dataset, fold_val_dataset, predictions\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(f\"  GPU memory cleared\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"10-FOLD CROSS-VALIDATION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4a79a5",
   "metadata": {},
   "source": [
    "## 10. Cross-Validation Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2473c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(fold_results)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"10-FOLD CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nPer-Fold Results:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Mean Accuracy: {results_df['accuracy'].mean():.4f} Â± {results_df['accuracy'].std():.4f}\")\n",
    "print(f\"Mean F1 Score: {results_df['f1'].mean():.4f} Â± {results_df['f1'].std():.4f}\")\n",
    "print(f\"\\nMin F1 Score: {results_df['f1'].min():.4f} (Fold {results_df.loc[results_df['f1'].idxmin(), 'fold']})\")\n",
    "print(f\"Max F1 Score: {results_df['f1'].max():.4f} (Fold {results_df.loc[results_df['f1'].idxmax(), 'fold']})\")\n",
    "\n",
    "# 95% confidence interval\n",
    "f1_mean = results_df['f1'].mean()\n",
    "f1_std = results_df['f1'].std()\n",
    "f1_ci_lower = f1_mean - 1.96 * f1_std / np.sqrt(N_FOLDS)\n",
    "f1_ci_upper = f1_mean + 1.96 * f1_std / np.sqrt(N_FOLDS)\n",
    "print(f\"\\n95% Confidence Interval: [{f1_ci_lower:.4f}, {f1_ci_upper:.4f}]\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(results_df['fold'], results_df['accuracy'])\n",
    "plt.axhline(y=results_df['accuracy'].mean(), color='r', linestyle='--', label='Mean')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy per Fold')\n",
    "plt.legend()\n",
    "plt.ylim([0.7, 1.0])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(results_df['fold'], results_df['f1'])\n",
    "plt.axhline(y=results_df['f1'].mean(), color='r', linestyle='--', label='Mean')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score per Fold')\n",
    "plt.legend()\n",
    "plt.ylim([0.7, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Trained and saved {len(model_paths)} models successfully\")\n",
    "print(f\"âœ“ Models saved to disk to conserve GPU memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9636e1d",
   "metadata": {},
   "source": [
    "## 11. Ensemble Predictions on Dev Set\n",
    "\n",
    "Use soft voting: average the predicted probabilities from all 10 models, then take the argmax for final prediction.\n",
    "\n",
    "**Memory-Efficient Loading:** Load one model at a time, get predictions, then unload to save GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da06fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GENERATING ENSEMBLE PREDICTIONS ON DEV SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Storage for all predictions\n",
    "all_predictions = []\n",
    "\n",
    "# Get predictions from each model (load one at a time to save memory)\n",
    "for fold, model_path in enumerate(model_paths, 1):\n",
    "    print(f\"Loading and evaluating Fold {fold} model...\")\n",
    "    \n",
    "    # Load model from disk\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "    model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Create a temporary trainer just for prediction\n",
    "    temp_trainer = Trainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir='./temp',\n",
    "            per_device_eval_batch_size=32,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            report_to='none'\n",
    "        ),\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "    \n",
    "    # Get predictions (logits)\n",
    "    predictions = temp_trainer.predict(dev_dataset_tokenized)\n",
    "    \n",
    "    # Convert logits to probabilities using softmax\n",
    "    probs = torch.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
    "    all_predictions.append(probs)\n",
    "    \n",
    "    print(f\"  Shape: {probs.shape}, Class 1 prob range: [{probs[:, 1].min():.3f}, {probs[:, 1].max():.3f}]\")\n",
    "    \n",
    "    # CRITICAL: Clear GPU memory after each model\n",
    "    del model, temp_trainer, predictions\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Convert to numpy array for easier manipulation\n",
    "all_predictions = np.array(all_predictions)  # Shape: (10, n_samples, 2)\n",
    "\n",
    "print(f\"\\nâœ“ Collected predictions from all {len(model_paths)} models\")\n",
    "print(f\"  Prediction array shape: {all_predictions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cf74ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble: Average probabilities across all models\n",
    "ensemble_probs = all_predictions.mean(axis=0)  # Shape: (n_samples, 2)\n",
    "\n",
    "# Get final predictions\n",
    "ensemble_predictions = np.argmax(ensemble_probs, axis=1)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ENSEMBLE PREDICTIONS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total predictions: {len(ensemble_predictions)}\")\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "unique, counts = np.unique(ensemble_predictions, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  Class {label}: {count} ({count/len(ensemble_predictions)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nEnsemble probability statistics:\")\n",
    "print(f\"  Class 0 prob - Mean: {ensemble_probs[:, 0].mean():.3f}, Std: {ensemble_probs[:, 0].std():.3f}\")\n",
    "print(f\"  Class 1 prob - Mean: {ensemble_probs[:, 1].mean():.3f}, Std: {ensemble_probs[:, 1].std():.3f}\")\n",
    "\n",
    "# Confidence analysis\n",
    "confidence = ensemble_probs.max(axis=1)\n",
    "print(f\"\\nPrediction confidence:\")\n",
    "print(f\"  Mean: {confidence.mean():.3f}\")\n",
    "print(f\"  Min: {confidence.min():.3f}\")\n",
    "print(f\"  Max: {confidence.max():.3f}\")\n",
    "print(f\"  Median: {np.median(confidence):.3f}\")\n",
    "\n",
    "# High confidence predictions\n",
    "high_conf_threshold = 0.9\n",
    "high_conf_count = (confidence > high_conf_threshold).sum()\n",
    "print(f\"\\nHigh confidence predictions (>{high_conf_threshold}): {high_conf_count} ({high_conf_count/len(confidence)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8540a907",
   "metadata": {},
   "source": [
    "## 12. Create Submission File\n",
    "\n",
    "Following the submission guidelines:\n",
    "- File format: CSV with columns `id` and `polarization`\n",
    "- File name: `pred_arb.csv`\n",
    "- Values: 0 or 1 for polarization labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc63ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': dev_df['id'],\n",
    "    'polarization': ensemble_predictions\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "output_file = 'pred_arb_ensemble.csv'\n",
    "submission_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"âœ“ Submission file created: {output_file}\")\n",
    "print(f\"\\nFile preview:\")\n",
    "print(submission_df.head(10))\n",
    "print(f\"\\nFile preview (tail):\")\n",
    "print(submission_df.tail(10))\n",
    "print(f\"\\nTotal predictions: {len(submission_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee831f6",
   "metadata": {},
   "source": [
    "## 13. Validation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360e68d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify submission file format\n",
    "print(\"=\"*80)\n",
    "print(\"VERIFYING SUBMISSION FILE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Read the file back\n",
    "verify_df = pd.read_csv(output_file)\n",
    "\n",
    "# Check columns\n",
    "expected_columns = ['id', 'polarization']\n",
    "if list(verify_df.columns) == expected_columns:\n",
    "    print(\"âœ“ Columns are correct: ['id', 'polarization']\")\n",
    "else:\n",
    "    print(f\"âœ— Column mismatch! Expected {expected_columns}, got {list(verify_df.columns)}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing = verify_df.isnull().sum()\n",
    "if missing.sum() == 0:\n",
    "    print(\"âœ“ No missing values\")\n",
    "else:\n",
    "    print(f\"âœ— Missing values found:\\n{missing}\")\n",
    "\n",
    "# Check polarization values\n",
    "unique_values = verify_df['polarization'].unique()\n",
    "if set(unique_values).issubset({0, 1}):\n",
    "    print(f\"âœ“ Polarization values are valid: {sorted(unique_values)}\")\n",
    "else:\n",
    "    print(f\"âœ— Invalid polarization values: {unique_values}\")\n",
    "\n",
    "# Check number of predictions\n",
    "if len(verify_df) == len(dev_df):\n",
    "    print(f\"âœ“ Number of predictions matches dev set: {len(verify_df)}\")\n",
    "else:\n",
    "    print(f\"âœ— Prediction count mismatch! Expected {len(dev_df)}, got {len(verify_df)}\")\n",
    "\n",
    "# Check IDs match\n",
    "if (verify_df['id'] == dev_df['id']).all():\n",
    "    print(\"âœ“ All IDs match the dev set\")\n",
    "else:\n",
    "    print(\"âœ— ID mismatch detected!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUBMISSION FILE READY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nðŸ“„ File: {output_file}\")\n",
    "print(f\"ðŸ“‹ Format: CSV with columns 'id' and 'polarization'\")\n",
    "print(f\"ðŸ“Š Predictions: {len(verify_df)}\")\n",
    "print(f\"\\nðŸŽ¯ Training Performance (10-Fold CV):\")\n",
    "print(f\"   Mean F1 Score: {results_df['f1'].mean():.4f} Â± {results_df['f1'].std():.4f}\")\n",
    "print(f\"   95% CI: [{f1_ci_lower:.4f}, {f1_ci_upper:.4f}]\")\n",
    "print(f\"\\nðŸ’¡ Ensemble Method: Soft voting (averaged probabilities from 10 models)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead6a7d",
   "metadata": {},
   "source": [
    "## 14. Comparison with Individual Models\n",
    "\n",
    "Let's see how each individual model would perform compared to the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d0ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"INDIVIDUAL MODEL PREDICTIONS ON DEV SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "individual_predictions = []\n",
    "\n",
    "for fold in range(N_FOLDS):\n",
    "    # Get predictions from this model (argmax of probabilities)\n",
    "    fold_preds = np.argmax(all_predictions[fold], axis=1)\n",
    "    individual_predictions.append(fold_preds)\n",
    "    \n",
    "    # Distribution for this fold\n",
    "    unique, counts = np.unique(fold_preds, return_counts=True)\n",
    "    dist_str = \", \".join([f\"Class {label}: {count}\" for label, count in zip(unique, counts)])\n",
    "    print(f\"Fold {fold+1}: {dist_str}\")\n",
    "\n",
    "print(f\"\\nEnsemble: \", end=\"\")\n",
    "unique, counts = np.unique(ensemble_predictions, return_counts=True)\n",
    "dist_str = \", \".join([f\"Class {label}: {count}\" for label, count in zip(unique, counts)])\n",
    "print(dist_str)\n",
    "\n",
    "# Calculate agreement between models\n",
    "individual_predictions = np.array(individual_predictions)  # Shape: (10, n_samples)\n",
    "\n",
    "# For each sample, count how many models agree with the ensemble\n",
    "agreement_counts = (individual_predictions == ensemble_predictions).sum(axis=0)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"MODEL AGREEMENT ANALYSIS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Mean agreement: {agreement_counts.mean():.2f} / 10 models\")\n",
    "print(f\"Min agreement: {agreement_counts.min()} models\")\n",
    "print(f\"Max agreement: {agreement_counts.max()} models\")\n",
    "print(f\"\\nAgreement distribution:\")\n",
    "for i in range(N_FOLDS+1):\n",
    "    count = (agreement_counts == i).sum()\n",
    "    if count > 0:\n",
    "        print(f\"  {i} models agree: {count} samples ({count/len(agreement_counts)*100:.1f}%)\")\n",
    "\n",
    "# Unanimous predictions\n",
    "unanimous = (agreement_counts == N_FOLDS).sum()\n",
    "print(f\"\\nUnanimous predictions (all 10 models agree): {unanimous} ({unanimous/len(agreement_counts)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb34d97",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Model Configuration\n",
    "- **Base Model:** MARBERT v2 (UBC-NLP/MARBERTv2)\n",
    "- **Preprocessing:** Basic (character normalization, diacritic removal, tatweel removal)\n",
    "- **Training Strategy:** 10-Fold Stratified Cross-Validation\n",
    "- **Number of Models:** 10 (one per fold)\n",
    "- **Ensemble Method:** Soft Voting (averaged predicted probabilities)\n",
    "\n",
    "### Hyperparameters\n",
    "- **Epochs:** 4\n",
    "- **Learning Rate:** 2e-5\n",
    "- **Batch Size:** 16\n",
    "- **Warmup Steps:** 500\n",
    "- **Weight Decay:** 0.01\n",
    "- **Random Seed:** 42\n",
    "\n",
    "### Training Data\n",
    "- **Total Samples:** Full training set\n",
    "- **Folds:** 10\n",
    "- **Train per fold:** 90% of data\n",
    "- **Validation per fold:** 10% of data\n",
    "- **Stratification:** Enabled (maintains class balance)\n",
    "\n",
    "### Performance\n",
    "- **Cross-Validation Mean F1:** Reported above\n",
    "- **Cross-Validation Std:** Reported above\n",
    "- **95% Confidence Interval:** Reported above\n",
    "\n",
    "### Output\n",
    "- **File:** `pred_arb_ensemble.csv`\n",
    "- **Format:** Two columns (`id`, `polarization`)\n",
    "- **Language:** Arabic (arb)\n",
    "- **Method:** Ensemble of 10 models with soft voting\n",
    "- **Ready for submission to Codabench Subtask 1**\n",
    "\n",
    "### Advantages of 10-Fold Ensemble\n",
    "1. **Reduced Variance:** Averaging predictions from 10 models reduces overfitting\n",
    "2. **Better Generalization:** Each model sees different validation data\n",
    "3. **Robust Predictions:** Ensemble captures broader patterns in the data\n",
    "4. **Full Data Utilization:** Every sample used for both training and validation\n",
    "5. **Confidence Estimation:** Agreement between models indicates prediction confidence"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
