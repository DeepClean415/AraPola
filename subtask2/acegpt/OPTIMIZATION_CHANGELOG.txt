================================================================================
ACEGPT CLASSIFIER OPTIMIZATION CHANGELOG
================================================================================
Date: January 3, 2026
Project: AraPola Subtask 2 - Multi-label Arabic Polarization Detection
Model: aubmindlab/aragpt2-medium

================================================================================
PROBLEM IDENTIFIED
================================================================================

Initial F1 Score: ~0.20 (very low)

Root Causes:
1. Complex Chain-of-Thought prompts that AraGPT2 (generative model) couldn't follow
2. Model expected structured output but generated natural Arabic text
3. Weak parsing logic that counted words incorrectly
4. Few-shot examples showed only labels without reasoning
5. No category-specific guidance

================================================================================
OPTIMIZATION HISTORY
================================================================================

────────────────────────────────────────────────────────────────────────────
VERSION 1: ORIGINAL (PROBLEMATIC)
────────────────────────────────────────────────────────────────────────────

Configuration:
- Temperature: 0.7
- Top_p: 0.85
- Max_new_tokens: 256
- Few-shot examples: 2

Prompt Structure:
```
المهمة: تحديد إذا كان النص يحتوي على {category}
السياق: {context}
{few_shot_examples}
الآن حلل هذا النص: "{text}"
خطوات التحليل:
1. الموضوع الرئيسي
2. الكلمات المفتاحية
3. النبرة (سلبية/إيجابية)
4. السياق الثقافي
5. القرار النهائي: نعم أو لا
التحليل:
```

Few-Shot Format:
```
1. "{text}" → نعم
2. "{text}" → لا
```

Parsing:
- Simple word counting (نعم vs لا)
- No context awareness
- Fallback: count positive/negative indicators

Results:
- F1 Macro: ~0.20
- High false negatives
- Model couldn't follow structured format

Issues:
❌ Too complex for generative model
❌ No reasoning in examples
❌ Poor parsing logic
❌ Generic prompts for all categories

────────────────────────────────────────────────────────────────────────────
VERSION 2: SIMPLIFIED PROMPTING
────────────────────────────────────────────────────────────────────────────

Configuration:
- Temperature: 0.5 → 0.3 (more deterministic)
- Top_p: 0.85 → 0.9 (slightly more diverse)
- Max_new_tokens: 256 → 100 (shorter, focused)
- Few-shot examples: 2

Changes:

1. COMPLETION-STYLE PROMPTS
   Old:
   ```
   المهمة: تحديد...
   خطوات التحليل: 1. 2. 3...
   ```
   
   New:
   ```
   {few_shot_examples}
   النص: "{text}"
   السؤال: هل يحتوي على {category}؟
   الإجابة:
   ```
   
   Benefit: Model naturally continues pattern instead of following complex steps

2. FEW-SHOT WITH REASONING
   Old:
   ```
   1. "{text}" → نعم
   ```
   
   New:
   ```
   النص: "{text}"
   السؤال: هل يحتوي على الاستقطاب السياسي؟
   الإجابة: نعم، يحتوي على الاستقطاب السياسي
   ```
   
   Benefit: Shows model HOW to reason, not just what to output

Results:
- F1 Macro: 0.40 (+100% improvement!)
- Better recall but precision still low
- Too many false positives

────────────────────────────────────────────────────────────────────────────
VERSION 3: CATEGORY-SPECIFIC HINTS
────────────────────────────────────────────────────────────────────────────

Configuration:
- Temperature: 0.3
- Top_p: 0.9
- Max_new_tokens: 100
- Few-shot examples: 2

Changes:

3. ADDED CATEGORY-SPECIFIC HINTS
   ```
   Political: "يجب أن يتعلق بالسياسة أو الحكومة أو الأحزاب السياسية"
   Racial/ethnic: "يجب أن يكون هناك تمييز واضح ضد جنسية أو عرق معين"
   Religious: "يجب أن يحتوي على كلمات مثل 'كافر' أو 'رافضي' أو هجوم مباشر"
   Gender/sexual: "يجب أن يكون هناك تمييز واضح ضد جنس معين"
   Other: "تأكد أنه لا يندرج تحت السياسة أو العرق أو الدين أو الجنس"
   ```
   
   New Prompt:
   ```
   {few_shot_examples}
   {category_hint}
   
   النص: "{text}"
   السؤال: هل يحتوي على {category}؟
   الإجابة:
   ```
   
   Benefit: Gives model explicit criteria for each category

Results:
- F1 Macro: 0.4244 (+6% improvement)
- Better precision on gender/sexual (+55%) and other (+50%)
- Still struggling with religious precision (0.18)

────────────────────────────────────────────────────────────────────────────
VERSION 4: BALANCED PARSING (FINAL)
────────────────────────────────────────────────────────────────────────────

Configuration:
- Temperature: 0.3
- Top_p: 0.9
- Max_new_tokens: 100
- Few-shot examples: 2 → 3 (better learning)

Changes:

4. WEIGHTED SCORING SYSTEM
   Old parsing:
   ```
   yes_count = count of "نعم", "يحتوي" anywhere
   no_count = count of "لا", "لا يحتوي" anywhere
   if yes_count > no_count: return 1
   ```
   
   New parsing:
   ```
   # Explicit patterns (highest priority)
   if "نعم، يحتوي" in first_50_chars: return 1
   if "لا، لا يحتوي" in first_50_chars: return 0
   
   # Weighted scoring
   first_50_chars:
     - Strong indicators (نعم, يحتوي): +4 points
     - Weak indicators (موجود, واضح): +2 points
   chars_50_100:
     - Strong indicators: +1 point
   
   # Decision threshold
   if yes_score > no_score * 1.2: return 1  # Requires 20% more evidence
   else: return 0
   ```
   
   Benefit: 
   - Focus on beginning of response (most important)
   - Weighted evidence instead of simple counting
   - Moderate threshold (not too aggressive, not too conservative)

5. INCREASED FEW-SHOT EXAMPLES
   2 → 3 examples per category
   
   Benefit: More diverse examples for better pattern learning

Results:
- F1 Macro: 0.4244 (BEST)
- Balanced precision/recall
- Political: 0.58 F1 ✓
- Racial/ethnic: 0.45 F1 ✓
- Religious: 0.27 F1 (still challenging)
- Gender/sexual: 0.36 F1 ✓
- Other: 0.46 F1 ✓

================================================================================
EXPERIMENTS THAT FAILED
================================================================================

❌ EXPERIMENT 1: Too Conservative Parsing (Threshold 2.0x)
   - F1 Macro: 0.15 (crashed)
   - Recall dropped to ~10%
   - Missed almost all positives
   - Lesson: Don't be too strict

❌ EXPERIMENT 2: Too Strict Threshold (1.4x)
   - F1 Macro: 0.36 (dropped 12%)
   - Precision dropped on religious (0.13) and gender (0.09)
   - Still too many false positives but missed more true positives
   - Lesson: 1.4x is too high for this model

❌ EXPERIMENT 3: Using Original CoT without modifications
   - F1 Macro: 0.20
   - Model ignored structured format
   - Generated natural text instead
   - Lesson: Generative models need completion-style prompts

================================================================================
FINAL OPTIMAL CONFIGURATION
================================================================================

Model Settings:
├── model_name: "aubmindlab/aragpt2-medium"
├── temperature: 0.3 (was 0.7)
├── top_p: 0.9 (was 0.85)
├── max_new_tokens: 100 (was 256)
└── few_shot_examples: 3 (was 2)

Prompt Strategy:
├── Style: Completion-based (not instruction-based)
├── Few-shot: With reasoning (not just labels)
├── Category hints: Enabled for all categories
└── Format: Simple question-answer pattern

Parsing Strategy:
├── Focus: First 50 characters (highest weight)
├── Scoring: Weighted (4-2-1 points)
├── Threshold: 1.2x (require 20% more yes than no)
└── Default: Conservative (predict 0 in ties)

Performance Metrics:
├── F1 Macro: 0.4244
├── F1 Micro: 0.4414
├── F1 Samples: 0.4112
├── Hamming Loss: 0.5400
└── Average Precision: 0.33 | Average Recall: 0.58

Per-Category Performance:
├── Political: P=0.50 R=0.69 F1=0.58
├── Racial/ethnic: P=0.37 R=0.58 F1=0.45
├── Religious: P=0.18 R=0.60 F1=0.27 (challenging)
├── Gender/sexual: P=0.23 R=0.67 F1=0.36
└── Other: P=0.38 R=0.60 F1=0.46

================================================================================
KEY LEARNINGS
================================================================================

1. Model Characteristics Matter
   ✓ AraGPT2 is generative (like GPT-2), not instruction-tuned
   ✓ Works best with completion patterns, not complex instructions
   ✓ Can't reliably produce structured output

2. Prompt Engineering Principles
   ✓ Keep it simple and natural
   ✓ Show reasoning in examples, not just labels
   ✓ Add category-specific guidance
   ✓ Match the model's training objective

3. Parsing Strategy
   ✓ Focus on beginning of response (first 50 chars)
   ✓ Use weighted scoring, not simple counting
   ✓ Set moderate thresholds (1.2x worked best)
   ✓ Default to conservative in ambiguous cases

4. Optimization Process
   ✓ Start with analysis of failure modes
   ✓ Make incremental changes and measure
   ✓ Don't over-optimize for one metric
   ✓ Balance precision and recall

================================================================================
RECOMMENDATIONS FOR FURTHER IMPROVEMENT
================================================================================

To Push F1 Macro from 0.42 to 0.50+:

1. Better Few-Shot Example Selection
   - Filter examples by keyword relevance
   - Score examples by clarity
   - Use category-specific keywords for selection

2. Fine-tune the Model
   - Fine-tune AraGPT2 on labeled data
   - Or switch to instruction-tuned model (e.g., AceGPT-7B)

3. Ensemble Approaches
   - Combine with BERT-based classifier
   - Use voting or weighted average

4. Data Quality
   - Review and correct training labels
   - Add more balanced examples per category
   - Augment rare categories (religious, gender)

5. Advanced Parsing
   - Train a small classifier on model outputs
   - Use sentence embeddings for semantic parsing
   - Add confidence thresholding

================================================================================
USAGE INSTRUCTIONS
================================================================================

To use the optimized classifier:

```python
# 1. Load data
df = pd.read_csv('/kaggle/input/arb-csv/arb.csv')
labels = ['political', 'racial/ethnic', 'religious', 'gender/sexual', 'other']

# 2. Fill NaN values
for label in labels:
    df[label] = df[label].fillna(0).astype(int)

# 3. Run classification
results = run_improved_classification(df, labels, eval_samples=30)

# 4. Access results
print(f"F1 Macro: {results['metrics']['f1_macro']:.4f}")
```

Files Generated:
- kaggle_improved_single_cell.py (complete classifier)
- predictions.csv (with true and predicted labels)
- This changelog

================================================================================
VERSION COMPARISON SUMMARY
================================================================================

Version          | F1 Macro | Key Change                    | Status
─────────────────┼──────────┼───────────────────────────────┼────────
Original         | 0.20     | Complex CoT prompts           | ❌ Failed
Simplified v1    | 0.40     | Completion-style prompts      | ✓ Better
+ Hints          | 0.4244   | Category-specific hints       | ✓✓ Best
+ Strict Parse   | 0.36     | 1.4x threshold (too strict)   | ❌ Worse
Final (Balanced) | 0.4244   | 1.2x threshold + 3 examples   | ✓✓ Optimal

Improvement: 0.20 → 0.42 (+112% relative improvement)

================================================================================
END OF CHANGELOG
================================================================================

For questions or issues, refer to:
- kaggle_improved_single_cell.py (implementation)
- acegpt_kaggle.ipynb (notebook with comparisons)
- This changelog (optimization history)

Last Updated: January 3, 2026
