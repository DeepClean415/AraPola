{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff8821a",
   "metadata": {},
   "source": [
    "## üîß Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b53b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (most are pre-installed on Kaggle)\n",
    "!pip install -q transformers accelerate bitsandbytes\n",
    "\n",
    "# Verify installations\n",
    "import torch\n",
    "print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úì GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fcd2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure environment to prevent model loading hangs\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Prevent common loading issues\n",
    "os.environ['DISABLE_FLASH_ATTENTION'] = '1'      # Disable flash attention (can cause hangs)\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'   # Prevent tokenizer warnings\n",
    "os.environ['BITSANDBYTES_NOWELCOME'] = '1'       # Suppress bitsandbytes messages\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'         # Better error messages\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/kaggle/working/.cache'  # Use working dir for cache\n",
    "\n",
    "# Clear GPU memory if available\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    mem_info = torch.cuda.mem_get_info(0)\n",
    "    free_mem = mem_info[0] / 1e9\n",
    "    total_mem = mem_info[1] / 1e9\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"GPU MEMORY STATUS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úì Free Memory: {free_mem:.2f} GB / {total_mem:.2f} GB\")\n",
    "    print(f\"‚úì Memory cleared and ready\")\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"‚ö† No GPU detected - will use CPU (very slow)\")\n",
    "\n",
    "print(\"\\n‚úì Environment configured to prevent loading hangs\")\n",
    "print(\"‚úì Flash attention disabled\")\n",
    "print(\"‚úì CUDA launch blocking enabled for better error messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2dee15",
   "metadata": {},
   "source": [
    "## üîß Configure Environment (Prevent Loading Hangs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a557eaef",
   "metadata": {},
   "source": [
    "## üìÅ Setup Kaggle Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c066cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect Kaggle environment\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    # Kaggle paths\n",
    "    INPUT_PATH = Path('/kaggle/input')\n",
    "    WORKING_PATH = Path('/kaggle/working')\n",
    "    \n",
    "    # List available datasets\n",
    "    print(\"Available datasets:\")\n",
    "    if INPUT_PATH.exists():\n",
    "        for item in INPUT_PATH.iterdir():\n",
    "            print(f\"  - {item.name}\")\n",
    "    \n",
    "    # Set dataset path - update this to match your uploaded dataset name\n",
    "    # When you upload a dataset, Kaggle creates a folder like: /kaggle/input/your-dataset-name/\n",
    "    DATA_PATH = INPUT_PATH / 'arb-data'  # Change 'arb-data' to your dataset name\n",
    "    \n",
    "    # Check if data path exists\n",
    "    if DATA_PATH.exists():\n",
    "        print(f\"\\n‚úì Data path found: {DATA_PATH}\")\n",
    "        print(\"Files in dataset:\")\n",
    "        for file in DATA_PATH.iterdir():\n",
    "            print(f\"  - {file.name}\")\n",
    "        \n",
    "        # Set data file path\n",
    "        DATA_FILE = DATA_PATH / 'arb.csv'\n",
    "    else:\n",
    "        print(f\"\\n‚ö† Data path not found: {DATA_PATH}\")\n",
    "        print(\"Looking for arb.csv in other locations...\")\n",
    "        \n",
    "        # Try to find arb.csv in any subdirectory\n",
    "        found = False\n",
    "        for item in INPUT_PATH.iterdir():\n",
    "            csv_path = item / 'arb.csv'\n",
    "            if csv_path.exists():\n",
    "                DATA_FILE = csv_path\n",
    "                print(f\"‚úì Found arb.csv at: {DATA_FILE}\")\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            print(\"\\nPlease:\")\n",
    "            print(\"  1. Upload your dataset using 'Add Data' button\")\n",
    "            print(\"  2. Make sure arb.csv is in the dataset\")\n",
    "            print(\"  3. Update DATA_PATH variable above to match your dataset name\")\n",
    "            DATA_FILE = DATA_PATH / 'arb.csv'  # Set default anyway\n",
    "    \n",
    "else:\n",
    "    # Local paths (fallback)\n",
    "    print(\"Running locally (not on Kaggle)\")\n",
    "    DATA_FILE = '../train/arb.csv'  # Changed to train set\n",
    "    WORKING_PATH = Path('.')\n",
    "\n",
    "print(f\"\\nWorking directory: {WORKING_PATH}\")\n",
    "print(f\"Data file: {DATA_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205546dd",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b8c3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, hamming_loss, classification_report, precision_recall_fscore_support\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Kaggle-Optimized Configuration\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name: str = \"aubmindlab/aragpt2-medium\"  # Smaller model (~1.4GB) that fits on Kaggle\n",
    "    max_length: int = 1024  # Optimized for Kaggle GPU\n",
    "    batch_size: int = 1\n",
    "    num_few_shot_examples: int = 2\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "    max_new_tokens: int = 256\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seed: int = 42\n",
    "    use_8bit: bool = False  # AraGPT2 is small enough, no need for quantization\n",
    "    eval_samples: int = 30  # Increased from 20 since Kaggle is faster\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(config.seed)\n",
    "torch.manual_seed(config.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(config.seed)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Device: {config.device}\")\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Few-shot examples: {config.num_few_shot_examples}\")\n",
    "print(f\"8-bit quantization: {config.use_8bit}\")\n",
    "print(f\"Evaluation samples: {config.eval_samples}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e784d7af",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Cultural Context Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b18228",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CulturalContextMapper:\n",
    "    \"\"\"Maps polarization categories to Arabic cultural perspectives.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cultural_contexts = {\n",
    "            'political': {\n",
    "                'ar_name': 'ÿßŸÑÿßÿ≥ÿ™ŸÇÿ∑ÿßÿ® ÿßŸÑÿ≥Ÿäÿßÿ≥Ÿä',\n",
    "                'context': 'ŸÅŸä ÿßŸÑÿ´ŸÇÿßŸÅÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©ÿå ÿßŸÑÿßÿ≥ÿ™ŸÇÿ∑ÿßÿ® ÿßŸÑÿ≥Ÿäÿßÿ≥Ÿä Ÿäÿ¥ŸÖŸÑ ÿßŸÑŸÜŸÇÿßÿ¥ÿßÿ™ ÿ≠ŸàŸÑ ÿßŸÑÿ≠ŸÉŸàŸÖÿßÿ™ÿå ÿßŸÑÿ£ÿ≠ÿ≤ÿßÿ® ÿßŸÑÿ≥Ÿäÿßÿ≥Ÿäÿ©ÿå ÿßŸÑŸÇÿßÿØÿ©ÿå ÿßŸÑÿ≥Ÿäÿßÿ≥ÿßÿ™ ÿßŸÑÿØÿßÿÆŸÑŸäÿ© ŸàÿßŸÑÿÆÿßÿ±ÿ¨Ÿäÿ©ÿå ŸàÿßŸÑÿµÿ±ÿßÿπÿßÿ™ ÿßŸÑÿ≥Ÿäÿßÿ≥Ÿäÿ© ÿ®ŸäŸÜ ÿßŸÑŸÅÿµÿßÿ¶ŸÑ ÿßŸÑŸÖÿÆÿ™ŸÑŸÅÿ©.',\n",
    "                'keywords': ['ÿ≠ŸÉŸàŸÖÿ©', 'ÿ≥Ÿäÿßÿ≥ÿ©', 'ÿ±ÿ¶Ÿäÿ≥', 'Ÿàÿ≤Ÿäÿ±', 'ÿ≠ÿ≤ÿ®', 'ÿßŸÜÿ™ÿÆÿßÿ®ÿßÿ™', 'ŸÖÿπÿßÿ±ÿ∂ÿ©', 'ŸÜÿ∏ÿßŸÖ', 'ÿ≥ŸÑÿ∑ÿ©', 'ÿØŸàŸÑÿ©']\n",
    "            },\n",
    "            'racial/ethnic': {\n",
    "                'ar_name': 'ÿßŸÑÿßÿ≥ÿ™ŸÇÿ∑ÿßÿ® ÿßŸÑÿπÿ±ŸÇŸä/ÿßŸÑÿ•ÿ´ŸÜŸä',\n",
    "                'context': 'ŸÅŸä ÿßŸÑÿ´ŸÇÿßŸÅÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©ÿå ÿßŸÑÿßÿ≥ÿ™ŸÇÿ∑ÿßÿ® ÿßŸÑÿπÿ±ŸÇŸä Ÿäÿ™ÿπŸÑŸÇ ÿ®ÿßŸÑÿ™ŸÖŸäŸäÿ≤ ÿ£Ÿà ÿßŸÑÿ™ÿ≠Ÿäÿ≤ ÿ∂ÿØ ŸÖÿ¨ŸÖŸàÿπÿßÿ™ ÿπÿ±ŸÇŸäÿ© ÿ£Ÿà ÿ•ÿ´ŸÜŸäÿ© ŸÖÿπŸäŸÜÿ©ÿå ŸÖÿ´ŸÑ ÿßŸÑÿπÿ±ÿ®ÿå ÿßŸÑÿ£ŸÉÿ±ÿßÿØÿå ÿßŸÑÿ£ŸÖÿßÿ≤Ÿäÿ∫ÿå ÿßŸÑÿ£ŸÅÿßÿ±ŸÇÿ©ÿå ÿ£Ÿà ÿ∫Ÿäÿ±ŸáŸÖ ŸÖŸÜ ÿßŸÑÿ¨ŸÜÿ≥Ÿäÿßÿ™ ŸàÿßŸÑÿ£ÿπÿ±ÿßŸÇ.',\n",
    "                'keywords': ['ÿπÿ±ÿ®Ÿä', 'ÿ£ÿ¨ŸÜÿ®Ÿä', 'ÿ¨ŸÜÿ≥Ÿäÿ©', 'ÿπÿ±ŸÇ', 'ÿ£ŸÅÿ±ŸäŸÇŸä', 'ÿ£Ÿàÿ±Ÿàÿ®Ÿä', 'ÿ¢ÿ≥ŸäŸàŸä', 'ÿ•ÿ´ŸÜŸä', 'ŸÇÿ®ŸäŸÑÿ©', 'ÿ®ÿØŸà']\n",
    "            },\n",
    "            'religious': {\n",
    "                'ar_name': 'ÿßŸÑÿßÿ≥ÿ™ŸÇÿ∑ÿßÿ® ÿßŸÑÿØŸäŸÜŸä',\n",
    "                'context': 'ŸÅŸä ÿßŸÑÿ´ŸÇÿßŸÅÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©ÿå ÿßŸÑÿßÿ≥ÿ™ŸÇÿ∑ÿßÿ® ÿßŸÑÿØŸäŸÜŸä Ÿäÿ¥ŸÖŸÑ ÿßŸÑÿ™ÿ≠Ÿäÿ≤ ÿ£Ÿà ÿßŸÑŸÉÿ±ÿßŸáŸäÿ© ÿ®ŸäŸÜ ÿßŸÑÿ∑Ÿàÿßÿ¶ŸÅ ÿßŸÑÿØŸäŸÜŸäÿ© ÿßŸÑŸÖÿÆÿ™ŸÑŸÅÿ© (ÿ≥ŸÜŸäÿå ÿ¥ŸäÿπŸäÿå ŸÖÿ≥Ÿäÿ≠Ÿäÿå ŸäŸáŸàÿØŸäÿå ÿ•ŸÑÿÆ) ÿ£Ÿà ÿßŸÑŸáÿ¨ŸàŸÖ ÿπŸÑŸâ ÿßŸÑŸÖÿπÿ™ŸÇÿØÿßÿ™ ÿßŸÑÿØŸäŸÜŸäÿ©.',\n",
    "                'keywords': ['ÿØŸäŸÜ', 'ÿ¥ŸäÿπŸä', 'ÿ≥ŸÜŸä', 'ŸÖÿ≥Ÿäÿ≠Ÿä', 'ŸäŸáŸàÿØŸä', 'ŸÉÿßŸÅÿ±', 'ÿ∑ÿßÿ¶ŸÅÿ©', 'ŸÖÿ∞Ÿáÿ®', 'ÿ≠Ÿàÿ≤ÿ©', 'ÿ™ŸÉŸÅŸäÿ±']\n",
    "            },\n",
    "            'gender/sexual': {\n",
    "                'ar_name': 'ÿßŸÑÿßÿ≥ÿ™ŸÇÿ∑ÿßÿ® ÿßŸÑÿ¨ŸÜÿ≥Ÿä/ÿßŸÑŸÜŸàÿπŸä',\n",
    "                'context': 'ŸÅŸä ÿßŸÑÿ´ŸÇÿßŸÅÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©ÿå Ÿáÿ∞ÿß ÿßŸÑŸÜŸàÿπ Ÿäÿ™ÿπŸÑŸÇ ÿ®ÿßŸÑÿ™ŸÖŸäŸäÿ≤ ÿ£Ÿà ÿßŸÑŸÉÿ±ÿßŸáŸäÿ© ÿπŸÑŸâ ÿ£ÿ≥ÿßÿ≥ ÿßŸÑÿ¨ŸÜÿ≥ ÿ£Ÿà ÿßŸÑŸáŸàŸäÿ© ÿßŸÑÿ¨ŸÜÿ≥Ÿäÿ©ÿå ÿ®ŸÖÿß ŸÅŸä ÿ∞ŸÑŸÉ ÿßŸÑÿ™ÿ≠ÿ±ÿ¥ÿå ÿßŸÑÿ•ÿ≥ÿßÿ°ÿ© ŸÑŸÑŸÖÿ±ÿ£ÿ©ÿå ÿ£Ÿà ÿßŸÑŸÖÿ´ŸÑŸäÿ© ÿßŸÑÿ¨ŸÜÿ≥Ÿäÿ©.',\n",
    "                'keywords': ['ÿßŸÖÿ±ÿ£ÿ©', 'ÿ±ÿ¨ŸÑ', 'ÿ¨ŸÜÿ≥', 'ÿ™ÿ≠ÿ±ÿ¥', 'ÿßÿ∫ÿ™ÿµÿßÿ®', 'ŸÖÿ´ŸÑŸä', 'ÿ¥ÿ∞Ÿàÿ∞', 'ÿπŸÜŸÅ ÿ£ÿ≥ÿ±Ÿä', 'ÿÆÿ™ÿßŸÜ', 'ÿ∑ŸÑÿßŸÇ']\n",
    "            },\n",
    "            'other': {\n",
    "                'ar_name': 'ÿßÿ≥ÿ™ŸÇÿ∑ÿßÿ® ÿ¢ÿÆÿ±',\n",
    "                'context': 'ŸÅŸä ÿßŸÑÿ´ŸÇÿßŸÅÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©ÿå Ÿáÿ∞ÿß Ÿäÿ¥ŸÖŸÑ ÿ£Ÿä ÿ¥ŸÉŸÑ ÿ¢ÿÆÿ± ŸÖŸÜ ÿßŸÑÿßÿ≥ÿ™ŸÇÿ∑ÿßÿ® ÿ£Ÿà ÿßŸÑŸÉÿ±ÿßŸáŸäÿ© ŸÑÿß ŸäŸÜÿØÿ±ÿ¨ ÿ™ÿ≠ÿ™ ÿßŸÑŸÅÿ¶ÿßÿ™ ÿßŸÑÿ≥ÿßÿ®ŸÇÿ©ÿå ŸÖÿ´ŸÑ ÿßŸÑÿ™ŸÖŸäŸäÿ≤ ÿπŸÑŸâ ÿ£ÿ≥ÿßÿ≥ ÿßŸÑÿ∑ÿ®ŸÇÿ© ÿßŸÑÿßÿ¨ÿ™ŸÖÿßÿπŸäÿ©ÿå ÿßŸÑŸÖŸáŸÜÿ©ÿå ÿ£Ÿà ÿßŸÑŸÖÿ∏Ÿáÿ± ÿßŸÑÿÆÿßÿ±ÿ¨Ÿä.',\n",
    "                'keywords': ['ŸÅŸÇŸäÿ±', 'ÿ∫ŸÜŸä', 'ÿ∑ÿ®ŸÇÿ©', 'ŸÖŸáŸÜÿ©', 'ÿ¥ŸÉŸÑ', 'ŸÖÿ∏Ÿáÿ±', 'ÿ™ÿπŸÑŸäŸÖ', 'ÿ´ŸÇÿßŸÅÿ©']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_context(self, category: str) -> str:\n",
    "        return self.cultural_contexts.get(category, {}).get('context', '')\n",
    "    \n",
    "    def get_ar_name(self, category: str) -> str:\n",
    "        return self.cultural_contexts.get(category, {}).get('ar_name', category)\n",
    "    \n",
    "    def format_with_context(self, text: str, category: str) -> str:\n",
    "        context = self.get_context(category)\n",
    "        ar_name = self.get_ar_name(category)\n",
    "        \n",
    "        formatted = f\"\"\"ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑÿ´ŸÇÿßŸÅŸä: {context}\n",
    "\n",
    "ÿßŸÑŸÜÿµ ÿßŸÑŸÖÿ±ÿßÿØ ÿ™ÿ≠ŸÑŸäŸÑŸá: \"{text}\"\n",
    "\n",
    "ÿßŸÑÿ≥ÿ§ÿßŸÑ: ŸÅŸä ÿßŸÑÿ´ŸÇÿßŸÅÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©ÿå ŸáŸÑ Ÿáÿ∞ÿß ÿßŸÑŸÜÿµ Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ {ar_name}ÿü\"\"\"\n",
    "        \n",
    "        return formatted\n",
    "\n",
    "# Initialize mapper\n",
    "cultural_mapper = CulturalContextMapper()\n",
    "print(\"‚úì Cultural Context Mapper initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b07dcc0",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Few-Shot Example Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7d06f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotExampleBank:\n",
    "    \"\"\"Manages few-shot examples for in-context learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, labels: List[str]):\n",
    "        self.df = df\n",
    "        self.labels = labels\n",
    "        self.example_bank = self._build_example_bank()\n",
    "    \n",
    "    def _build_example_bank(self) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Build bank of clear examples for each category.\"\"\"\n",
    "        bank = {label: {'positive': [], 'negative': []} for label in self.labels}\n",
    "        \n",
    "        for _, row in self.df.iterrows():\n",
    "            text = row['text']\n",
    "            \n",
    "            # Skip if text is too long (saves memory)\n",
    "            if len(text) > 200:\n",
    "                continue\n",
    "            \n",
    "            for label in self.labels:\n",
    "                label_val = row[label]\n",
    "                if pd.isna(label_val):\n",
    "                    continue\n",
    "                    \n",
    "                if label_val == 1:\n",
    "                    if len(bank[label]['positive']) < 100:\n",
    "                        bank[label]['positive'].append({\n",
    "                            'text': text,\n",
    "                            'label': 1,\n",
    "                            'all_labels': {l: int(row[l]) if pd.notna(row[l]) else 0 for l in self.labels}\n",
    "                        })\n",
    "                elif label_val == 0:\n",
    "                    other_labels_sum = sum([row[l] for l in self.labels if pd.notna(row[l]) and l != label])\n",
    "                    if other_labels_sum == 0 and len(bank[label]['negative']) < 100:\n",
    "                        bank[label]['negative'].append({\n",
    "                            'text': text,\n",
    "                            'label': 0,\n",
    "                            'all_labels': {l: int(row[l]) if pd.notna(row[l]) else 0 for l in self.labels}\n",
    "                        })\n",
    "        \n",
    "        return bank\n",
    "    \n",
    "    def get_few_shot_examples(self, category: str, n: int = 2) -> List[Dict]:\n",
    "        positive_examples = self.example_bank[category]['positive']\n",
    "        negative_examples = self.example_bank[category]['negative']\n",
    "        \n",
    "        pos_sample = np.random.choice(\n",
    "            len(positive_examples), \n",
    "            size=min(n, len(positive_examples)), \n",
    "            replace=False\n",
    "        ) if len(positive_examples) > 0 else []\n",
    "        \n",
    "        neg_sample = np.random.choice(\n",
    "            len(negative_examples), \n",
    "            size=min(n, len(negative_examples)), \n",
    "            replace=False\n",
    "        ) if len(negative_examples) > 0 else []\n",
    "        \n",
    "        examples = []\n",
    "        for idx in pos_sample:\n",
    "            examples.append(positive_examples[idx])\n",
    "        for idx in neg_sample:\n",
    "            examples.append(negative_examples[idx])\n",
    "        \n",
    "        np.random.shuffle(examples)\n",
    "        return examples\n",
    "    \n",
    "    def format_few_shot_prompt(self, category: str, examples: List[Dict]) -> str:\n",
    "        if not examples:\n",
    "            return \"\"\n",
    "            \n",
    "        prompt = f\"ÿ£ŸÖÿ´ŸÑÿ©:\\n\"\n",
    "        \n",
    "        for i, example in enumerate(examples, 1):\n",
    "            label_text = \"ŸÜÿπŸÖ\" if example['label'] == 1 else \"ŸÑÿß\"\n",
    "            text = example['text'][:100] + \"...\" if len(example['text']) > 100 else example['text']\n",
    "            prompt += f\"{i}. \\\"{text}\\\" ‚Üí {label_text}\\n\"\n",
    "        \n",
    "        return prompt + \"\\n\"\n",
    "\n",
    "print(\"‚úì FewShotExampleBank class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21256841",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Chain-of-Thought Prompter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a1d914",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è IMPROVED PROMPTING (Try This First!)\n",
    "\n",
    "**Problem Identified**: Current prompts don't work well with generative models like AraGPT2.\n",
    "\n",
    "**Solutions**:\n",
    "1. **Completion-style prompts** (easier for GPT-2 style models)\n",
    "2. **Clear few-shot examples with reasoning**\n",
    "3. **Better parsing that handles natural language**\n",
    "4. **Shorter, more direct prompts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac51b1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedPrompter:\n",
    "    \"\"\"Simplified prompts that work better with generative models.\"\"\"\n",
    "    \n",
    "    def __init__(self, cultural_mapper: CulturalContextMapper):\n",
    "        self.cultural_mapper = cultural_mapper\n",
    "    \n",
    "    def create_completion_prompt(self, text: str, category: str, few_shot_examples: str = \"\") -> str:\n",
    "        \"\"\"\n",
    "        Completion-style prompt - model just continues the pattern.\n",
    "        Works better for GPT-2 style models.\n",
    "        \"\"\"\n",
    "        ar_name = self.cultural_mapper.get_ar_name(category)\n",
    "        \n",
    "        # Very simple, direct prompt\n",
    "        prompt = f\"\"\"{few_shot_examples}ÿßŸÑŸÜÿµ: \"{text}\"\n",
    "ÿßŸÑÿ≥ÿ§ÿßŸÑ: ŸáŸÑ Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ {ar_name}ÿü\n",
    "ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©:\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def create_simple_cot_prompt(self, text: str, category: str, few_shot_examples: str = \"\") -> str:\n",
    "        \"\"\"\n",
    "        Simpler CoT that guides model to complete naturally.\n",
    "        \"\"\"\n",
    "        ar_name = self.cultural_mapper.get_ar_name(category)\n",
    "        \n",
    "        prompt = f\"\"\"{few_shot_examples}ÿßŸÑŸÜÿµ: \"{text}\"\n",
    "\n",
    "ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ: Ÿáÿ∞ÿß ÿßŸÑŸÜÿµ\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def parse_response_flexible(self, response: str) -> Tuple[int, str]:\n",
    "        \"\"\"\n",
    "        More flexible parsing that handles natural language better.\n",
    "        \"\"\"\n",
    "        response_lower = response.lower().strip()\n",
    "        reasoning = response\n",
    "        \n",
    "        # First 200 characters are most important\n",
    "        first_part = response_lower[:200]\n",
    "        \n",
    "        # Strong positive indicators\n",
    "        strong_yes = ['ŸÜÿπŸÖ', 'Ÿäÿ≠ÿ™ŸàŸä', 'ŸäŸàÿ¨ÿØ', 'ŸÖŸàÿ¨ŸàÿØ', 'Ÿàÿßÿ∂ÿ≠']\n",
    "        strong_no = ['ŸÑÿß Ÿäÿ≠ÿ™ŸàŸä', 'ŸÑÿß ŸäŸàÿ¨ÿØ', 'ŸÑŸäÿ≥', 'ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØ', 'ŸÑÿß']\n",
    "        \n",
    "        # Count in first part (more weight)\n",
    "        first_yes = sum(3 for word in strong_yes if word in first_part)\n",
    "        first_no = sum(3 for word in strong_no if word in first_part)\n",
    "        \n",
    "        # Count in full response\n",
    "        full_yes = sum(1 for word in strong_yes if word in response_lower)\n",
    "        full_no = sum(1 for word in strong_no if word in response_lower)\n",
    "        \n",
    "        total_yes = first_yes + full_yes\n",
    "        total_no = first_no + full_no\n",
    "        \n",
    "        # Decision\n",
    "        if total_yes > total_no:\n",
    "            return 1, reasoning\n",
    "        elif total_no > total_yes:\n",
    "            return 0, reasoning\n",
    "        else:\n",
    "            # Tie-breaker: default to 0 (less false positives)\n",
    "            return 0, reasoning\n",
    "    \n",
    "    def format_few_shot_with_reasoning(self, examples: List[Dict], category: str) -> str:\n",
    "        \"\"\"\n",
    "        Show examples WITH reasoning to teach model the pattern.\n",
    "        \"\"\"\n",
    "        if not examples:\n",
    "            return \"\"\n",
    "        \n",
    "        ar_name = self.cultural_mapper.get_ar_name(category)\n",
    "        prompt = \"\"\n",
    "        \n",
    "        for example in examples:\n",
    "            text = example['text'][:80] + \"...\" if len(example['text']) > 80 else example['text']\n",
    "            label = example['label']\n",
    "            \n",
    "            if label == 1:\n",
    "                reasoning = f\"Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ {ar_name}\"\n",
    "                answer = \"ŸÜÿπŸÖ\"\n",
    "            else:\n",
    "                reasoning = f\"ŸÑÿß Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ {ar_name}\"\n",
    "                answer = \"ŸÑÿß\"\n",
    "            \n",
    "            prompt += f\"\"\"ÿßŸÑŸÜÿµ: \"{text}\"\n",
    "ÿßŸÑÿ≥ÿ§ÿßŸÑ: ŸáŸÑ Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ {ar_name}ÿü\n",
    "ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©: {answer}ÿå ŸÑÿ£ŸÜ ÿßŸÑŸÜÿµ {reasoning}.\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "\n",
    "print(\"‚úì ImprovedPrompter class defined\")\n",
    "\n",
    "# Test it\n",
    "improved_prompter = ImprovedPrompter(cultural_mapper)\n",
    "print(\"‚úì ImprovedPrompter initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29477e5",
   "metadata": {},
   "source": [
    "## üîÑ Updated Classifier (Uses Improved Prompting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f6c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedClassifier:\n",
    "    \"\"\"\n",
    "    Classifier using improved prompting strategies.\n",
    "    Use THIS instead of AdvancedAceGPTClassifier for better results.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, cultural_mapper, few_shot_bank, improved_prompter, labels):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cultural_mapper = cultural_mapper\n",
    "        self.few_shot_bank = few_shot_bank\n",
    "        self.improved_prompter = improved_prompter\n",
    "        self.labels = labels\n",
    "    \n",
    "    def classify_single_category(self, text: str, category: str, \n",
    "                                num_few_shot: int = 2, \n",
    "                                prompt_style: str = 'completion') -> Dict:\n",
    "        \"\"\"\n",
    "        prompt_style: 'completion' (recommended) or 'simple_cot'\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get few-shot examples\n",
    "            examples = self.few_shot_bank.get_few_shot_examples(category, n=num_few_shot)\n",
    "            few_shot_text = self.improved_prompter.format_few_shot_with_reasoning(examples, category)\n",
    "            \n",
    "            # Create prompt based on style\n",
    "            if prompt_style == 'completion':\n",
    "                prompt = self.improved_prompter.create_completion_prompt(text, category, few_shot_text)\n",
    "            else:\n",
    "                prompt = self.improved_prompter.create_simple_cot_prompt(text, category, few_shot_text)\n",
    "            \n",
    "            # Generate\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", \n",
    "                                   max_length=config.max_length, truncation=True).to(config.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs, \n",
    "                    max_new_tokens=100,  # Shorter output for completion style\n",
    "                    temperature=0.5,      # Less random\n",
    "                    do_sample=True,\n",
    "                    top_p=0.85,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    repetition_penalty=1.2  # Reduce repetition\n",
    "                )\n",
    "            \n",
    "            response = self.tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], \n",
    "                                            skip_special_tokens=True)\n",
    "            \n",
    "            # Parse with flexible parser\n",
    "            prediction, reasoning = self.improved_prompter.parse_response_flexible(response)\n",
    "            \n",
    "            return {\n",
    "                'category': category,\n",
    "                'prediction': prediction,\n",
    "                'reasoning': response,\n",
    "                'prompt_length': len(prompt),\n",
    "                'prompt_style': prompt_style\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error classifying {category}: {e}\")\n",
    "            return {\n",
    "                'category': category, \n",
    "                'prediction': 0, \n",
    "                'reasoning': f\"Error: {str(e)}\",\n",
    "                'prompt_length': 0,\n",
    "                'prompt_style': prompt_style\n",
    "            }\n",
    "    \n",
    "    def classify_text(self, text: str, num_few_shot: int = 2, \n",
    "                     prompt_style: str = 'completion') -> Dict:\n",
    "        results = {'text': text, 'predictions': {}, 'category_details': {}}\n",
    "        \n",
    "        for category in self.labels:\n",
    "            category_result = self.classify_single_category(\n",
    "                text, category, num_few_shot, prompt_style\n",
    "            )\n",
    "            results['predictions'][category] = category_result['prediction']\n",
    "            results['category_details'][category] = category_result\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def batch_classify(self, texts: List[str], num_few_shot: int = 2,\n",
    "                      prompt_style: str = 'completion', show_progress: bool = True) -> List[Dict]:\n",
    "        results = []\n",
    "        iterator = tqdm(texts, desc=\"Classifying\") if show_progress else texts\n",
    "        \n",
    "        for text in iterator:\n",
    "            try:\n",
    "                result = self.classify_text(text, num_few_shot, prompt_style)\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing text: {e}\")\n",
    "                results.append({\n",
    "                    'text': text, \n",
    "                    'predictions': {label: 0 for label in self.labels}, \n",
    "                    'category_details': {}\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"‚úì ImprovedClassifier class defined\")\n",
    "print(\"\\nüìå Use this classifier instead of AdvancedAceGPTClassifier for better F1 scores!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0c1656",
   "metadata": {},
   "source": [
    "## üî¨ Diagnosis: Compare Prompts Side-by-Side\n",
    "\n",
    "Run this to see the difference and diagnose what's wrong with your current approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f049f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell helps diagnose the issue - run AFTER loading model and data\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PROMPT COMPARISON & DIAGNOSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sample text\n",
    "test_text = \"ÿ±ÿ¶Ÿäÿ≥ ÿßŸÑÿØŸàŸÑÿ© ŸÉÿßŸÅÿ± ŸàÿßŸÑÿ¥ÿπÿ® ÿ≥ÿßŸÉÿ™\"\n",
    "test_category = \"religious\"\n",
    "\n",
    "# Get few-shot examples (need data loaded first)\n",
    "try:\n",
    "    examples = few_shot_bank.get_few_shot_examples(test_category, n=2)\n",
    "    \n",
    "    print(\"\\n1Ô∏è‚É£ CURRENT PROMPT (Complex CoT):\")\n",
    "    print(\"-\"*80)\n",
    "    few_shot_old = few_shot_bank.format_few_shot_prompt(test_category, examples)\n",
    "    old_prompt = cot_prompter.create_cot_prompt(test_text, test_category, few_shot_old)\n",
    "    print(old_prompt)\n",
    "    print(f\"\\nLength: {len(old_prompt)} chars\")\n",
    "    \n",
    "    print(\"\\n\\n2Ô∏è‚É£ IMPROVED PROMPT (Simple Completion):\")\n",
    "    print(\"-\"*80)\n",
    "    few_shot_new = improved_prompter.format_few_shot_with_reasoning(examples, test_category)\n",
    "    new_prompt = improved_prompter.create_completion_prompt(test_text, test_category, few_shot_new)\n",
    "    print(new_prompt)\n",
    "    print(f\"\\nLength: {len(new_prompt)} chars\")\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"KEY DIFFERENCES:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"‚ùå Old prompt:\")\n",
    "    print(\"   - Asks for 5 analysis steps (model ignores this)\")\n",
    "    print(\"   - Long and complex (confusing for GPT-2)\")\n",
    "    print(\"   - Examples show only labels, no reasoning\")\n",
    "    print(\"   - Expects structured output (won't happen)\")\n",
    "    \n",
    "    print(\"\\n‚úÖ New prompt:\")\n",
    "    print(\"   - Simple completion pattern (model continues naturally)\")\n",
    "    print(\"   - Shows reasoning in examples (teaches the pattern)\")\n",
    "    print(\"   - Shorter and clearer\")\n",
    "    print(\"   - Natural for generative models\")\n",
    "    \n",
    "    print(\"\\n\\nüí° RECOMMENDATION:\")\n",
    "    print(\"Use ImprovedClassifier with prompt_style='completion'\")\n",
    "    print(\"Expected F1 improvement: +0.10 to +0.25\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except NameError:\n",
    "    print(\"\\n‚ö†Ô∏è Run this cell AFTER loading data and initializing classes\")\n",
    "    print(\"   (After the 'üìä Load and Prepare Data' cell)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108d4b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChainOfThoughtPrompter:\n",
    "    \"\"\"Creates Chain-of-Thought prompts for step-by-step reasoning.\"\"\"\n",
    "    \n",
    "    def __init__(self, cultural_mapper: CulturalContextMapper):\n",
    "        self.cultural_mapper = cultural_mapper\n",
    "    \n",
    "    def create_cot_prompt(self, text: str, category: str, few_shot_examples: str = \"\") -> str:\n",
    "        ar_name = self.cultural_mapper.get_ar_name(category)\n",
    "        context = self.cultural_mapper.get_context(category)\n",
    "        \n",
    "        prompt = f\"\"\"ÿßŸÑŸÖŸáŸÖÿ©: ÿ™ÿ≠ÿØŸäÿØ ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑŸÜÿµ Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ {ar_name}\n",
    "\n",
    "ÿßŸÑÿ≥ŸäÿßŸÇ: {context}\n",
    "\n",
    "{few_shot_examples}ÿßŸÑÿ¢ŸÜ ÿ≠ŸÑŸÑ Ÿáÿ∞ÿß ÿßŸÑŸÜÿµ:\n",
    "\"{text}\"\n",
    "\n",
    "ÿÆÿ∑Ÿàÿßÿ™ ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ:\n",
    "1. ÿßŸÑŸÖŸàÿ∂Ÿàÿπ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿä\n",
    "2. ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖŸÅÿ™ÿßÿ≠Ÿäÿ©\n",
    "3. ÿßŸÑŸÜÿ®ÿ±ÿ© (ÿ≥ŸÑÿ®Ÿäÿ©/ÿ•Ÿäÿ¨ÿßÿ®Ÿäÿ©)\n",
    "4. ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑÿ´ŸÇÿßŸÅŸä\n",
    "5. ÿßŸÑŸÇÿ±ÿßÿ± ÿßŸÑŸÜŸáÿßÿ¶Ÿä: ŸÜÿπŸÖ ÿ£Ÿà ŸÑÿß\n",
    "\n",
    "ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ:\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def parse_cot_response(self, response: str) -> Tuple[int, str]:\n",
    "        response_lower = response.lower()\n",
    "        reasoning = response\n",
    "        \n",
    "        positive_patterns = [\n",
    "            r'ÿßŸÑŸÇÿ±ÿßÿ± ÿßŸÑŸÜŸáÿßÿ¶Ÿä[:\\s]*ŸÜÿπŸÖ',\n",
    "            r'ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©[:\\s]*ŸÜÿπŸÖ',\n",
    "            r'ŸÜÿπŸÖ[,ÿå.]?\\s*Ÿäÿ≠ÿ™ŸàŸä',\n",
    "            r'ŸÜÿπŸÖ[,ÿå.]?\\s*ŸäŸàÿ¨ÿØ',\n",
    "            r'ÿßŸÑŸÇÿ±ÿßÿ±[:\\s]*ŸÜÿπŸÖ'\n",
    "        ]\n",
    "        \n",
    "        negative_patterns = [\n",
    "            r'ÿßŸÑŸÇÿ±ÿßÿ± ÿßŸÑŸÜŸáÿßÿ¶Ÿä[:\\s]*ŸÑÿß',\n",
    "            r'ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©[:\\s]*ŸÑÿß',\n",
    "            r'ŸÑÿß[,ÿå.]?\\s*ŸÑÿß\\s*Ÿäÿ≠ÿ™ŸàŸä',\n",
    "            r'ŸÑÿß[,ÿå.]?\\s*ŸÑÿß\\s*ŸäŸàÿ¨ÿØ',\n",
    "            r'ÿßŸÑŸÇÿ±ÿßÿ±[:\\s]*ŸÑÿß'\n",
    "        ]\n",
    "        \n",
    "        for pattern in positive_patterns:\n",
    "            if re.search(pattern, response, re.IGNORECASE):\n",
    "                return 1, reasoning\n",
    "        \n",
    "        for pattern in negative_patterns:\n",
    "            if re.search(pattern, response, re.IGNORECASE):\n",
    "                return 0, reasoning\n",
    "        \n",
    "        # Fallback\n",
    "        positive_indicators = ['ŸÜÿπŸÖ', 'ŸäŸàÿ¨ÿØ', 'Ÿäÿ≠ÿ™ŸàŸä', 'Ÿàÿßÿ∂ÿ≠', 'ŸÖŸàÿ¨ŸàÿØ']\n",
    "        negative_indicators = ['ŸÑÿß', 'ŸÑÿß ŸäŸàÿ¨ÿØ', 'ŸÑÿß Ÿäÿ≠ÿ™ŸàŸä', 'ÿ∫Ÿäÿ± Ÿàÿßÿ∂ÿ≠', 'ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØ']\n",
    "        \n",
    "        pos_count = sum(1 for ind in positive_indicators if ind in response_lower)\n",
    "        neg_count = sum(1 for ind in negative_indicators if ind in response_lower)\n",
    "        \n",
    "        label = 1 if pos_count > neg_count else 0\n",
    "        return label, reasoning\n",
    "\n",
    "print(\"‚úì ChainOfThoughtPrompter class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1015bd7c",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ RLAIF Scorer (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1774b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAIFScorer:\n",
    "    \"\"\"Reinforcement Learning from AI Feedback scorer.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def create_feedback_prompt(self, original_text: str, reasoning: str, prediction: int, category: str) -> str:\n",
    "        ar_name = cultural_mapper.get_ar_name(category)\n",
    "        pred_text = \"ŸÜÿπŸÖ\" if prediction == 1 else \"ŸÑÿß\"\n",
    "        \n",
    "        prompt = f\"\"\"ŸÇŸäŸëŸÖ Ÿáÿ∞ÿß ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ:\n",
    "\n",
    "ÿßŸÑŸÜÿµ: \"{original_text[:100]}...\"\n",
    "ÿßŸÑŸÅÿ¶ÿ©: {ar_name}\n",
    "ÿßŸÑŸÇÿ±ÿßÿ±: {pred_text}\n",
    "\n",
    "ŸÇŸäŸëŸÖ ŸÖŸÜ 0-10:\n",
    "ÿßŸÑÿ¥ŸÖŸàŸÑŸäÿ©: [ÿØÿ±ÿ¨ÿ©]\n",
    "ÿßŸÑŸÖŸÜÿ∑ŸÇŸäÿ©: [ÿØÿ±ÿ¨ÿ©]\n",
    "ÿßŸÑÿØŸÇÿ©: [ÿØÿ±ÿ¨ÿ©]\n",
    "ÿßŸÑÿ´ŸÇÿ©: [ÿØÿ±ÿ¨ÿ©]\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def parse_feedback_scores(self, feedback: str) -> Dict[str, float]:\n",
    "        scores = {'comprehensiveness': 5.0, 'logic': 5.0, 'accuracy': 5.0, 'confidence': 5.0, 'overall': 5.0}\n",
    "        \n",
    "        patterns = {\n",
    "            'comprehensiveness': r'ÿßŸÑÿ¥ŸÖŸàŸÑŸäÿ©[:\\s]+(\\d+(?:\\.\\d+)?)',\n",
    "            'logic': r'ÿßŸÑŸÖŸÜÿ∑ŸÇŸäÿ©[:\\s]+(\\d+(?:\\.\\d+)?)',\n",
    "            'accuracy': r'ÿßŸÑÿØŸÇÿ©[:\\s]+(\\d+(?:\\.\\d+)?)',\n",
    "            'confidence': r'ÿßŸÑÿ´ŸÇÿ©[:\\s]+(\\d+(?:\\.\\d+)?)',\n",
    "        }\n",
    "        \n",
    "        for key, pattern in patterns.items():\n",
    "            match = re.search(pattern, feedback)\n",
    "            if match:\n",
    "                try:\n",
    "                    score = float(match.group(1))\n",
    "                    scores[key] = min(10.0, max(0.0, score))\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        scores['overall'] = np.mean([scores['comprehensiveness'], scores['logic'], \n",
    "                                     scores['accuracy'], scores['confidence']])\n",
    "        return scores\n",
    "    \n",
    "    def generate_feedback(self, original_text: str, reasoning: str, \n",
    "                         prediction: int, category: str, temperature: float = 0.3) -> Dict[str, float]:\n",
    "        try:\n",
    "            feedback_prompt = self.create_feedback_prompt(original_text, reasoning, prediction, category)\n",
    "            \n",
    "            inputs = self.tokenizer(feedback_prompt, return_tensors=\"pt\", \n",
    "                                   max_length=config.max_length // 2, truncation=True).to(config.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(**inputs, max_new_tokens=128, temperature=temperature,\n",
    "                                             do_sample=True, top_p=0.9, \n",
    "                                             pad_token_id=self.tokenizer.eos_token_id)\n",
    "            \n",
    "            feedback_text = self.tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], \n",
    "                                                  skip_special_tokens=True)\n",
    "            \n",
    "            scores = self.parse_feedback_scores(feedback_text)\n",
    "            scores['feedback_text'] = feedback_text\n",
    "            return scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'comprehensiveness': 5.0, 'logic': 5.0, 'accuracy': 5.0, \n",
    "                   'confidence': 5.0, 'overall': 5.0, 'feedback_text': f\"Error: {str(e)}\"}\n",
    "\n",
    "print(\"‚úì RLAIFScorer class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5150662",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Main Classifier Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b842b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedAceGPTClassifier:\n",
    "    \"\"\"Advanced multi-label classifier with cultural context, few-shot, CoT, and RLAIF.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, cultural_mapper, few_shot_bank, cot_prompter, labels):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cultural_mapper = cultural_mapper\n",
    "        self.few_shot_bank = few_shot_bank\n",
    "        self.cot_prompter = cot_prompter\n",
    "        self.rlaif_scorer = RLAIFScorer(model, tokenizer)\n",
    "        self.labels = labels\n",
    "    \n",
    "    def classify_single_category(self, text: str, category: str, \n",
    "                                use_rlaif: bool = False, num_few_shot: int = 2) -> Dict:\n",
    "        try:\n",
    "            # Get few-shot examples\n",
    "            few_shot_examples = self.few_shot_bank.get_few_shot_examples(category, n=num_few_shot)\n",
    "            few_shot_text = self.few_shot_bank.format_few_shot_prompt(category, few_shot_examples)\n",
    "            \n",
    "            # Create CoT prompt\n",
    "            cot_prompt = self.cot_prompter.create_cot_prompt(text, category, few_shot_text)\n",
    "            \n",
    "            # Generate reasoning\n",
    "            inputs = self.tokenizer(cot_prompt, return_tensors=\"pt\", \n",
    "                                   max_length=config.max_length, truncation=True).to(config.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(**inputs, max_new_tokens=config.max_new_tokens,\n",
    "                                             temperature=config.temperature, do_sample=True,\n",
    "                                             top_p=config.top_p, \n",
    "                                             pad_token_id=self.tokenizer.eos_token_id)\n",
    "            \n",
    "            reasoning = self.tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], \n",
    "                                             skip_special_tokens=True)\n",
    "            \n",
    "            # Parse prediction\n",
    "            prediction, parsed_reasoning = self.cot_prompter.parse_cot_response(reasoning)\n",
    "            \n",
    "            # RLAIF scoring (optional)\n",
    "            rlaif_scores = None\n",
    "            if use_rlaif:\n",
    "                rlaif_scores = self.rlaif_scorer.generate_feedback(text, reasoning, prediction, category)\n",
    "            \n",
    "            return {\n",
    "                'category': category,\n",
    "                'prediction': prediction,\n",
    "                'reasoning': reasoning,\n",
    "                'rlaif_scores': rlaif_scores,\n",
    "                'prompt_length': len(cot_prompt)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error classifying {category}: {e}\")\n",
    "            return {'category': category, 'prediction': 0, 'reasoning': f\"Error: {str(e)}\",\n",
    "                   'rlaif_scores': None, 'prompt_length': 0}\n",
    "    \n",
    "    def classify_text(self, text: str, use_rlaif: bool = False, num_few_shot: int = 2) -> Dict:\n",
    "        results = {'text': text, 'predictions': {}, 'category_details': {}}\n",
    "        \n",
    "        for category in self.labels:\n",
    "            category_result = self.classify_single_category(text, category, use_rlaif, num_few_shot)\n",
    "            results['predictions'][category] = category_result['prediction']\n",
    "            results['category_details'][category] = category_result\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def batch_classify(self, texts: List[str], use_rlaif: bool = False, \n",
    "                      num_few_shot: int = 2, show_progress: bool = True) -> List[Dict]:\n",
    "        results = []\n",
    "        iterator = tqdm(texts, desc=\"Classifying\") if show_progress else texts\n",
    "        \n",
    "        for text in iterator:\n",
    "            try:\n",
    "                result = self.classify_text(text, use_rlaif, num_few_shot)\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing text: {e}\")\n",
    "                results.append({'text': text, \n",
    "                              'predictions': {label: 0 for label in self.labels}, \n",
    "                              'category_details': {}})\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"‚úì AdvancedAceGPTClassifier class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acaa411",
   "metadata": {},
   "source": [
    "## üìä Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1018cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(f\"Loading data from: {DATA_FILE}\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(DATA_FILE)\n",
    "    print(f\"‚úì Data loaded: {len(df)} samples\")\n",
    "    \n",
    "    # Define labels\n",
    "    labels = ['political', 'racial/ethnic', 'religious', 'gender/sexual', 'other']\n",
    "    \n",
    "    # Fill NaN values\n",
    "    for label in labels:\n",
    "        if label in df.columns:\n",
    "            df[label] = df[label].fillna(0).astype(int)\n",
    "    \n",
    "    # Build few-shot example bank\n",
    "    print(\"\\nBuilding few-shot example bank...\")\n",
    "    few_shot_bank = FewShotExampleBank(df, labels)\n",
    "    \n",
    "    print(\"\\nExample bank statistics:\")\n",
    "    for label in labels:\n",
    "        pos_count = len(few_shot_bank.example_bank[label]['positive'])\n",
    "        neg_count = len(few_shot_bank.example_bank[label]['negative'])\n",
    "        print(f\"  {label}: {pos_count} positive, {neg_count} negative\")\n",
    "    \n",
    "    # Initialize CoT prompter\n",
    "    cot_prompter = ChainOfThoughtPrompter(cultural_mapper)\n",
    "    \n",
    "    print(\"\\n‚úì Data preparation complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error loading data: {e}\")\n",
    "    print(\"\\nPlease ensure:\")\n",
    "    print(\"  1. You've uploaded the dataset using 'Add Data'\")\n",
    "    print(\"  2. DATA_FILE path is correct\")\n",
    "    print(\"  3. The CSV file contains the required columns\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1bd9a4",
   "metadata": {},
   "source": [
    "## üöÄ Load AceGPT Model (Kaggle GPU Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d19c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LOADING ACEGPT MODEL (KAGGLE GPU OPTIMIZED)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Device: {config.device}\")\n",
    "print(f\"8-bit quantization: {config.use_8bit}\")\n",
    "print(\"\\n‚ö† If loading appears stuck, it's actually working - please wait!\")\n",
    "print(\"Expected: 3-5 minutes with progress indicators\\n\")\n",
    "\n",
    "try:\n",
    "    # Load tokenizer\n",
    "    print(\"Step 1/3: Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config.model_name,\n",
    "        trust_remote_code=True,\n",
    "        use_fast=True\n",
    "    )\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(\"‚úì Tokenizer loaded\\n\")\n",
    "    \n",
    "    # Step 2: Prepare model configuration\n",
    "    print(\"Step 2/3: Configuring model loading...\")\n",
    "    \n",
    "    if config.use_8bit and config.device == \"cuda\":\n",
    "        print(\"  ‚Üí Using 8-bit quantization (saves 50% memory)\")\n",
    "        \n",
    "        # More robust quantization config\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_threshold=6.0,\n",
    "            llm_int8_skip_modules=None,\n",
    "            llm_int8_enable_fp32_cpu_offload=False,\n",
    "        )\n",
    "        \n",
    "        load_kwargs = {\n",
    "            \"trust_remote_code\": True,\n",
    "            \"quantization_config\": quantization_config,\n",
    "            \"device_map\": \"auto\",\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "            \"offload_folder\": \"offload\",\n",
    "            \"max_memory\": {0: \"14GB\", \"cpu\": \"30GB\"}  # Explicit limits for Kaggle\n",
    "        }\n",
    "        \n",
    "        print(\"  ‚Üí Memory limits: 14GB GPU, 30GB CPU\")\n",
    "        print(\"  ‚Üí Offloading enabled if needed\")\n",
    "        \n",
    "    elif config.device == \"cuda\":\n",
    "        print(\"  ‚Üí Using FP16 (no quantization)\")\n",
    "        load_kwargs = {\n",
    "            \"trust_remote_code\": True,\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"device_map\": \"auto\",\n",
    "            \"low_cpu_mem_usage\": True\n",
    "        }\n",
    "    else:\n",
    "        print(\"  ‚Üí Using CPU mode (FP32)\")\n",
    "        load_kwargs = {\n",
    "            \"trust_remote_code\": True,\n",
    "            \"torch_dtype\": torch.float32,\n",
    "            \"low_cpu_mem_usage\": True\n",
    "        }\n",
    "    \n",
    "    print(\"‚úì Configuration ready\\n\")\n",
    "    \n",
    "    # Step 3: Load model (this is the slow part)\n",
    "    print(\"Step 3/3: Loading model into memory...\")\n",
    "    print(\"‚è≥ This takes 3-5 minutes - loading layers...\")\n",
    "    print(\"   (Low GPU utilization is normal during initial loading)\\n\")\n",
    "    \n",
    "    # Import for progress\n",
    "    import sys\n",
    "    from datetime import datetime\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.model_name,\n",
    "        **load_kwargs\n",
    "    )\n",
    "    \n",
    "    elapsed = (datetime.now() - start_time).total_seconds()\n",
    "    print(f\"\\n‚úì Model loaded in {elapsed:.1f} seconds!\")\n",
    "    \n",
    "    # Move to device if needed (CPU mode)\n",
    "    if config.device == \"cpu\" and \"device_map\" not in load_kwargs:\n",
    "        print(\"Moving model to CPU...\")\n",
    "        model = model.to(config.device)\n",
    "    \n",
    "    # Set to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Clear cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úì MODEL LOADED SUCCESSFULLY!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"  Device: {next(model.parameters()).device}\")\n",
    "    print(f\"  Dtype: {next(model.parameters()).dtype}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        mem_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        mem_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        mem_free = torch.cuda.mem_get_info()[0] / 1e9\n",
    "        \n",
    "        print(f\"  GPU Memory Allocated: {mem_allocated:.2f} GB\")\n",
    "        print(f\"  GPU Memory Reserved: {mem_reserved:.2f} GB\")\n",
    "        print(f\"  GPU Memory Free: {mem_free:.2f} GB\")\n",
    "    \n",
    "    # Initialize classifier\n",
    "    print(\"\\nüîß Initializing classifier...\")\n",
    "    classifier = AdvancedAceGPTClassifier(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        cultural_mapper=cultural_mapper,\n",
    "        few_shot_bank=few_shot_bank,\n",
    "        cot_prompter=cot_prompter,\n",
    "        labels=labels\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Classifier ready!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"‚úó ERROR LOADING MODEL\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Error: {str(e)}\\n\")\n",
    "    \n",
    "    # Provide detailed troubleshooting\n",
    "    print(\"Troubleshooting Steps:\")\n",
    "    print(\"\\n1. RESTART KERNEL:\")\n",
    "    print(\"   ‚Üí Session ‚Üí Restart Session\")\n",
    "    print(\"   ‚Üí Re-run from the beginning\")\n",
    "    \n",
    "    print(\"\\n2. CHECK GPU:\")\n",
    "    print(\"   ‚Üí Settings ‚Üí Accelerator ‚Üí Select 'GPU T4 x2'\")\n",
    "    print(\"   ‚Üí Settings ‚Üí Internet ‚Üí Enable\")\n",
    "    \n",
    "    print(\"\\n3. CLEAR CACHE:\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"   ‚Üí Running: torch.cuda.empty_cache()\")\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\"   ‚úì Cache cleared\")\n",
    "    \n",
    "    print(\"\\n4. IF STILL FAILING - Use smaller model:\")\n",
    "    print(\"   ‚Üí Change model_name to: 'aubmindlab/aragpt2-base'\")\n",
    "    print(\"   ‚Üí Or use: 'aubmindlab/bert-base-arabertv2'\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    \n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b08d3d3",
   "metadata": {},
   "source": [
    "## üß™ Test on Sample Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab35dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on sample texts\n",
    "test_samples = [\n",
    "    \"ÿ±ÿ¶Ÿäÿ≥ ÿßŸÑÿØŸàŸÑÿ© ŸÉÿßŸÅÿ± ŸàÿßŸÑÿ¥ÿπÿ® ÿ≥ÿßŸÉÿ™ ÿÆÿßÿ∑ÿ±Ÿà ÿ¥ÿπÿ® ÿ∑ÿ≠ÿßŸÜ\",\n",
    "    \"ÿ™ŸÉÿßÿ´ÿ± ÿ∫Ÿäÿ± ŸÖÿ≥ÿ®ŸàŸÇ ŸÑŸÑÿ£ŸÅÿßÿ±ŸÇÿ©...Ÿáÿ¨ŸàŸÖ ÿπŸÑŸâ ÿßŸÑŸÖŸÜÿßÿ≤ŸÑ\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING CLASSIFIER ON SAMPLE TEXTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, text in enumerate(test_samples, 1):\n",
    "    print(f\"\\nSample {i}: {text}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    result = classifier.classify_text(text, use_rlaif=False, num_few_shot=2)\n",
    "    \n",
    "    print(\"\\nPredictions:\")\n",
    "    for category, prediction in result['predictions'].items():\n",
    "        pred_text = \"‚úì Yes\" if prediction == 1 else \"‚úó No\"\n",
    "        print(f\"  {category}: {pred_text}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úì Sample testing complete!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94c4e12",
   "metadata": {},
   "source": [
    "## üìà Run Full Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d03ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare evaluation data\n",
    "print(\"Preparing evaluation data...\")\n",
    "\n",
    "df_labeled = df[(df[labels].notna().all(axis=1)) & (df[labels].sum(axis=1) > 0)].copy()\n",
    "print(f\"Total labeled samples: {len(df_labeled)}\")\n",
    "\n",
    "# Use config.eval_samples (30 on Kaggle)\n",
    "eval_size = min(config.eval_samples, len(df_labeled))\n",
    "eval_df = df_labeled.sample(n=eval_size, random_state=config.seed)\n",
    "\n",
    "print(f\"Evaluation set size: {len(eval_df)}\")\n",
    "print(\"\\nLabel distribution:\")\n",
    "for label in labels:\n",
    "    count = eval_df[label].sum()\n",
    "    print(f\"  {label}: {count} ({count/len(eval_df)*100:.1f}%)\")\n",
    "\n",
    "# Run evaluation\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"RUNNING EVALUATION\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Processing {len(eval_df)} samples √ó {len(labels)} categories\")\n",
    "print(\"Estimated time on Kaggle GPU: 10-20 minutes\\n\")\n",
    "\n",
    "eval_results = classifier.batch_classify(\n",
    "    texts=eval_df['text'].tolist(),\n",
    "    use_rlaif=True,\n",
    "    num_few_shot=2,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2673a79",
   "metadata": {},
   "source": [
    "## üìä Calculate and Display Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2974d735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predictions and ground truth\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for idx, row in eval_df.iterrows():\n",
    "    true_labels = [int(row[label]) for label in labels]\n",
    "    y_true.append(true_labels)\n",
    "\n",
    "for result in eval_results:\n",
    "    pred_labels = [result['predictions'][label] for label in labels]\n",
    "    y_pred.append(pred_labels)\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Calculate metrics\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "hamming = hamming_loss(y_true, y_pred)\n",
    "f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "f1_samples = f1_score(y_true, y_pred, average='samples', zero_division=0)\n",
    "\n",
    "print(\"Overall Metrics:\")\n",
    "print(f\"  Hamming Loss: {hamming:.4f} (lower is better)\")\n",
    "print(f\"  F1 Micro: {f1_micro:.4f}\")\n",
    "print(f\"  F1 Macro: {f1_macro:.4f}\")\n",
    "print(f\"  F1 Samples: {f1_samples:.4f}\")\n",
    "\n",
    "# Per-class metrics\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "print(f\"{'Category':<20} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true[:, i], y_pred[:, i], average='binary', zero_division=0\n",
    "    )\n",
    "    pos_support = int(y_true[:, i].sum())\n",
    "    print(f\"{label:<20} {precision:<12.4f} {recall:<12.4f} {f1:<12.4f} {pos_support}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62494d56",
   "metadata": {},
   "source": [
    "## üíæ Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6bd70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to Kaggle working directory\n",
    "results_df = eval_df.copy()\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    results_df[f'{label}_pred'] = y_pred[:, i]\n",
    "    results_df[f'{label}_true'] = y_true[:, i]\n",
    "\n",
    "# Save predictions\n",
    "output_path = WORKING_PATH / 'acegpt_predictions.csv'\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"‚úì Predictions saved to: {output_path}\")\n",
    "\n",
    "# Save metrics\n",
    "metrics_summary = {\n",
    "    'config': {\n",
    "        'model': config.model_name,\n",
    "        'few_shot_examples': config.num_few_shot_examples,\n",
    "        'max_length': config.max_length,\n",
    "        'use_8bit': config.use_8bit,\n",
    "        'device': config.device\n",
    "    },\n",
    "    'evaluation': {\n",
    "        'eval_size': len(eval_df),\n",
    "        'hamming_loss': float(hamming),\n",
    "        'f1_micro': float(f1_micro),\n",
    "        'f1_macro': float(f1_macro),\n",
    "        'f1_samples': float(f1_samples)\n",
    "    },\n",
    "    'per_class': {}\n",
    "}\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true[:, i], y_pred[:, i], average='binary', zero_division=0\n",
    "    )\n",
    "    metrics_summary['per_class'][label] = {\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1': float(f1),\n",
    "        'support': int(y_true[:, i].sum())\n",
    "    }\n",
    "\n",
    "metrics_path = WORKING_PATH / 'acegpt_metrics.json'\n",
    "with open(metrics_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úì Metrics saved to: {metrics_path}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"‚úì ALL RESULTS SAVED SUCCESSFULLY!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"\\nYou can download the results from the Output tab on the right ‚Üí\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493ffb22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Done!\n",
    "\n",
    "### What You've Accomplished:\n",
    "\n",
    "‚úÖ Loaded a 7B parameter Arabic LLM on Kaggle's free GPU  \n",
    "‚úÖ Implemented cultural context mapping for Arabic text  \n",
    "‚úÖ Used few-shot learning for better generalization  \n",
    "‚úÖ Applied chain-of-thought reasoning for transparency  \n",
    "‚úÖ Evaluated on multi-label Arabic polarization detection  \n",
    "‚úÖ Saved results for download  \n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Download Results**: Click the Output tab ‚Üí Download CSV and JSON files\n",
    "2. **Analyze**: Review the predictions and metrics\n",
    "3. **Iterate**: Adjust `config.num_few_shot_examples` or `max_length` and re-run\n",
    "4. **Scale Up**: Increase `config.eval_samples` to evaluate on more data\n",
    "5. **Enable RLAIF**: Set `use_rlaif=True` in classification calls for quality scoring\n",
    "\n",
    "### Performance on Kaggle:\n",
    "\n",
    "- **GPU**: T4 or P100 (16GB VRAM) ‚úì\n",
    "- **Memory**: ~10-12 GB used ‚úì\n",
    "- **Speed**: ~30-50 seconds per sample ‚úì\n",
    "- **Quality**: State-of-the-art Arabic LLM ‚úì\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or Issues?** Check the cell outputs above for troubleshooting tips!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
