{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b1a7e4a",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad73a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Add subtask1 directory to path for imports\n",
    "sys.path.insert(0, str(Path.cwd() / 'subtask1'))\n",
    "\n",
    "# Import preprocessing modules\n",
    "from ArbPreBasic import ArabicBasicPreprocessor\n",
    "from ArbPreAdv import ArabicAdvancedPreprocessor\n",
    "\n",
    "# Import feature engineering module (reload to get latest changes)\n",
    "import importlib\n",
    "import feature_engineering\n",
    "importlib.reload(feature_engineering)\n",
    "\n",
    "from feature_engineering import (\n",
    "    Subtask1FeatureEngineer,\n",
    "    Subtask2FeatureEngineer,\n",
    "    Subtask3FeatureEngineer,\n",
    "    ClassificationDecision,\n",
    "    CascadingResult,\n",
    ")\n",
    "\n",
    "print(\"âœ“ All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db3c24d",
   "metadata": {},
   "source": [
    "## 2. Configure Pipeline Settings\n",
    "\n",
    "**Select your preprocessing method and target subtask:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d06d0c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Preprocessing: BASIC\n",
      "  Target Subtask: 1\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# USER CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "# PREPROCESSING METHOD: Choose 'basic' or 'advanced'\n",
    "PREPROCESSING_METHOD = 'basic'  # Options: 'basic', 'advanced'\n",
    "\n",
    "# TARGET SUBTASK: Choose 1, 2, or 3\n",
    "TARGET_SUBTASK = 1  # Options: 1 (binary polarization), 2 (multi-label toxicity), 3 (hate-speech severity)\n",
    "\n",
    "# ============================================\n",
    "# ADVANCED PREPROCESSING OPTIONS (only used if PREPROCESSING_METHOD = 'advanced')\n",
    "# ============================================\n",
    "ADVANCED_CONFIG = {\n",
    "    'split_proclitics': None,  # Set of proclitic types to split, e.g., {'CONJ', 'PREP'}\n",
    "    'split_enclitics': {'PRON'},  # Split pronominal enclitics (recommended)\n",
    "    'keep_definite_article': True,  # Keep 'Al+' attached (recommended)\n",
    "    'keep_particles': True,  # Keep particles attached (recommended)\n",
    "    'use_light_stemming': False,  # Apply light stemming\n",
    "    'use_lemmatization': False,  # Apply lemmatization\n",
    "    'use_basic_preprocessing': True  # Apply basic preprocessing first\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Preprocessing: {PREPROCESSING_METHOD.upper()}\")\n",
    "print(f\"  Target Subtask: {TARGET_SUBTASK}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0678fc",
   "metadata": {},
   "source": [
    "## 3. Initialize Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "288d6c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Initialized ArabicBasicPreprocessor\n",
      "  - Character standardization (Alef, Hamza, Yaa variants)\n",
      "  - Diacritics removal\n",
      "  - Tatweel removal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the selected preprocessor\n",
    "if PREPROCESSING_METHOD.lower() == 'basic':\n",
    "    preprocessor = ArabicBasicPreprocessor()\n",
    "    print(\"âœ“ Initialized ArabicBasicPreprocessor\")\n",
    "    print(\"  - Character standardization (Alef, Hamza, Yaa variants)\")\n",
    "    print(\"  - Diacritics removal\")\n",
    "    print(\"  - Tatweel removal\")\n",
    "    \n",
    "elif PREPROCESSING_METHOD.lower() == 'advanced':\n",
    "    preprocessor = ArabicAdvancedPreprocessor(**ADVANCED_CONFIG)\n",
    "    print(\"âœ“ Initialized ArabicAdvancedPreprocessor\")\n",
    "    print(\"  - All basic preprocessing features\")\n",
    "    print(\"  - Morphological analysis using CAMeL Tools\")\n",
    "    print(\"  - Selective segmentation (pronominal enclitics)\")\n",
    "    if ADVANCED_CONFIG['use_lemmatization']:\n",
    "        print(\"  - Lemmatization enabled\")\n",
    "    if ADVANCED_CONFIG['use_light_stemming']:\n",
    "        print(\"  - Light stemming enabled\")\n",
    "else:\n",
    "    raise ValueError(f\"Invalid preprocessing method: {PREPROCESSING_METHOD}. Choose 'basic' or 'advanced'.\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb05a275",
   "metadata": {},
   "source": [
    "## 4. Initialize Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22eb7ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Initialized Subtask 1 Feature Engineer (Binary Polarization)\n",
      "  - Dialectal normalization\n",
      "  - Toxic lexicon ratio scoring (Mubarak et al., 2017)\n",
      "  - Prefilter threshold: 0.3\n",
      "\n",
      "Pipeline ready!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the selected feature engineer\n",
    "if TARGET_SUBTASK == 1:\n",
    "    feature_engineer = Subtask1FeatureEngineer(preprocessor=preprocessor)\n",
    "    subtask_name = \"Binary Polarization\"\n",
    "    prefilter_threshold = 0.30\n",
    "    \n",
    "elif TARGET_SUBTASK == 2:\n",
    "    feature_engineer = Subtask2FeatureEngineer(preprocessor=preprocessor)\n",
    "    subtask_name = \"Multi-label Toxicity\"\n",
    "    prefilter_threshold = 0.25\n",
    "    \n",
    "elif TARGET_SUBTASK == 3:\n",
    "    feature_engineer = Subtask3FeatureEngineer(preprocessor=preprocessor)\n",
    "    subtask_name = \"Hate-Speech Severity\"\n",
    "    prefilter_threshold = 0.20\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"Invalid subtask: {TARGET_SUBTASK}. Choose 1, 2, or 3.\")\n",
    "\n",
    "print(f\"âœ“ Initialized Subtask {TARGET_SUBTASK} Feature Engineer ({subtask_name})\")\n",
    "print(\"  - Dialectal normalization\")\n",
    "print(\"  - Toxic lexicon ratio scoring (Mubarak et al., 2017)\")\n",
    "print(f\"  - Prefilter threshold: {prefilter_threshold}\")\n",
    "print()\n",
    "print(\"Pipeline ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8332d7c8",
   "metadata": {},
   "source": [
    "## 5. Test Pipeline on Sample Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad514c9",
   "metadata": {},
   "source": [
    "## 5b. Cascading Classifier - Batch Processing Demo\n",
    "\n",
    "The cascading classifier uses rule-based pre-filtering to bypass deep learning for obvious cases:\n",
    "- **TOXIC** (high confidence): toxic_ratio > 0.15\n",
    "- **CLEAN** (moderate confidence): toxic_ratio < 0.02\n",
    "- **UNCERTAIN**: Falls back to model prediction (~40% of samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c451ac0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH CASCADING CLASSIFICATION RESULTS\n",
      "================================================================================\n",
      "1. ðŸŸ¢ CLEAN (âš¡ Rule) | ratio=0.00 | hits=[]\n",
      "   Text: ÙƒØªØ§Ø¨Ù‡Ù… Ø¬Ù…ÙŠÙ„ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…ÙˆÙ† ÙŠØ¯Ø±Ø³ÙˆÙ† ÙÙŠ Ø§Ù„Ù…Ø¯Ø±Ø³Ø©...\n",
      "\n",
      "2. ðŸ”´ TOXIC (âš¡ Rule) | ratio=0.60 | hits=['Ø¬Ø§Ù‡Ù„', 'ÙØ§Ø³Ù‚ÙŠÙ†', 'ÙƒÙ„Ø§Ø¨']\n",
      "   Text: Ø´Ø¹Ø¨ Ø¬Ø§Ù‡Ù„ ÙˆØ­ÙƒØ§Ù… ÙØ§Ø³Ù‚ÙŠÙ† ÙƒÙ„Ø§Ø¨...\n",
      "\n",
      "3. ðŸŸ¢ CLEAN (âš¡ Rule) | ratio=0.00 | hits=[]\n",
      "   Text: Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ Ø¹Ø§Ø¯ÙŠ...\n",
      "\n",
      "4. ðŸ”´ TOXIC (âš¡ Rule) | ratio=0.33 | hits=['Ø­Ù…Ø§Ø±']\n",
      "   Text: Ø§Ù†Øª Ø­Ù…Ø§Ø± ÙˆØºØ¨ÙŠ...\n",
      "\n",
      "5. ðŸŸ¢ CLEAN (âš¡ Rule) | ratio=0.00 | hits=[]\n",
      "   Text: Ø§Ù„Ø·Ù‚Ø³ Ø¬Ù…ÙŠÙ„ Ø§Ù„ÙŠÙˆÙ…...\n",
      "\n",
      "6. ðŸ”´ TOXIC (âš¡ Rule) | ratio=0.50 | hits=['ÙƒÙ„Ø¨', 'Ø®Ù†Ø²ÙŠØ±']\n",
      "   Text: ÙŠØ§ ÙƒÙ„Ø¨ ÙŠØ§ Ø®Ù†Ø²ÙŠØ±...\n",
      "\n",
      "7. ðŸŸ¢ CLEAN (âš¡ Rule) | ratio=0.00 | hits=[]\n",
      "   Text: Ù…Ø±Ø­Ø¨Ø§ ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ...\n",
      "\n",
      "8. ðŸ”´ TOXIC (âš¡ Rule) | ratio=0.33 | hits=['Ù…Ù†Ø§ÙÙ‚']\n",
      "   Text: Ø§Ù†Øª Ù…Ù†Ø§ÙÙ‚ ÙˆØ®Ø§Ø¦Ù†...\n",
      "\n",
      "================================================================================\n",
      "CASCADE STATISTICS\n",
      "================================================================================\n",
      "Total samples: 8\n",
      "Rule-based decisions: 8 (100.0%)\n",
      "  - Toxic by rule: 4\n",
      "  - Clean by rule: 4\n",
      "Model required: 0 (0.0%)\n",
      "\n",
      "ðŸ’¡ This means the model only needs to process 0.0% of samples!\n"
     ]
    }
   ],
   "source": [
    "# Batch cascading classification demo\n",
    "batch_texts = [\n",
    "    \"ÙƒØªØ§Ø¨Ù‡Ù… Ø¬Ù…ÙŠÙ„ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…ÙˆÙ† ÙŠØ¯Ø±Ø³ÙˆÙ† ÙÙŠ Ø§Ù„Ù…Ø¯Ø±Ø³Ø©\",  # Clean\n",
    "    \"Ø´Ø¹Ø¨ Ø¬Ø§Ù‡Ù„ ÙˆØ­ÙƒØ§Ù… ÙØ§Ø³Ù‚ÙŠÙ† ÙƒÙ„Ø§Ø¨\",  # Toxic (3 hits)\n",
    "    \"Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ Ø¹Ø§Ø¯ÙŠ\",  # Clean\n",
    "    \"Ø§Ù†Øª Ø­Ù…Ø§Ø± ÙˆØºØ¨ÙŠ\",  # Toxic (2 hits in 3 tokens = 0.67)\n",
    "    \"Ø§Ù„Ø·Ù‚Ø³ Ø¬Ù…ÙŠÙ„ Ø§Ù„ÙŠÙˆÙ…\",  # Clean\n",
    "    \"ÙŠØ§ ÙƒÙ„Ø¨ ÙŠØ§ Ø®Ù†Ø²ÙŠØ±\",  # Toxic\n",
    "    \"Ù…Ø±Ø­Ø¨Ø§ ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ\",  # Clean\n",
    "    \"Ø§Ù†Øª Ù…Ù†Ø§ÙÙ‚ ÙˆØ®Ø§Ø¦Ù†\",  # Toxic\n",
    "]\n",
    "\n",
    "# Apply batch cascading classifier\n",
    "results = feature_engineer.cascading_classify_batch(\n",
    "    batch_texts,\n",
    "    toxic_threshold=0.15,\n",
    "    clean_threshold=0.02\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"BATCH CASCADING CLASSIFICATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "for i, (text, result) in enumerate(zip(batch_texts, results), 1):\n",
    "    status = \"ðŸ”´ TOXIC\" if result.decision.value == \"TOXIC\" else \"ðŸŸ¢ CLEAN\" if result.decision.value == \"CLEAN\" else \"ðŸŸ¡ UNCERTAIN\"\n",
    "    model_flag = \"ðŸ“Š Model\" if result.requires_model else \"âš¡ Rule\"\n",
    "    print(f\"{i}. {status} ({model_flag}) | ratio={result.toxic_ratio:.2f} | hits={result.toxic_hits}\")\n",
    "    print(f\"   Text: {text[:50]}...\")\n",
    "    print()\n",
    "\n",
    "# Get cascade statistics\n",
    "stats = feature_engineer.get_cascade_stats(results)\n",
    "print(\"=\"*80)\n",
    "print(\"CASCADE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total samples: {stats['total']}\")\n",
    "print(f\"Rule-based decisions: {stats['rule_based']} ({stats['rule_based_pct']}%)\")\n",
    "print(f\"  - Toxic by rule: {stats['toxic_by_rule']}\")\n",
    "print(f\"  - Clean by rule: {stats['clean_by_rule']}\")\n",
    "print(f\"Model required: {stats['model_required']} ({stats['model_required_pct']}%)\")\n",
    "print()\n",
    "print(\"ðŸ’¡ This means the model only needs to process\", \n",
    "      f\"{stats['model_required_pct']}% of samples!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3990169f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE TEXT PROCESSING WITH CASCADING CLASSIFIER\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Sample 1\n",
      "================================================================================\n",
      "Original text:\n",
      "  ÙƒØªØ§Ø¨Ù‡Ù… Ø¬Ù…ÙŠÙ„ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…ÙˆÙ† ÙŠØ¯Ø±Ø³ÙˆÙ† ÙÙŠ Ø§Ù„Ù…Ø¯Ø±Ø³Ø©\n",
      "\n",
      "Preprocessed text:\n",
      "  ÙƒØªØ§Ø¨Ù‡Ù… Ø¬Ù…ÙŠÙ„ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…ÙˆÙ† ÙŠØ¯Ø±Ø³ÙˆÙ† ÙÙŠ Ø§Ù„Ù…Ø¯Ø±Ø³Ø©\n",
      "\n",
      "Dialect-normalized text:\n",
      "  ÙƒØªØ§Ø¨Ù‡Ù… Ø¬Ù…ÙŠÙ„ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…ÙˆÙ† ÙŠØ¯Ø±Ø³ÙˆÙ† ÙÙŠ Ø§Ù„Ù…Ø¯Ø±Ø³Ø©\n",
      "\n",
      "Tokens: 6\n",
      "  ['ÙƒØªØ§Ø¨Ù‡Ù…', 'Ø¬Ù…ÙŠÙ„', 'ÙˆØ§Ù„Ù…Ø¹Ù„Ù…ÙˆÙ†', 'ÙŠØ¯Ø±Ø³ÙˆÙ†', 'ÙÙŠ', 'Ø§Ù„Ù…Ø¯Ø±Ø³Ø©']\n",
      "\n",
      "Toxic Lexicon Ratio: 0.0000\n",
      "Toxic Lexicon Hits: []\n",
      "\n",
      "CASCADING CLASSIFIER RESULT:\n",
      "  Decision: CLEAN\n",
      "  Confidence: 0.85\n",
      "  Requires Model: No (rule-based)\n",
      "\n",
      "================================================================================\n",
      "Sample 2\n",
      "================================================================================\n",
      "Original text:\n",
      "  Ø´Ø¹Ø¨ Ø¬Ø§Ù‡Ù„ ÙˆØ­ÙƒØ§Ù… ÙØ§Ø³Ù‚ÙŠÙ† ÙƒÙ„Ø§Ø¨ Ø§Ù…Ø±ÙŠÙƒÙŠ ÙˆØ§Ù„ØµÙ‡Ø§ÙŠÙ†Ø©\n",
      "\n",
      "Preprocessed text:\n",
      "  Ø´Ø¹Ø¨ Ø¬Ø§Ù‡Ù„ ÙˆØ­ÙƒØ§Ù… ÙØ§Ø³Ù‚ÙŠÙ† ÙƒÙ„Ø§Ø¨ Ø§Ù…Ø±ÙŠÙƒÙŠ ÙˆØ§Ù„ØµÙ‡Ø§ÙŠÙ†Ø©\n",
      "\n",
      "Dialect-normalized text:\n",
      "  Ø´Ø¹Ø¨ Ø¬Ø§Ù‡Ù„ ÙˆØ­ÙƒØ§Ù… ÙØ§Ø³Ù‚ÙŠÙ† ÙƒÙ„Ø§Ø¨ Ø§Ù…Ø±ÙŠÙƒÙŠ ÙˆØ§Ù„ØµÙ‡Ø§ÙŠÙ†Ø©\n",
      "\n",
      "Tokens: 7\n",
      "  ['Ø´Ø¹Ø¨', 'Ø¬Ø§Ù‡Ù„', 'ÙˆØ­ÙƒØ§Ù…', 'ÙØ§Ø³Ù‚ÙŠÙ†', 'ÙƒÙ„Ø§Ø¨', 'Ø§Ù…Ø±ÙŠÙƒÙŠ', 'ÙˆØ§Ù„ØµÙ‡Ø§ÙŠÙ†Ø©']\n",
      "\n",
      "Toxic Lexicon Ratio: 0.4286\n",
      "Toxic Lexicon Hits: ['Ø¬Ø§Ù‡Ù„', 'ÙØ§Ø³Ù‚ÙŠÙ†', 'ÙƒÙ„Ø§Ø¨']\n",
      "\n",
      "CASCADING CLASSIFIER RESULT:\n",
      "  Decision: TOXIC\n",
      "  Confidence: 0.90\n",
      "  Requires Model: No (rule-based)\n",
      "\n",
      "================================================================================\n",
      "Sample 3\n",
      "================================================================================\n",
      "Original text:\n",
      "  Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ø¹Ø§Ø¯ÙŠØ© ÙÙ‚Ø·\n",
      "\n",
      "Preprocessed text:\n",
      "  Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„ÙŠ ÙƒÙ„Ù…Ø§Øª Ø¹Ø§Ø¯ÙŠØ© ÙÙ‚Ø·\n",
      "\n",
      "Dialect-normalized text:\n",
      "  Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„ÙŠ ÙƒÙ„Ù…Ø§Øª Ø¹Ø§Ø¯ÙŠØ© ÙÙ‚Ø·\n",
      "\n",
      "Tokens: 7\n",
      "  ['Ù‡Ø°Ø§', 'Ø§Ù„Ù†Øµ', 'ÙŠØ­ØªÙˆÙŠ', 'Ø¹Ù„ÙŠ', 'ÙƒÙ„Ù…Ø§Øª', 'Ø¹Ø§Ø¯ÙŠØ©', 'ÙÙ‚Ø·']\n",
      "\n",
      "Toxic Lexicon Ratio: 0.0000\n",
      "Toxic Lexicon Hits: []\n",
      "\n",
      "CASCADING CLASSIFIER RESULT:\n",
      "  Decision: CLEAN\n",
      "  Confidence: 0.85\n",
      "  Requires Model: No (rule-based)\n",
      "\n",
      "================================================================================\n",
      "Sample 4\n",
      "================================================================================\n",
      "Original text:\n",
      "  Ø§Ù†Øª Ø­Ù…Ø§Ø± ÙˆØºØ¨ÙŠ ÙˆÙ…Ù†Ø§ÙÙ‚\n",
      "\n",
      "Preprocessed text:\n",
      "  Ø§Ù†Øª Ø­Ù…Ø§Ø± ÙˆØºØ¨ÙŠ ÙˆÙ…Ù†Ø§ÙÙ‚\n",
      "\n",
      "Dialect-normalized text:\n",
      "  Ø§Ù†Øª Ø­Ù…Ø§Ø± ÙˆØºØ¨ÙŠ ÙˆÙ…Ù†Ø§ÙÙ‚\n",
      "\n",
      "Tokens: 4\n",
      "  ['Ø§Ù†Øª', 'Ø­Ù…Ø§Ø±', 'ÙˆØºØ¨ÙŠ', 'ÙˆÙ…Ù†Ø§ÙÙ‚']\n",
      "\n",
      "Toxic Lexicon Ratio: 0.2500\n",
      "Toxic Lexicon Hits: ['Ø­Ù…Ø§Ø±']\n",
      "\n",
      "CASCADING CLASSIFIER RESULT:\n",
      "  Decision: TOXIC\n",
      "  Confidence: 0.90\n",
      "  Requires Model: No (rule-based)\n",
      "\n",
      "================================================================================\n",
      "Sample 5\n",
      "================================================================================\n",
      "Original text:\n",
      "  Ø¨Ø¹Ù„ Ø§Ù…ÙˆÙ† Ø±Ø¯ Ø¨Ø§Ù„Ùƒ ØªÙˆØµÙ„ Ø§Ù„Ù„ÙŠØ¨ÙŠØ§Øª ÙŠØ§ØªÙˆÙ†Ø³ÙŠ ÙŠØ§Ø¨ØºÙ„\n",
      "\n",
      "Preprocessed text:\n",
      "  Ø¨Ø¹Ù„ Ø§Ù…ÙˆÙ† Ø±Ø¯ Ø¨Ø§Ù„Ùƒ ØªÙˆØµÙ„ Ø§Ù„Ù„ÙŠØ¨ÙŠØ§Øª ÙŠØ§ØªÙˆÙ†Ø³ÙŠ ÙŠØ§Ø¨ØºÙ„\n",
      "\n",
      "Dialect-normalized text:\n",
      "  Ø¨Ø¹Ù„ Ø§Ù…ÙˆÙ† Ø±Ø¯ Ø¨Ø§Ù„Ùƒ ØªÙˆØµÙ„ Ø§Ù„Ù„ÙŠØ¨ÙŠØ§Øª ÙŠØ§ØªÙˆÙ†Ø³ÙŠ ÙŠØ§Ø¨ØºÙ„\n",
      "\n",
      "Tokens: 8\n",
      "  ['Ø¨Ø¹Ù„', 'Ø§Ù…ÙˆÙ†', 'Ø±Ø¯', 'Ø¨Ø§Ù„Ùƒ', 'ØªÙˆØµÙ„', 'Ø§Ù„Ù„ÙŠØ¨ÙŠØ§Øª', 'ÙŠØ§ØªÙˆÙ†Ø³ÙŠ', 'ÙŠØ§Ø¨ØºÙ„']\n",
      "\n",
      "Toxic Lexicon Ratio: 0.0000\n",
      "Toxic Lexicon Hits: []\n",
      "\n",
      "CASCADING CLASSIFIER RESULT:\n",
      "  Decision: CLEAN\n",
      "  Confidence: 0.85\n",
      "  Requires Model: No (rule-based)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Sample Arabic texts for testing (includes words from expanded lexicon)\n",
    "sample_texts = [\n",
    "    \"ÙƒØªØ§Ø¨Ù‡Ù… Ø¬Ù…ÙŠÙ„ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…ÙˆÙ† ÙŠØ¯Ø±Ø³ÙˆÙ† ÙÙŠ Ø§Ù„Ù…Ø¯Ø±Ø³Ø©\",  # Clean text\n",
    "    \"Ø´Ø¹Ø¨ Ø¬Ø§Ù‡Ù„ ÙˆØ­ÙƒØ§Ù… ÙØ§Ø³Ù‚ÙŠÙ† ÙƒÙ„Ø§Ø¨ Ø§Ù…Ø±ÙŠÙƒÙŠ ÙˆØ§Ù„ØµÙ‡Ø§ÙŠÙ†Ø©\",  # Contains: Ø¬Ø§Ù‡Ù„, ÙØ§Ø³Ù‚ÙŠÙ†, ÙƒÙ„Ø§Ø¨\n",
    "    \"Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ø¹Ø§Ø¯ÙŠØ© ÙÙ‚Ø·\",  # Clean text\n",
    "    \"Ø§Ù†Øª Ø­Ù…Ø§Ø± ÙˆØºØ¨ÙŠ ÙˆÙ…Ù†Ø§ÙÙ‚\",  # High toxic ratio: Ø­Ù…Ø§Ø±, ØºØ¨ÙŠ, Ù…Ù†Ø§ÙÙ‚\n",
    "    \"Ø¨Ø¹Ù„ Ø§Ù…ÙˆÙ† Ø±Ø¯ Ø¨Ø§Ù„Ùƒ ØªÙˆØµÙ„ Ø§Ù„Ù„ÙŠØ¨ÙŠØ§Øª ÙŠØ§ØªÙˆÙ†Ø³ÙŠ ÙŠØ§Ø¨ØºÙ„\",  # High toxic ratio: Ø­Ù…Ø§Ø±, ØºØ¨ÙŠ, Ù…Ù†Ø§ÙÙ‚\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE TEXT PROCESSING WITH CASCADING CLASSIFIER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Sample {i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Original text:\\n  {text}\")\n",
    "    print()\n",
    "    \n",
    "    # Process text through the pipeline\n",
    "    features = feature_engineer.transform_text(text)\n",
    "    \n",
    "    print(f\"Preprocessed text:\\n  {features['preprocessed_text']}\")\n",
    "    print()\n",
    "    print(f\"Dialect-normalized text:\\n  {features['dialect_normalized_text']}\")\n",
    "    print()\n",
    "    print(f\"Tokens: {len(features['tokens'])}\")\n",
    "    print(f\"  {features['tokens']}\")\n",
    "    print()\n",
    "    print(f\"Toxic Lexicon Ratio: {features['toxic_lexicon_ratio']:.4f}\")\n",
    "    print(f\"Toxic Lexicon Hits: {features['toxic_lexicon_hits']}\")\n",
    "    print()\n",
    "    \n",
    "    # Apply cascading classifier\n",
    "    result = feature_engineer.cascading_classify(\n",
    "        text,\n",
    "        toxic_threshold=0.15,\n",
    "        clean_threshold=0.02\n",
    "    )\n",
    "    \n",
    "    print(f\"CASCADING CLASSIFIER RESULT:\")\n",
    "    print(f\"  Decision: {result.decision.value}\")\n",
    "    print(f\"  Confidence: {result.confidence:.2f}\")\n",
    "    print(f\"  Requires Model: {'Yes' if result.requires_model else 'No (rule-based)'}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363757d0",
   "metadata": {},
   "source": [
    "## 6. Process DataFrame (Batch Processing)\n",
    "\n",
    "Apply the pipeline to a pandas DataFrame with multiple texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22e7c520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   id                                     text\n",
      "0   1  ÙƒØªØ§Ø¨Ù‡Ù… Ø¬Ù…ÙŠÙ„ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…ÙˆÙ† ÙŠØ¯Ø±Ø³ÙˆÙ† ÙÙŠ Ø§Ù„Ù…Ø¯Ø±Ø³Ø©\n",
      "1   2              Ø´Ù„ÙˆÙ† Ø§Ù„Ø­Ø§Ù„ØŸ ÙˆÙŠÙ† Ø±Ø§ÙŠØ­ Ø§Ù„ÙŠÙˆÙ…ØŸ\n",
      "2   3       Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ø¹Ø§Ø¯ÙŠØ© ÙÙ‚Ø·\n",
      "3   4                     Ø§Ù„Ù†Øµ Ø§Ù„Ø±Ø§Ø¨Ø¹ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a sample DataFrame\n",
    "sample_df = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4],\n",
    "    'text': [\n",
    "        \"ÙƒØªØ§Ø¨Ù‡Ù… Ø¬Ù…ÙŠÙ„ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…ÙˆÙ† ÙŠØ¯Ø±Ø³ÙˆÙ† ÙÙŠ Ø§Ù„Ù…Ø¯Ø±Ø³Ø©\",\n",
    "        \"Ø´Ù„ÙˆÙ† Ø§Ù„Ø­Ø§Ù„ØŸ ÙˆÙŠÙ† Ø±Ø§ÙŠØ­ Ø§Ù„ÙŠÙˆÙ…ØŸ\",\n",
    "        \"Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ø¹Ø§Ø¯ÙŠØ© ÙÙ‚Ø·\",\n",
    "        \"Ø§Ù„Ù†Øµ Ø§Ù„Ø±Ø§Ø¨Ø¹ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(sample_df)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23d61f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed DataFrame with features:\n",
      "   id                                     text                          text_normalized  toxic_ratio\n",
      "0   1  ÙƒØªØ§Ø¨Ù‡Ù… Ø¬Ù…ÙŠÙ„ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…ÙˆÙ† ÙŠØ¯Ø±Ø³ÙˆÙ† ÙÙŠ Ø§Ù„Ù…Ø¯Ø±Ø³Ø©  ÙƒØªØ§Ø¨Ù‡Ù… Ø¬Ù…ÙŠÙ„ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…ÙˆÙ† ÙŠØ¯Ø±Ø³ÙˆÙ† ÙÙŠ Ø§Ù„Ù…Ø¯Ø±Ø³Ø©          0.0\n",
      "1   2              Ø´Ù„ÙˆÙ† Ø§Ù„Ø­Ø§Ù„ØŸ ÙˆÙŠÙ† Ø±Ø§ÙŠØ­ Ø§Ù„ÙŠÙˆÙ…ØŸ                 ÙƒÙŠÙ Ø§Ù„Ø­Ø§Ù„ Ø£ÙŠÙ† Ø±Ø§ÙŠØ­ Ø§Ù„ÙŠÙˆÙ…          0.0\n",
      "2   3       Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ø¹Ø§Ø¯ÙŠØ© ÙÙ‚Ø·       Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„ÙŠ ÙƒÙ„Ù…Ø§Øª Ø¹Ø§Ø¯ÙŠØ© ÙÙ‚Ø·          0.0\n",
      "3   4                     Ø§Ù„Ù†Øµ Ø§Ù„Ø±Ø§Ø¨Ø¹ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±                     Ø§Ù„Ù†Øµ Ø§Ù„Ø±Ø§Ø¨Ø¹ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±          0.0\n",
      "\n",
      "\n",
      "Detailed view of toxic lexicon hits:\n",
      "ID 1: []\n",
      "ID 2: []\n",
      "ID 3: []\n",
      "ID 4: []\n"
     ]
    }
   ],
   "source": [
    "# Apply feature engineering to DataFrame\n",
    "processed_df = feature_engineer.add_features(\n",
    "    sample_df,\n",
    "    text_col='text',\n",
    "    normalized_col='text_normalized',\n",
    "    ratio_col='toxic_ratio',\n",
    "    hits_col='toxic_hits',\n",
    "    already_preprocessed=False,\n",
    "    inplace=False\n",
    ")\n",
    "\n",
    "print(\"Processed DataFrame with features:\")\n",
    "print(processed_df[['id', 'text', 'text_normalized', 'toxic_ratio']].to_string())\n",
    "print()\n",
    "print(\"\\nDetailed view of toxic lexicon hits:\")\n",
    "for idx, row in processed_df.iterrows():\n",
    "    print(f\"ID {row['id']}: {row['toxic_hits']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7e6767",
   "metadata": {},
   "source": [
    "## 7. Load and Process Real Data\n",
    "\n",
    "Apply the pipeline to actual training/dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "106becda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: subtask1\n",
      "Train file: subtask1/train/arb.csv\n",
      "Dev file: subtask1/dev/arb.csv\n",
      "\n",
      "âš  Training file not found: subtask1/train/arb.csv\n",
      "âš  Dev file not found: subtask1/dev/arb.csv\n"
     ]
    }
   ],
   "source": [
    "# Define data paths based on selected subtask\n",
    "data_dir = Path(f'subtask{TARGET_SUBTASK}')\n",
    "train_file = data_dir / 'train' / 'arb.csv'\n",
    "dev_file = data_dir / 'dev' / 'arb.csv'\n",
    "\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Train file: {train_file}\")\n",
    "print(f\"Dev file: {dev_file}\")\n",
    "print()\n",
    "\n",
    "# Load data if files exist\n",
    "if train_file.exists():\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    print(f\"âœ“ Loaded training data: {len(train_df)} samples\")\n",
    "    print(f\"  Columns: {list(train_df.columns)}\")\n",
    "else:\n",
    "    print(f\"âš  Training file not found: {train_file}\")\n",
    "    train_df = None\n",
    "\n",
    "if dev_file.exists():\n",
    "    dev_df = pd.read_csv(dev_file)\n",
    "    print(f\"âœ“ Loaded dev data: {len(dev_df)} samples\")\n",
    "    print(f\"  Columns: {list(dev_df.columns)}\")\n",
    "else:\n",
    "    print(f\"âš  Dev file not found: {dev_file}\")\n",
    "    dev_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a15ecbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No training data to process\n"
     ]
    }
   ],
   "source": [
    "# Process training data (if loaded)\n",
    "if train_df is not None:\n",
    "    print(\"Processing training data...\")\n",
    "    \n",
    "    # Determine the text column name (might be 'text' or 'sentence')\n",
    "    text_col = 'text' if 'text' in train_df.columns else 'sentence'\n",
    "    \n",
    "    train_processed = feature_engineer.add_features(\n",
    "        train_df,\n",
    "        text_col=text_col,\n",
    "        normalized_col='text_normalized',\n",
    "        ratio_col='toxic_ratio',\n",
    "        hits_col='toxic_hits',\n",
    "        already_preprocessed=False,\n",
    "        inplace=False\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ Processed {len(train_processed)} training samples\")\n",
    "    print(f\"\\nFirst 3 samples:\")\n",
    "    display_cols = ['id', text_col, 'text_normalized', 'toxic_ratio']\n",
    "    display_cols = [c for c in display_cols if c in train_processed.columns]\n",
    "    print(train_processed[display_cols].head(3).to_string())\n",
    "    \n",
    "    print(f\"\\nToxic ratio statistics:\")\n",
    "    print(train_processed['toxic_ratio'].describe())\n",
    "else:\n",
    "    print(\"No training data to process\")\n",
    "    train_processed = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47c7493e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No dev data to process\n"
     ]
    }
   ],
   "source": [
    "# Process dev data (if loaded)\n",
    "if dev_df is not None:\n",
    "    print(\"Processing dev data...\")\n",
    "    \n",
    "    # Determine the text column name\n",
    "    text_col = 'text' if 'text' in dev_df.columns else 'sentence'\n",
    "    \n",
    "    dev_processed = feature_engineer.add_features(\n",
    "        dev_df,\n",
    "        text_col=text_col,\n",
    "        normalized_col='text_normalized',\n",
    "        ratio_col='toxic_ratio',\n",
    "        hits_col='toxic_hits',\n",
    "        already_preprocessed=False,\n",
    "        inplace=False\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ Processed {len(dev_processed)} dev samples\")\n",
    "    print(f\"\\nFirst 3 samples:\")\n",
    "    display_cols = ['id', text_col, 'text_normalized', 'toxic_ratio']\n",
    "    display_cols = [c for c in display_cols if c in dev_processed.columns]\n",
    "    print(dev_processed[display_cols].head(3).to_string())\n",
    "    \n",
    "    print(f\"\\nToxic ratio statistics:\")\n",
    "    print(dev_processed['toxic_ratio'].describe())\n",
    "else:\n",
    "    print(\"No dev data to process\")\n",
    "    dev_processed = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb7f831",
   "metadata": {},
   "source": [
    "## 8. Save Processed Data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3b76857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saving disabled. Set SAVE_PROCESSED_DATA = True to save processed files.\n"
     ]
    }
   ],
   "source": [
    "# Save processed data to CSV files\n",
    "SAVE_PROCESSED_DATA = False  # Set to True to save\n",
    "\n",
    "if SAVE_PROCESSED_DATA:\n",
    "    output_dir = data_dir / 'processed'\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    method_suffix = PREPROCESSING_METHOD[:3]  # 'bas' or 'adv'\n",
    "    \n",
    "    if train_processed is not None:\n",
    "        output_file = output_dir / f'train_arb_{method_suffix}.csv'\n",
    "        train_processed.to_csv(output_file, index=False)\n",
    "        print(f\"âœ“ Saved training data to: {output_file}\")\n",
    "    \n",
    "    if dev_processed is not None:\n",
    "        output_file = output_dir / f'dev_arb_{method_suffix}.csv'\n",
    "        dev_processed.to_csv(output_file, index=False)\n",
    "        print(f\"âœ“ Saved dev data to: {output_file}\")\n",
    "else:\n",
    "    print(\"Data saving disabled. Set SAVE_PROCESSED_DATA = True to save processed files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ceea85",
   "metadata": {},
   "source": [
    "## 9. Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2160a986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PIPELINE SUMMARY\n",
      "================================================================================\n",
      "Preprocessing Method: BASIC\n",
      "Target Subtask: 1 (Binary Polarization)\n",
      "Prefilter Threshold: 0.3\n",
      "\n",
      "Processing Steps:\n",
      "  1. Text preprocessing (character normalization, diacritics removal)\n",
      "  3. Dialectal normalization\n",
      "  4. Toxic lexicon ratio calculation\n",
      "\n",
      "Output Features:\n",
      "  - preprocessed_text: Text after basic/advanced preprocessing\n",
      "  - text_normalized: Text after dialectal normalization\n",
      "  - toxic_ratio: Proportion of toxic terms (Mubarak lexicon)\n",
      "  - toxic_hits: List of detected toxic terms\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PIPELINE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Preprocessing Method: {PREPROCESSING_METHOD.upper()}\")\n",
    "print(f\"Target Subtask: {TARGET_SUBTASK} ({subtask_name})\")\n",
    "print(f\"Prefilter Threshold: {prefilter_threshold}\")\n",
    "print()\n",
    "print(\"Processing Steps:\")\n",
    "print(\"  1. Text preprocessing (character normalization, diacritics removal)\")\n",
    "if PREPROCESSING_METHOD == 'advanced':\n",
    "    print(\"  2. Morphological analysis and segmentation (CAMeL Tools)\")\n",
    "print(\"  3. Dialectal normalization\")\n",
    "print(\"  4. Toxic lexicon ratio calculation\")\n",
    "print()\n",
    "print(\"Output Features:\")\n",
    "print(\"  - preprocessed_text: Text after basic/advanced preprocessing\")\n",
    "print(\"  - text_normalized: Text after dialectal normalization\")\n",
    "print(\"  - toxic_ratio: Proportion of toxic terms (Mubarak lexicon)\")\n",
    "print(\"  - toxic_hits: List of detected toxic terms\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
